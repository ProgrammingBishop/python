{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Sparse Rectifier Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectifier (Hinge Loss) Function:\n",
    "$$max(0, x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIF is the firing rate and the input current:\n",
    "- $f(I)$\n",
    "    - $[ t\\ log( \\frac{ E + RI - V_r }{ E + RI - V_{th} } ) + t_{ref} ]^{-1}$ if $E + RI > V_{th}$\n",
    "    - 0 if $E + RI \\le V_{th}$\n",
    "    \n",
    "Where $t_{ref}$ is the refactory period (time between two action potentials), $I$ is the input current, $V_r$ is the resting potential and the $V_{th}$ the threshold potential (with V_{th} > V_r), and $R$, $E$, $t$ the membrane resistance, potential and time constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most commonly used activation functions in deep learning and neural networks are the logistic sigmoid (between 0 and 1) and hyperbolic tangent (steady state at 0 and preferred from optimization standpoint; number between -1 and 1, but forces asymmetry around 0), which are equivelant to a linear transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Advantages of Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Disentangling: dense representations is highly entangled because almost any change in input modifies most entries in representation. If a representation is sparse to small inputs, set of non-zero features is almost always conserved by small changes in inputs.\n",
    "\n",
    "<br>\n",
    "\n",
    "Efficient Variable-Size Representation: Different inputs may contain different amounts of informatio and would be more conveniently rperesented using a variable-size data structure. By varying number of active neurons allows a model to control the effective dimensionality of the representation for a given input and the required precision.\n",
    "\n",
    "<br>\n",
    "\n",
    "Linear Separability: more likely to be linearly seperable because the information is represented in a high-dimension space. \n",
    "\n",
    "<br>\n",
    "\n",
    "Distributed but Sparse: sparse representations are exponentially greater with the power being the number of non-zero features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Rectifier Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectifier function is one-sided and does not enforce a sign symmetry or antisymmety. 0 represent no response. We can, however obtain symmetry or antisymmetry by combining two rectifier units sharing parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows a network to obtain sparse representations. After uniform initialization half of hidden units continuous output values are real zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$softplus(x) = log(1 + e^x)$$\n",
    "\n",
    "which is a smooth version of the rectifying non-linearty. Hard non-linearities do not hurt so long as the gradient can propagate along some paths (some hidden units in each layer are non-zero).\n",
    "\n",
    "It can be recommended to use the $L_1$ penalty on the activation function to enable even greater sparsity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Reconstruction Function:\n",
    "$$f(x, \\theta) = W_{dec}\\ max( W_{enc}x + b_{enc}, 0 ) + b_{dec}$$\n",
    "\n",
    "with $\\tilde x$ denoting the corrupted version of x, $\\sigma()$ the logistic sigmoid function and $\\theta$ the model parameters $(W_{enc}, b_{enc}, W_{dec}, b_{dec})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use softplus activation function for the reconstruction layer, along with a quadratic cost:\n",
    "$$L(x, \\theta) = || x - log( 1 + exp( f( \\tilde x, \\theta ) ) ) ||$$\n",
    "\n",
    "This strategy is proven to yield better generalizations on image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the rectifier activation values coming from the previous encoding layer to bound them between 0 and 1, then use a sigmoid activation function for the reconstruction layer along with a cross-entropy reconstruction set.\n",
    "- $L( x, \\theta ) = $\n",
    "    - $-x\\ log( \\sigma( f( \\tilde x, \\theta ) ) )$\n",
    "    - $-( 1 - x )\\ log( 1 - \\sigma( f( \\tilde x, \\theta ) ) )$\n",
    "    \n",
    "This has proven to privde better generalizations on text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a linear activation function for reconstruction layer along with a quadratic cost. And use a rectifier activation function for the reconstruction layer along with a quadratic cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative Log-Likelihood as the cost function for training:\n",
    "$$-log\\ P( CorrectClass | input )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite hard threshold at 0, networks trained with the rectifier activation function can find local minima of greater or equal quality than those obtained by softplus. This makes it so rectifiers are both biologically plausible and computationally efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectifiers also do not depend as much on unsupervised pre-training. Rectifier networks improve after an unsupervised pre-training phase in a semi-supervised setting. Adding hidden layers actually hurts the overall RMSE.\n",
    "\n",
    "<br>\n",
    "\n",
    "This concludes that rectifier units help to bridge gap between unsupervised pre-training and no pre-training, which suggests they may be better and finding better minima during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectifier networks tend to perform best for sentiment analysis as text-based tasks have a very large degree of sparsity and also good for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lesson: Rectifier Neural Networks work well with Sparse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

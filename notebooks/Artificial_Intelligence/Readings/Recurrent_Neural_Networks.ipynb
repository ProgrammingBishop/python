{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks that have persistent memory in the form of loops that pass informtion from one step of the network to the next. Think of these networks as multiple copies of the same network, each passing message to successor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designed to avoid the classic long-term dependency problem by being able to remember information for long periods of time. Classing RNNs would just have a single tanh layer, but LSTMs have a different structure, which is composed of four network layers. \n",
    "\n",
    "This type of Network aids in vanishing and exploding gradients caused by long-term memory. Weights will not be updated proportionately toward the beginning of the network during backpropagation.\n",
    "\n",
    "###### Architecture\n",
    "Cell State: runs down the entire chain with minor linear interactions. Information can be removed or added to the Cell State by the use of structures called Gates. \n",
    "\n",
    "###### Gates\n",
    "A way to optionally let information through and are composed out of a sigmoid neural net layer and a pointwise multiplication operation. The sigmoid layer outputs numbers between 0 and 1 describing how much of each component should be let through. 0 = none and 1 = all. LSTMs have three of these gates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\sigma$: Sigmoid Function\n",
    "- $W_f$: Weights\n",
    "- $b_f$: Biases\n",
    "- $i_t$: Values Updated\n",
    "- $\\tilde C_t$: Values Added to State\n",
    "- $C_{t - 1}$: Old Cell State\n",
    "- $C_t$: New Cell State\n",
    "- $f_t$: The Forget Output\n",
    "\n",
    "<br />\n",
    "<img src=\".\\images\\rnn_2.PNG\" />\n",
    "<br />\n",
    "\n",
    "First step is to decide what information will be thrown away from the cell state and is determined by a first sogmoid layer called the Forget Gate Layer. This looks at $h_{t - 1}$ and $x_t$, and outputs a number between 0 and 1 for each number in the Cell State $C_{t - 1}$. $h_{t - 1}$ is the output from the previous timestep and $x_t$ is the current input. SO the operation at this first Forget Gate Layer is:\n",
    "$$f_t = \\sigma(W_f\\ dot\\ [h_{t - 1}, x_t] + b_f)$$\n",
    "\n",
    "Next step is to decide what new information will be stored in a cell state; this step can be broken into two parts:\n",
    "- Sigmoid Layer called Input Gate Layer decides values to be updated\n",
    "$$i_t = \\sigma(W_i\\ dot\\ [h_{t - 1}, x_t] + b_i)$$\n",
    "\n",
    "- tanh Layer creates a vector of new candidate values $\\tilde C_t$ that can be added to the state.\n",
    "$$\\tilde C_t = tanh(W_C\\ dot\\ [h_{t - 1}, x_t] + b_C)$$\n",
    "\n",
    "Next we multiply the old state $C_{t - 1}$ by $f_t$, which will forget values from the Cell State. Then we add $i_t\\ dot\\ \\tilde C_t$, which represent the new candidate values scaled by how much we decided to update each state value. So:\n",
    "$$C_t = (f_t\\ dot\\ C_{t - 1}) + (i_t\\ dot\\ \\tilde C_t)$$\n",
    "\n",
    "this essentially comes out to be the Cell State is equal to the Cell State after forgetting values that came in from the previous Cell State with the addition of the New Values and Updated Values from the current Cell State.\n",
    "\n",
    "Now to decide what will be output, which will be based on the filtered version of the Cell State. We run a Sigmoid Layer to decide what parts of the Cell State will be output, then put the Cell State through a tanh (pushing values to be between -1 and 1) and then multiply it by the output of the sigmoid gate. \n",
    "$$o_t = \\sigma(W_o [h_{t - 1}, x_t] + b_o)$$\n",
    "$$h_t = o_t\\ dot\\ tanh(C_t)$$\n",
    "\n",
    "This part will decide which inputs will be sent through with the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variants on Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gers & Schmidhuber - Peephole Connections\n",
    "This allows the Gate Layer to look at the Cell State:\n",
    "- $f_t = \\sigma(W_f\\ dot\\ [C_{t - 1}, h_{t - 1}, x_t] + b_f)$\n",
    "- $i_t = \\sigma(W_i\\ dot\\ [C_{t - 1}, h_{t - 1}, x_t] + b_i)$\n",
    "- $o_t = \\sigma(W_o\\ dot\\ [C_t, h_{t - 1}, x_t] + b_o$\n",
    "\n",
    "###### Use Coupled Forget and Input Gates\n",
    "Make the decision to forget and add information is made together (only forget when we will inut something in its place):\n",
    "$$C_t = f_t\\ dot\\ C_{t - 1} + (1 - f_t)\\ dot\\ \\tilde C_t$$\n",
    "\n",
    "###### Cho - Gated Recurrent Unit (GRU)\n",
    "Combines the forget and input gates into a single Update Gate and merges the Cell State and the Hidden State."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagram Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<img src=\".\\images\\rnn_3.PNG\" />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x_t$: input of the current step\n",
    "- $h_{t - 1}$: is the output from the previous LSTM unit\n",
    "- $C_{t - 1}$: is the memory of the previous unit\n",
    "- $h_t$: is the output of the current network\n",
    "- $C_t$: memory of the current unit\n",
    "\n",
    "From the top pipe for $C_{t - 1}$, the $X$ represents the Forget Gate and the $+$ represents the merging of old memory not forgotten and the new memory. From here the output is the new memory $C_t$.\n",
    "\n",
    "The first layer - Forget Gate - will be given a bias vector $b_o$ - pink circle along with the other inputs. \n",
    "\n",
    "The second layer - New Memory Layer - and influences how much the new memory should influence the old memory. The new memory is generated by another layer that uses a tanh as the activation function.\n",
    "\n",
    "The third layer - Output Layer - and dictates how much new memory should output to the next LSTM unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras        import Sequential\n",
    "from keras.layers import Dense, RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Arguments\n",
    "- units: dimension of the output\n",
    "- activation: default is tanh\n",
    "- recurrent activation: activation to use for recurrent step; default is the hard sigmoid\n",
    "- use_bias: bool on whether ot use bias or not\n",
    "- kernel_initializer: used for linear transformatins of the recurrent state\n",
    "- bias_initializer: initializes the bias vector\n",
    "- unit_forget_bias: if True, add 1 to the bias of the forget gate initialization. This will force bias_initializer = 'zeros and is recommended by Jazefowicz\n",
    "- kernel_regularizer: regularizer function applied to the bias vector\n",
    "- recurrent_regularizer: regularizer function added to the recurrent_kernel weights matrix\n",
    "- bias_regularizer: regularizer function applied to bias vector\n",
    "- activity_regularizer: regularizer function applied to output of the layer\n",
    "- dropout: float between 0.0 and 1.0 and represents the fraction of untis to drop for the linear transformation of the inputs\n",
    "- recurrent_dropout: float between 0.0 and 1.0 and represents the fraction of units to drop for linear transformation of the recurrent state\n",
    "- stateful: default is false, but if true the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch\n",
    "\n",
    "Also CuDNNLSTM for GPU-enabled machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

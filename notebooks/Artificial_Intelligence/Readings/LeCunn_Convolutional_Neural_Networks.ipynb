{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeCunn Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nomenclature\n",
    "- GT: Graph Transformer\n",
    "- GTN Graph Transformer Network: allows training all modules to optimize a global performance criterion.\n",
    "- HMM: Hidden Markov Model\n",
    "- HOSL Heuristic Over-Segmentation: seperating individual characters out from neighborswithin a word or sentence.\n",
    "- KNN: K-Nearest Neighbors\n",
    "- NN: Neural Network\n",
    "- OCR: Optical Character Recognition\n",
    "- PCA: Principal Component Analysis\n",
    "- RBF: Radial Basis Function\n",
    "- RS-SVM: Reduced-set Support Vector Method\n",
    "- SDNN: Space Displacement Neural Network\n",
    "- SVM: Support Vector Machine\n",
    "- TDNN: Time Delay Neural Network\n",
    "- V-SVM: Virtual Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split process into two methods:\n",
    "- Feature Extractor: transforms input patterns so they can be represented by low-dimensional vectors or short strings of symbols that can easily be matched or compared and are relatively invariant with respect to transformations and distortions of the input patterns that do not change their nature.\n",
    "- Classifier: general purpose and trainable.\n",
    "\n",
    "Every problem needs to identify the set of features to use, hence why that is a step of its own. \n",
    "\n",
    "Convolutional Neural Networks incorporate knowledge about invariances of 2D shapes by using local connection patterns, and by imposing constraints on the weights. \n",
    "\n",
    "Gradient-based learning: learning machine computes a function:\n",
    "$$Y^p = F(Z^p, W)$$\n",
    "\n",
    "where $Z^p$ is the $p$-th input pattern, and $W$ represents the collection of adjustable parameters in the system. $Y^p$ is the recognized class label of the pattern or the scores/probability associated with a range of classes. The loss function:\n",
    "$$E^p = D(D^p, F(W, Z^p))$$\n",
    "\n",
    "measures the discrepancy between the actual value and the estimated value from the input pattern. The gap between the training and testing sets decrease with the number of training samples:\n",
    "$$E_{test} - E_{train} = k(\\frac{h}{P})^{\\alpha}$$\n",
    "\n",
    "where:\n",
    "- $P$ is the number of training samples\n",
    "- $h$ is a measure of effective capacity\n",
    "- $\\alpha$ is a number between 0.5 and 1.0\n",
    "- $k$ is a constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient-Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E(W)$ is continuous and differentiable everywhere. Simplest gradient descent algorithm is:\n",
    "$$W_k = W_{k - 1} - \\epsilon \\frac{\\partial E(W)}{\\partial W}$$\n",
    "\n",
    "where $\\epsilon$ is a scaler constant. A popular minimization procedure is the stochastic gradient algorithm consists of updating the parameter vector using a noisy, or approximated version of the average gradient. Common instance is:\n",
    "$$W_k = W_{k - 1} - \\epsilon \\frac{\\partial E^{pk}(W)}{\\partial W}$$\n",
    "\n",
    "which converges on large training sets with redundant samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropogation Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initialization\n",
    "$W$ can be initialized randomly from a uniform distribution with mean = 0 and standard deviation of 1.0.\n",
    "\n",
    "###### Forward Propagatation\n",
    "Start with the input we have, pass through the network layer and calculate the actual output of the model.\n",
    "\n",
    "###### Loss Function\n",
    "At this point we will have the output of the randomly initialized network given the randomly initialized weights and the actual values the network is trying to target. We use the square of the difference so the values are positive and that large errors are penalized more than small errors. \n",
    "\n",
    "###### Differentiation\n",
    "Optimization technique that modifies the internal weights of the network to minimize the total loss function previously defined. This differentiation calculates the derivative of the loss function (i.e. speed at which the function is changing its value at a given point).\n",
    "\n",
    "A question to ask is: how much of the total error will change if we change the internal weight of the neural network with a certain small value $\\partial W$.\n",
    "\n",
    "What is important is the rate in the increase of total errors relative to the change in weight. This is computed as:\n",
    "$$\\frac{\\nabla y}{\\nabla x}$$\n",
    "\n",
    "Once we receive an output, we adjust the weight in the following way:\n",
    "- Check derivative\n",
    "- If positive, meaning error increases with the weight increase, then decrease the weight.\n",
    "- If negative, meaning error decreases if we increase the weights, then increase the weight.\n",
    "- If 0, we do nothing as we have reached a stable point.\n",
    "\n",
    "##### Back-Propogation\n",
    "The derivative is decomposable, thus can be back-propogated. We start with a starting point of errors, then derive it. Example:  \n",
    "\n",
    "Input -> Layer 1 (3 * x) -> Layer 2 (2 * x) -> Output\n",
    "\n",
    "Consider $\\Delta x = 0.001$. After Layer 1 $\\Delta x = 0.003$, and after Layer 2 $\\Delta x = 0.006$. Back propogation will calculate the final result $0.006$ to $0.001$ in the reverse of the above process.\n",
    "\n",
    "Forward-propagation applies a function to the input, and back-propagation is performed by knowing the derivativeof the function. So we need to keep a stack of function calls during the forward pass with their parameters in order to back-propogate the errors using the derivatives of these functions. \n",
    "\n",
    "<br />\n",
    "<img src=\".\\images\\back_propogation.PNG\" />\n",
    "<br />\n",
    "\n",
    "- Derivative of the loss function in respect to the output ->\n",
    "- Derivative of the output in respect to the input variables \n",
    "\n",
    "So the final step of the back-propogation:\n",
    "$$\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x_i}$$\n",
    "\n",
    "<br />\n",
    "<img src=\".\\images\\back_propogation_2.PNG\" />\n",
    "<br />\n",
    "\n",
    "where $\\partial L$ is the derivative of the loss function. It is best to consider this as the chain rule: derivative of the outside by the inside multiplied by the derivative of the inside. \n",
    "\n",
    "###### Weight Update\n",
    "Delta Rule: \n",
    "$$W_t = W_{t - 1} - (\\nabla_w * \\lambda)$$\n",
    "\n",
    "- If derivative rate is positive, an increase in weight will increase the error, thus the new weight should be smaller.\n",
    "- If the derivative rate is negative, an increase in weight will decrease the error, thus we need to increase the weights.\n",
    "- If the derivative is 0, it means we are stable in a minimum and no updates are needed.\n",
    "\n",
    "Stochastic Gradient Descent: updates the weights after each single input is observed. Better suited for most occasions over the Delta Rule. Faster convergence.\n",
    "\n",
    "###### Iterate Until Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $X = 3x3$ and $W = 2x2$ without any padding and a stride of $1$ generating output $H = 2x2$. While performing the forward pass, cache the variables $X$ and $W$ (this is helpful for performaing backpropagation).\n",
    "\n",
    "The first iteration will be a 2x2 window in the top-left of a 3x3 matrix. This will map to the (1, 1) of the next layer represented as a 2x2 matrix. The value in (1, 1) will be the linear combination of the input $X$ in the current window and the $W$ for each value of the 2x2 window.\n",
    "\n",
    "<br />\n",
    "<img src=\".\\images\\back_propogation_3.PNG\" />\n",
    "<br />\n",
    "\n",
    "###### Notation\n",
    "- $\\partial h_{ij} = \\frac{\\partial L}{\\partial h_{ij}}$\n",
    "- $\\partial w_{ij} = \\frac{\\partial L}{\\partial w_{ij}}$\n",
    "\n",
    "It is important to note that any change in the weight of the filter window will affect all of the output pixels.\n",
    "\n",
    "<br />\n",
    "<img src=\".\\images\\back_propogation_4.PNG\" />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Globally Trainable Systems\n",
    "If the partial derivative of $E^p$ with respect to $X_n$ is known, then the partial derivatives of $E^p$ with respect to $W_n$ and $X_{n - 1}$ can be computed using the backward recurrence:\n",
    "$$\\frac{\\partial E^p}{\\partial W_n} = \\frac{\\partial F}{\\partial W}(W_n, X_{n - 1})\\ \\frac{\\partial E^p}{\\partial X_n}$$\n",
    "\n",
    "This equation computes some terms of the gradient of $E^p(W)$.\n",
    "\n",
    "$$\\frac{\\partial E^p}{\\partial X_{n - 1}} = \\frac{\\partial F}{\\partial X}(W_n, X_{n - 1})\\ \\frac{\\partial E^p}{\\partial X_n}$$\n",
    "\n",
    "This equation generates a backward recurrence - backpropagation procedure for the neural network. \n",
    "\n",
    "where $\\frac{\\partial F}{\\partial W}(W_n, X_{n - 1})$ is the Jacobian of $F$ with respect to $W$ evaluated at the point $(W_n, X_{n - 1})$, and $\\frac{\\partial F}{\\partial X}(W_n, X_{n - 1})$ is the Jacobian of $F$ with respect to $X$. \n",
    "\n",
    "The Jacobian of a vector function is a matrix containing the partial derivatives of all outputs with respect to all the inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three architectural ideas\n",
    "\n",
    "###### Local Receptive Fields\n",
    "Neurons can extract elementary visual features like edges, end-points, corners, etc. These are combined by subsequent layers to determine higher-order features. A complete convolutional layer is composed of several feature maps (each feature map focusing on a different feature i.e. using a different weight vector to allow different features to be derived from each location). Once a feature is detected, its exact location is irrelevant; only its position relative to other features is important.\n",
    "\n",
    "Sub-sampling of layers decreases the resolution by taking regional averages. By taking these regional averages variations of the overall image can be indentified by the network so that if a new observation comes to be predicted that has differences, the network can still output the correct label. \n",
    "\n",
    "LeNet\n",
    "\n",
    "<br />\n",
    "<img src=\".\\images\\back_propogation_5.PNG\" />\n",
    "<br />\n",
    "\n",
    "$n$: is the layer index\n",
    "$C_n$: Convolutional layer $n$\n",
    "$S_n$: Sub-sampling layer $n$\n",
    "\n",
    "- Comprises of 7 layers (excluding input layer of 32x32)\n",
    "- Input is normalized so mean is near 0\n",
    "- $C_1$ has 6 feature maps of 28x28 (prevents input from falling of the boundary) where each unit in each map is connected to a 5x5 neighborhood in the input\n",
    "- $S_2$ has 6 feature maps of 14x14. Each unit in each feature map is connected to a 2x2 neighborhood in the corresponding feature map of $C_1$. The four inputs to a unit in $S_2$ are multiplied by a trainable coefficient and added to a trainable bias. The result is passed through a sigmoid function. The 2x2 receptive fields are non-overlapping therefore feature maps in this layer have half the number of rows and columns as the previous layer $C_1$.\n",
    "- Layer $C_3$ has 16 feature maps. Each unit in each feature map is connected to a 5x5 neighborhood at identical locations in a subset of $S_2$. We can add dropout so every unit in the previous layer does not map to a unit in the current layer. The hopes is to force the feature map to extract different features because they get different sets of inputs. \n",
    "- Layer $S_4$ has 16 feature maps of size 5x5. Each unit is the feature map is connected to a 2x2 neighborhood in the corresponding feature map prior. \n",
    "- Layer $C_5$ has 120 feature maps. Each unit is connected to a 5x5 neighborhood on all of $S_4$s feature maps. The size of this layer's feature map is 1x1, therefore amounts to a fully connection, but not labeled as a fully connected layer.\n",
    "- Layer $F_6$ comes next. \n",
    "\n",
    "Layers up to the fully connected layer compute a dot product between their input vector and their weight vector, to which bias is added. This weighted sum $a_i$ for unit $i$ is then passed through a sigmoid squashing the function to produce the state of unit $i$ denoted by $x_i$.\n",
    "\n",
    "The squashing function is a scaled tanh function:\n",
    "$$f(a) = A tanh(Sa)$$\n",
    "\n",
    "where $A$ is the amplitude and $S$ is the slope at the origin. The constant $A$ is chosen to be $1.7159$\n",
    "\n",
    "The output layer is composed of Euclidean Radial Basis Function units, one for each class, with the inputs from $F_6$. The outputs of each RBF $y_i$ is computed as:\n",
    "$$y_i = \\sum_j (x_j - w_{ij})^2$$\n",
    "\n",
    "The above is the Euclidean distance between the input vector and its parameter vector. RBF can be interpreted as the unnormalized negative log-likelihood of a Gaussian distribution in the space of configurations of layer $F_6$. \n",
    "\n",
    "###### Shared Weights\n",
    "\n",
    "\n",
    "###### Spatial or Temporal Sub-Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood Estimation, which is equivelant to the Minimum Mean Squared Error:\n",
    "$$E(W) = \\frac{1}{P} \\sum_{p = 1}^ P\\ y_{D_p}(Z^p, W)$$\n",
    "\n",
    "where $y_{D_p}$ is the output of the $D_p$-th RBF unit (one corresponding to the correct class $Z^p$). This cost function is appropriate for most cases. A better criterion to use that avoids a collapsing effect - input is ignored and RBF outputs are equal to zero:\n",
    "$$E(W) = \\frac{1}{P} \\sum_{p = 1}^ P\\ y_{D_p}(Z^p, W) + log(e_{-j} + \\sum_i e^{-y_i(Z^p, W)})$$\n",
    "\n",
    "This prevents collapsing effect when the RBF parameters are learned because it keeps the RBD centers apart from each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invariance and Noise Resistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet-5 works for scaled variations of up to a factor of 2, vertical shift variations of plus or minus about half the height of the character, and rotations up to plus or minus 30 degrees. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

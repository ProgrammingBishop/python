{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture: memory cell that maintains its state over time, and non-linear gsting units which regulate information flow into and out of the cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x^t$ is the input vector at time $t$\n",
    "- $N$ the number of LSTM blocks\n",
    "- $M$ is the number of inputs\n",
    "\n",
    "###### LSTM Layer Weights\n",
    "- $W_z, W_i, W_f, W_o \\in R^{NxM}$: Input Weights\n",
    "- $R_z, R_i, R_f, R_o \\in R^{NxN}$: Recurrent Weights\n",
    "- $p_i, p_f, p_o \\in R^N$: Peephole Weights\n",
    "- $b_z, b_i, b_f, b_o \\in R^N$: Bias Weights\n",
    "\n",
    "###### Vector Formulas in Forward Pass\n",
    "- $\\bar z^t = W_zx^t + R_zy^{t - 1} + b_z$  \n",
    "- $z^t = g(\\bar z^t)$: Block Input\n",
    "- $\\bar i^t = W_ix^t + R_iy^{t - 1} + p_i\\ dot\\ c^{t - 1} + b_i$\n",
    "- $i^t = \\sigma(\\bar i^t)$: Input Gate\n",
    "- $\\bar f^t = W_fx^t + R_fy^{t - 1} + p_f\\ dot\\ c^{t - 1} + b_f$\n",
    "- $f^t = \\sigma(\\bar f^t)$ Forget Gate\n",
    "- $c^t = z^t\\ dot\\ i^t + c^{t - 1}\\ dot\\ f^t$\n",
    "- $\\bar o^t = W_ox^t + R_oy^{t - 1} + p_o\\ dot\\ c^t + b_o$\n",
    "- $o^t = \\sigma(\\bar o^t)$ Output Gate\n",
    "- $y^t = h(c^t)\\ dot\\ o^t$ Block Output\n",
    "\n",
    "where $\\sigma, g, h$ are point-wise non-linear activation functions.\n",
    "- Logistic Sigmoid: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is used as a gate activation function\n",
    "- Hyperbolic Tangent: $g(x) = h(x) = tanh(x)$ is usually used as the block input and output activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\delta y^t = \\Delta^t + R^T_z \\delta z^{t + 1} + R^T_i \\delta i^{t + 1} + R^T_f \\delta f^{t + 1} + R^T_o \\delta o^{t + 1}$\n",
    "- $\\delta \\bar o^t = \\delta y^t\\ dot\\ h(c^t)\\ dot\\ o'(\\bar o^t)$\n",
    "- $\\delta c^t = \\delta y^t\\ dot\\ o^t\\ dot\\ h'(c^t) + p_o\\ dot\\ \\delta \\bar o^t + p_i\\ dot\\ \\delta \\bar i^{t + 1} + p_f\\ dot\\ \\delta \\bar f^{t + 1} + \\delta c^{t + 1}\\ dot\\ f^{t + 1}$\n",
    "- $\\delta \\bar f^t = \\delta c^t\\ dot\\ c^{t - 1}\\ dot\\ o'(\\bar f^t)$\n",
    "- $\\delta \\bar i^t = \\delta c^t\\ dot\\ z^t\\ dot\\ o'(\\bar i^t)$\n",
    "- $\\delta \\bar z^t = \\delta c^t\\ dot\\ i^t\\ dot\\ g'(\\bar z^t)$\n",
    "\n",
    "where $\\Delta^t$ is vector of deltas passed down from the layer above. Deltas are only needed if the layers below need training, and can be computed as follows:\n",
    "$$\\delta x^t = W^T_z \\delta \\bar z^t + W^T_i \\delta \\bar i^t + W^T_f \\delta \\bar f^t + W^T_o \\delta \\bar o^t$$\n",
    "\n",
    "And the gradient of the weights can be calculated as the following:\n",
    "\n",
    "<br />\n",
    "<img src=\".\\images\\rnn_1.PNG\" />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

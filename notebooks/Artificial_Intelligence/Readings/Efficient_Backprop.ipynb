{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning and Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function $M(Z^p, W)$\n",
    "- $Z^p$ is the p-th input parameter\n",
    "- $W$ is the collection of adjustable parameters in the system\n",
    "\n",
    "Cost function $E^p = C(D^p, M(Z^p, W))$ measures discrepancy\n",
    "- $D^p$is the correct output for $Z^p$\n",
    "\n",
    "Goal is to find $W$ to minimize $E^p_{train}$. A common cost function is the MSE:\n",
    "$E^p = \\frac{1}{2}(D^p - M(Z^p, W))^2$\n",
    "\n",
    "Bias is how much a network differs from the output and variance is how much the network differs between new datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional multi-layer neural networks are a special case where modules are alternated layers of matrix multiplications and component-wise sigmoid functions.\n",
    "- $Y_n = W_n\\ X_{n-1}$\n",
    "- $X_n = F(Y_n)$\n",
    "\n",
    "where $W_n$ is a matrix whose number of columns is the dimension of $X_{n - 1}$, and number of rows is the dimension of $X_n$. $F$ is a vector function that applies a sigmoid function to each component of its input. $Y_n$ is the vector of weighted sums, or total inputs, to layer n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochasitc versus Batch Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an optimal minimization procedure where $W$ is iteratively adjusted:\n",
    "$W(t) = W(t - 1) - \\eta \\frac{\\partial E}{\\partial W}$\n",
    "\n",
    "where $\\eta$ is a scaler constant. The issue is that this equation requires a complete pass through the entire dataset in order to compute the average or true gradient (batch learning). Stochastic (online) learning where a single example ${Z^t, D^t}$ is chosen randomly from the training set at each iteration $t$. An estimate of the tru gradient is then computed based on the error $E^t$ of that example, and then the weights are updated:\n",
    "$W(t + 1) = W(t) - \\eta \\frac{\\partial E^t}{\\partial W}$\n",
    "\n",
    "The noise from this is advantageous. Stochastic is:\n",
    "- faster, particularly on redundant datasets\n",
    "- results in better solutions. The noice can help a gradient jump into an adjacent basin that could be potentially a global minima. Batch learning will find the minimum of the basin it starts in, which could end up being a local minima.\n",
    "- can be used to track changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks learn the fastest from the most unexpected sample. Choose a sample at each iteration that is the most unfamiliar to the system. This only applies to stochastic learning. It is best to do this by choosing successive examples that are from different classes since training examples belonging to the same class will contain similar information. Another method is to examine the error between network output and the target value. Large error means the input has not been learned and therefore contains new information. It would be best to present this input more frequently into the network while the error is large. The process of modifying the probability of appearance of each pattern is called an emphasizing scheme. \n",
    "\n",
    "The above technique applies to data that contains outliers can be detrimental as they produce large errors, but should not be presented repeatedly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence is faster if the average of each input variable over the training set is close to zero and the inputs are scaled so they all have the same covariance $C_i$ where:\n",
    "$C_i = \\frac{1}{P}\\ \\sum_{p = 1}^P\\ (z^p_i)^2$\n",
    "\n",
    "where $P$ is the number of training examples, $C_i$ is the covariance of the $i$th input variable, and $z^p_i$ is the $i$th component of the $p$th training example. This balances how the weights connected to input nodes learn and thus speeds the process up. \n",
    "\n",
    "Another opportunity is decorrelating the inputs. It is harder to solve for two inputs simultaneously than independently. PCA can remove linear correlations. So steps of transformation are:\n",
    "- shift inputs to mean zero\n",
    "- decorrelate inputs\n",
    "- equalize covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid functions:\n",
    "- Logistic $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- Hyperbolic Tangent $tanh(x)$\n",
    "\n",
    "Symmetric sigmoids like hyperbolic tangent tend to converge faster than standard logistic functions. This is because $tanh(x)$ has a mean around 0. A recommended sigmoid is $f(x) = 1.7159 tanh(\\frac{2}{3}x)$. It is also beneficial to sometimes add a small linear term $f(x) = tanh(x) + ax$ to avoid flat spots. \n",
    "\n",
    "One issue with the symmetric sigmoids is the error surface can be flat near the origin. For this reason it is wise to initialize weights at small numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target values set to the Sigmoid's asymptote have several drawbacks:\n",
    "- Weights will be drvien to larger and larger values where the sigmoid derivatives are close to zero. Large weights increase the gradient, but when multiplied by an exponentially small sigmoid derivative, a weight results in being close to zero, causing the weights to become stuck. \n",
    "- When input patterns fall near a decision boundary the output class is uncertain. For this reason a network should output a value that is inbetween two possibloe output values; not near either asymptote. Large weights tend to force outputs to the tails of the sigmoid, which causes a wrong class prediction without indication of uncertainty.\n",
    "\n",
    "The solution to avoid the above scenarios is to set target values to be within the range of the sigmoid rather than at the asymptotic values. The best way to do this is to set target values to the point of the maximum second derivative on the sigmoid, which is around $\\pm 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights should be activated randomly, but in a way that the sigmoid is activated in its linear region. Large weights makes learning slow by over-saturation, small weights result in small gradients and also slow learning. The approach is to assure the distribution of the outputs of each node have a standard deviation of around 1 (aka normalizing the training set).\n",
    "\n",
    "To maintain this standard deviation of 1 for each output at each layer, just use the sigmoid function with the requirement that the input to the sigmoid also has a standard deviation of 1:\n",
    "$\\sigma_{yi} = (\\sum_j\\ w^2_{ij})^{\\frac{1}{2}}$\n",
    "\n",
    "So to insure a standard deviation of 1, wieghts should be randomly drawn from a distribution with mean 0 and standard deviation given by:\n",
    "$\\sigma_w = m^{-\\frac{1}{2}}$\n",
    "\n",
    "where $m$ is the number of inputs to the unit. This is a uniform distribuion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is advisable for stochastic gradient descent to pick a learning rate $\\eta_i$ for each weight as this can improve convergence. To ensure that all weights converge around the same speed, it is best to use larger rates in the lower layers and smaller weights in the higher layers. \n",
    "\n",
    "Other tricks to improve convergence:\n",
    "- Momentum: $\\Delta w(t + 1) = \\eta \\frac{\\partial E_{t + 1}}{\\partial w} + \\mu \\Delta w(t)$ can increase the convergence when the cost surface is non-spherical because it damps the size of the steps along directions of high curvature thus yielding a larger effective learning rate along directions of low curvature. Generally this is used more in batch mode.\n",
    "- Adaptive Learning Rates: increase or decrease learning rate based on the error. \n",
    "    - Smallest eigenvalue of the Hessian is smaller than the second smallest eigenvalue and therefore after a large number of iterations. the parameter vector $w(t)$ will approach the minimum from the direction of the minimum eigenvector of the Hessian. \n",
    "    - Put simply, if the error is large proceed with big steps, and if the error is small it anneals the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Basis Functions vs Sigmoid Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

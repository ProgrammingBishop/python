{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning and Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function $M(Z^p, W)$\n",
    "- $Z^p$ is the p-th input parameter\n",
    "- $W$ is the collection of adjustable parameters in the system\n",
    "\n",
    "Cost function $E^p = C(D^p, M(Z^p, W))$ measures discrepancy\n",
    "- $D^p$is the correct output for $Z^p$\n",
    "\n",
    "Goal is to find $W$ to minimize $E^p_{train}$. A common cost function is the MSE:\n",
    "$E^p = \\frac{1}{2}(D^p - M(Z^p, W))^2$\n",
    "\n",
    "Bias is how much a network differs from the output and variance is how much the network differs between new datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional multi-layer neural networks are a special case where modules are alternated layers of matrix multiplications and component-wise sigmoid functions.\n",
    "- $Y_n = W_n\\ X_{n-1}$\n",
    "- $X_n = F(Y_n)$\n",
    "\n",
    "where $W_n$ is a matrix whose number of columns is the dimension of $X_{n - 1}$, and number of rows is the dimension of $X_n$. $F$ is a vector function that applies a sigmoid function to each component of its input. $Y_n$ is the vector of weighted sums, or total inputs, to layer n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochasitc versus Batch Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an optimal minimization procedure where $W$ is iteratively adjusted:\n",
    "$W(t) = W(t - 1) - \\eta \\frac{\\partial E}{\\partial W}$\n",
    "\n",
    "where $\\eta$ is a scaler constant. The issue is that this equation requires a complete pass through the entire dataset in order to compute the average or true gradient (batch learning). Stochastic (online) learning where a single example ${Z^t, D^t}$ is chosen randomly from the training set at each iteration $t$. An estimate of the tru gradient is then computed based on the error $E^t$ of that example, and then the weights are updated:\n",
    "$W(t + 1) = W(t) - \\eta \\frac{\\partial E^t}{\\partial W}$\n",
    "\n",
    "The noise from this is advantageous. Stochastic is:\n",
    "- faster, particularly on redundant datasets\n",
    "- results in better solutions. The noice can help a gradient jump into an adjacent basin that could be potentially a global minima. Batch learning will find the minimum of the basin it starts in, which could end up being a local minima.\n",
    "- can be used to track changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks learn the fastest from the most unexpected sample. Choose a sample at each iteration that is the most unfamiliar to the system. This only applies to stochastic learning. It is best to do this by choosing successive examples that are from different classes since training examples belonging to the same class will contain similar information. Another method is to examine the error between network output and the target value. Large error means the input has not been learned and therefore contains new information. It would be best to present this input more frequently into the network while the error is large. The process of modifying the probability of appearance of each pattern is called an emphasizing scheme. \n",
    "\n",
    "The above technique applies to data that contains outliers can be detrimental as they produce large errors, but should not be presented repeatedly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence is faster if the average of each input variable over the training set is close to zero and the inputs are scaled so they all have the same covariance $C_i$ where:\n",
    "$C_i = \\frac{1}{P}\\ \\sum_{p = 1}^P\\ (z^p_i)^2$\n",
    "\n",
    "where $P$ is the number of training examples, $C_i$ is the covariance of the $i$th input variable, and $z^p_i$ is the $i$th component of the $p$th training example. This balances how the weights connected to input nodes learn and thus speeds the process up. \n",
    "\n",
    "Another opportunity is decorrelating the inputs. It is harder to solve for two inputs simultaneously than independently. PCA can remove linear correlations. So steps of transformation are:\n",
    "- shift inputs to mean zero\n",
    "- decorrelate inputs\n",
    "- equalize covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid functions:\n",
    "- Logistic $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- Hyperbolic Tangent $tanh(x)$\n",
    "\n",
    "Symmetric sigmoids like hyperbolic tangent tend to converge faster than standard logistic functions. This is because $tanh(x)$ has a mean around 0. A recommended sigmoid is $f(x) = 1.7159 tanh(\\frac{2}{3}x)$. It is also beneficial to sometimes add a small linear term $f(x) = tanh(x) + ax$ to avoid flat spots. \n",
    "\n",
    "One issue with the symmetric sigmoids is the error surface can be flat near the origin. For this reason it is wise to initialize weights at small numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target values set to the Sigmoid's asymptote have several drawbacks:\n",
    "- Weights will be drvien to larger and larger values where the sigmoid derivatives are close to zero. Large weights increase the gradient, but when multiplied by an exponentially small sigmoid derivative, a weight results in being close to zero, causing the weights to become stuck. \n",
    "- When input patterns fall near a decision boundary the output class is uncertain. For this reason a network should output a value that is inbetween two possibloe output values; not near either asymptote. Large weights tend to force outputs to the tails of the sigmoid, which causes a wrong class prediction without indication of uncertainty.\n",
    "\n",
    "The solution to avoid the above scenarios is to set target values to be within the range of the sigmoid rather than at the asymptotic values. The best way to do this is to set target values to the point of the maximum second derivative on the sigmoid, which is around $\\pm 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights should be activated randomly, but in a way that the sigmoid is activated in its linear region. Large weights makes learning slow by over-saturation, small weights result in small gradients and also slow learning. The approach is to assure the distribution of the outputs of each node have a standard deviation of around 1 (aka normalizing the training set).\n",
    "\n",
    "To maintain this standard deviation of 1 for each output at each layer, just use the sigmoid function with the requirement that the input to the sigmoid also has a standard deviation of 1:\n",
    "$\\sigma_{yi} = (\\sum_j\\ w^2_{ij})^{\\frac{1}{2}}$\n",
    "\n",
    "So to insure a standard deviation of 1, wieghts should be randomly drawn from a distribution with mean 0 and standard deviation given by:\n",
    "$\\sigma_w = m^{-\\frac{1}{2}}$\n",
    "\n",
    "where $m$ is the number of inputs to the unit. This is a uniform distribuion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is advisable for stochastic gradient descent to pick a learning rate $\\eta_i$ for each weight as this can improve convergence. To ensure that all weights converge around the same speed, it is best to use larger rates in the lower layers and smaller weights in the higher layers. \n",
    "\n",
    "Other tricks to improve convergence:\n",
    "- Momentum: $\\Delta w(t + 1) = \\eta \\frac{\\partial E_{t + 1}}{\\partial w} + \\mu \\Delta w(t)$ can increase the convergence when the cost surface is non-spherical because it damps the size of the steps along directions of high curvature thus yielding a larger effective learning rate along directions of low curvature. Generally this is used more in batch mode.\n",
    "- Adaptive Learning Rates: increase or decrease learning rate based on the error. \n",
    "    - Smallest eigenvalue of the Hessian is smaller than the second smallest eigenvalue and therefore after a large number of iterations. the parameter vector $w(t)$ will approach the minimum from the direction of the minimum eigenvector of the Hessian. \n",
    "    - Put simply, if the error is large proceed with big steps, and if the error is small it anneals the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Basis Functions vs Sigmoid Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radial Basis Functions (RBF) uses Euclidean Distance between the input and the weights and the Sigmoid is replaced by an exponential. Output is:  \n",
    "$g(x) = \\sum_{i = 1}^N\\ w_i exp( \\frac{1}{2\\sigma_i^2} || x - v_i ||^2 )$\n",
    "\n",
    "where $v_i$ is the mean standard deviation of the $i$-th Gaussian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update equation for gradient descent is:  \n",
    "$W(t + 1) = W(t) - \\eta\\ \\frac{dE(W)}{dW}$\n",
    "\n",
    "- if $\\eta < \\eta_{opt}$ then the weight will move toward the minimum\n",
    "- if $\\eta = \\eta_{opt}$ then the weight will meet the local minimum in one step\n",
    "- if $\\eta > \\eta_{opt}$ then the weight will oscillate around the minimum eventually obtaining the minimum.\n",
    "- if $\\eta > 2\\eta_{opt}$ then divergence will occur and the weight will never reach the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Transformations Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Subtract the Means from the Input Variables\n",
    "Nonzero mean in input creates a very large eigenvalue, i.e. the cost surface will be steep in some directions and shallow in others so that convergence will be very slow. For a single linear neuron, the eigenvectors of the Hessian (mean subtracted) point along the principal axes of the cloud of training vectors. \n",
    "\n",
    "###### Normalize the Variances of the Input Variables\n",
    "Inputs that have a large variation in spread along different directions of the input space will have a large condition number and slow learning. If inputs are correlated, this will make the error surface spherical, but possibly reduce eccentricity. Correlated input variables cause eigenvectors of the Hessian Matrix to be rotated away from the corrdinate axes, thus weight updates are not decoupled. Decoupling makes the one learning rate per weight optimal. \n",
    "\n",
    "###### Decorrelate the Input Variables \n",
    "If we assign each weight its own learning rate then the descent direction will be in the direction of the other that points in the direction of the minimum. \n",
    "\n",
    "###### Use a Seperate Learning Rate for Each Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagonal Levenberg Marquardt Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uses the Square Jacobi approximation\n",
    "- Designed for batch learning\n",
    "- Have complexity of $O(N^3)$\n",
    "- Most importantly, work only for mean squared error loss functions.\n",
    "\n",
    "This method has a regularization parameter $\\mu$ that prevents it from exploding, if some eigenvalues are small:\n",
    "$$\\Delta w = (\\sum_p \\frac{\\partial f(w, x_p)^T}{\\partial w}\\ \\frac{\\partial f(w, x_p)}{\\partial w} + \\mu I)^{-1} \\nabla E(w)$$\n",
    "\n",
    "where $I$ denotes the unity matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjugate Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Properties\n",
    "- $O(N)$ method\n",
    "- Does not use Hessian explicitly\n",
    "- Attempts to find descent directions that try to minimally spoil the result achieved in the previous iterations\n",
    "- It uses a line search\n",
    "- Most importantly: works only for batch learning\n",
    "\n",
    "It is for the last reason that we use this method if the training set is not too large or if the task is regression.\n",
    "\n",
    "- $p_k$ is the descent direction\n",
    "- $k$ is the iterations\n",
    "\n",
    "The evolution of the descent direction is given by:  \n",
    "$p_k = -\\nabla E(w_k) + \\beta_kp_{k - 1}$\n",
    "\n",
    "where the choice of $\\beta_k$ can be done according to Fletcher and Reeves or Polak and Ribiere.\n",
    "\n",
    "Polak and Ribiere generally works better in practice:  \n",
    "$$\\beta_{k} = \\frac{\\nabla E(w_{k})^T(\\nabla E(w_{k})}{\\nabla E(w_{k - 1})^T \\nabla E(w_{k - 1})}$$\n",
    "$$p_{k + 1} = max(\\beta_{k + 1}, 0)\\ p_k - \\nabla f(x_{k + 1})$$\n",
    "\n",
    "We place the max() for the update of $p_k$ to avoid the update to $\\beta$ becoming negative. Two directions $p_k$ and $p_{k - 1}$ are defined as conjugate if:\n",
    "$$p^T_k\\ Hp_{k - 1} = 0$$\n",
    "\n",
    "i.e conjugate directions are orthogonal directions in space of the identity Hessian matrix. Polak and Ribiere's choice seems more robust for non-quadratic function. This gradient can also be viewed as a good choice for the momentum term. \n",
    "\n",
    "On large redundant classification problems, stochastic back-propogation is faster, which is why the other oprion for training is Stochastic or Levenberg Marquadt Methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shuffle examples\n",
    "- Center inputs by subtracting the mean\n",
    "- Normalize input to standard deviation of 1\n",
    "- Decorrelate input variables\n",
    "- Pick network with Sigmoid function $1.7159 tanh(\\frac{2}{3}(x))$\n",
    "- Set target values; typically -1 and +1\n",
    "- Initialize the weights to random values; uniform random distribution with mean 0 and standard deviation $\\sigma_w = m^{-\\frac{1}{2}}$\n",
    "\n",
    "- If training set is large (more than a few hundred samples) and redundant, and if the task is classification, use stochastic gradient with careful tuning, or use stochastic diagonal Levenberg Marquardt method\n",
    "- If training set is not too large, or if the task is regression, use Conjugate Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

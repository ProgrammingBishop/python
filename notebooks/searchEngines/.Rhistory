train.positive.sd <- sapply(train.positive, sd, na.rm = TRUE)
train.negative.sd <- sapply(train.negative, sd, na.rm = TRUE)
# Get the normalization parameters
ptrain.num <- t(t(x.train) - train.positive.m)
ntrain.num <- t(t(x.train) - train.negative.m)
ptrain.std <- t(t(ptrain.num) / train.positive.sd)
ntrain.std <- t(t(ntrain.num) / train.negative.sd)
# Get gaussian log probability for each instance in regards to each class
# Getting probability for each instance for each class
# This is so we can compare probabilities and categorize instance for the most probable class
# Positive Class
ptrain.log <- -(1 / 2) *
rowSums( apply(ptrain.std, c(1, 2), function(x) x ^ 2), na.rm = TRUE ) +
( nrow(train.positive) / ( nrow(train.positive) + nrow(train.negative) ) )
# Negative Class
ntrain.log <- -(1 / 2) *
rowSums( apply(ntrain.std, c(1, 2), function(x) x ^ 2), na.rm = TRUE ) +
( nrow(train.negative) / ( nrow(train.positive) + nrow(train.negative) ) )
# Return the predictions via classification (classified via the most probable)
class.predictions <- ptrain.log         > ntrain.log
matches           <- class.predictions == y.train
# Return accuracy for the training set for iteration
train.score[iteration] <- sum(matches) / ( sum(matches) + sum(!matches) )
# Get the mean for the test set
ptest.num <- t(t(x.test) - train.positive.m)
ntest.num <- t(t(x.test) - train.negative.m)
# Get the sd for the test set
ptest.std <- t(t(ptest.num) / train.positive.sd)
ntest.std <- t(t(ntest.num) / train.negative.sd)
# Get gaussian log probability for each instance in regards to each class
# Positive Class
ptest.log <- -(1 / 2) *
rowSums( apply(ptest.std, c(1, 2), function(x) x ^ 2), na.rm = TRUE ) +
( nrow(train.positive) / ( nrow(train.positive) + nrow(train.negative) ) )
# Negative Class
ntest.log <- -(1 / 2) *
rowSums( apply(ntest.std, c(1, 2), function(x) x ^ 2), na.rm = TRUE ) +
( nrow(train.negative) / ( nrow(train.positive) + nrow(train.negative) ) )
# Evaluate the test set
class.positive <- ptest.log       > ntest.log
matches        <- class.positive == y.test
# Return accuracy for the test set for iteration
test.score[iteration] <- sum(matches) / ( sum(matches) + sum(!matches) )
}
test.score
mean(test.score)
mean(trial.score)
mean(train.score)
train.score <- array(dim = 10)
test.score  <- array(dim = 10)
for (iteration in 1:10)
{
# Seperate Data into Training and Test Sets
partition <- createDataPartition(df$V9, p = 0.80, list = FALSE)
# Create the test label vector and test class vector
df.test <- df[ -partition, ]
x.test  <- df.test[ , 1:8]
y.test  <- df.test$V9
# Create the train label vector and train class vector
df.train <- df[ partition, ]
x.train  <- df.train[ , 1:8]
y.train  <- df.train$V9
# Seperate training data into its class factors (1 = positive / 0 = negative)
splitter       <- y.train > 0
train.positive <- x.train[ splitter, ]
train.negative <- x.train[!splitter, ]
# Get the normal distributions for the features
# Get the mean for the training set features
train.positive.m <- sapply(train.positive, mean, na.action = na.omit)
train.negative.m <- sapply(train.negative, mean, na.action = na.omit)
# Get the sd for the training set feratures
train.positive.sd <- sapply(train.positive, sd, na.action = na.omit)
train.negative.sd <- sapply(train.negative, sd, na.action = na.omit)
# Get the normalization parameters
ptrain.num <- t(t(x.train) - train.positive.m)
ntrain.num <- t(t(x.train) - train.negative.m)
ptrain.std <- t(t(ptrain.num) / train.positive.sd)
ntrain.std <- t(t(ntrain.num) / train.negative.sd)
# Get gaussian log probability for each instance in regards to each class
# Getting probability for each instance for each class
# This is so we can compare probabilities and categorize instance for the most probable class
# Positive Class
ptrain.log <- -(1 / 2) *
rowSums( apply(ptrain.std, c(1, 2), function(x) x ^ 2), na.action = na.omit ) +
( nrow(train.positive) / ( nrow(train.positive) + nrow(train.negative) ) )
# Negative Class
ntrain.log <- -(1 / 2) *
rowSums( apply(ntrain.std, c(1, 2), function(x) x ^ 2), na.action = na.omit ) +
( nrow(train.negative) / ( nrow(train.positive) + nrow(train.negative) ) )
# Return the predictions via classification (classified via the most probable)
class.predictions <- ptrain.log         > ntrain.log
matches           <- class.predictions == y.train
# Return accuracy for the training set for iteration
train.score[iteration] <- sum(matches) / ( sum(matches) + sum(!matches) )
# Get the mean for the test set
ptest.num <- t(t(x.test) - train.positive.m)
ntest.num <- t(t(x.test) - train.negative.m)
# Get the sd for the test set
ptest.std <- t(t(ptest.num) / train.positive.sd)
ntest.std <- t(t(ntest.num) / train.negative.sd)
# Get gaussian log probability for each instance in regards to each class
# Positive Class
ptest.log <- -(1 / 2) *
rowSums( apply(ptest.std, c(1, 2), function(x) x ^ 2), na.action = na.omit ) +
( nrow(train.positive) / ( nrow(train.positive) + nrow(train.negative) ) )
# Negative Class
ntest.log <- -(1 / 2) *
rowSums( apply(ntest.std, c(1, 2), function(x) x ^ 2), na.action = na.omit ) +
( nrow(train.negative) / ( nrow(train.positive) + nrow(train.negative) ) )
# Evaluate the test set
class.positive <- ptest.log       > ntest.log
matches        <- class.positive == y.test
# Return accuracy for the test set for iteration
test.score[iteration] <- sum(matches) / ( sum(matches) + sum(!matches) )
}
mean(test.score)
train.score <- array(dim = 10)
test.score  <- array(dim = 10)
for (iteration in 1:10)
{
# Seperate Data into Training and Test Sets
partition <- createDataPartition(df$V9, p = 0.80, list = FALSE)
# Create the test label vector and test class vector
df.test <- df[ -partition, ]
x.test  <- df.test[ , 1:8]
y.test  <- df.test$V9
# Create the train label vector and train class vector
df.train <- df[ partition, ]
x.train  <- df.train[ , 1:8]
y.train  <- df.train$V9
# Seperate training data into its class factors (1 = positive / 0 = negative)
splitter       <- y.train > 0
train.positive <- x.train[ splitter, ]
train.negative <- x.train[!splitter, ]
# Get the normal distributions for the features
# Get the mean for the training set features
train.positive.m <- sapply(train.positive, mean, na.rm = TRUE)
train.negative.m <- sapply(train.negative, mean, na.rm = TRUE)
# Get the sd for the training set feratures
train.positive.sd <- sapply(train.positive, sd, na.rm = TRUE)
train.negative.sd <- sapply(train.negative, sd, na.rm = TRUE)
# Get the normalization parameters
ptrain.num <- t(t(x.train) - train.positive.m)
ntrain.num <- t(t(x.train) - train.negative.m)
ptrain.std <- t(t(ptrain.num) / train.positive.sd)
ntrain.std <- t(t(ntrain.num) / train.negative.sd)
# Get gaussian log probability for each instance in regards to each class
# Getting probability for each instance for each class
# This is so we can compare probabilities and categorize instance for the most probable class
# Positive Class
ptrain.log <- -(1 / 2) *
rowSums( apply(ptrain.std, c(1, 2), function(x) x ^ 2), na.rm = TRUE ) +
( nrow(train.positive) / ( nrow(train.positive) + nrow(train.negative) ) )
# Negative Class
ntrain.log <- -(1 / 2) *
rowSums( apply(ntrain.std, c(1, 2), function(x) x ^ 2), na.rm = TRUE ) +
( nrow(train.negative) / ( nrow(train.positive) + nrow(train.negative) ) )
# Return the predictions via classification (classified via the most probable)
class.predictions <- ptrain.log         > ntrain.log
matches           <- class.predictions == y.train
# Return accuracy for the training set for iteration
train.score[iteration] <- sum(matches) / ( sum(matches) + sum(!matches) )
# Get the mean for the test set
ptest.num <- t(t(x.test) - train.positive.m)
ntest.num <- t(t(x.test) - train.negative.m)
# Get the sd for the test set
ptest.std <- t(t(ptest.num) / train.positive.sd)
ntest.std <- t(t(ntest.num) / train.negative.sd)
# Get gaussian log probability for each instance in regards to each class
# Positive Class
ptest.log <- -(1 / 2) *
rowSums( apply(ptest.std, c(1, 2), function(x) x ^ 2), na.rm = TRUE ) +
( nrow(train.positive) / ( nrow(train.positive) + nrow(train.negative) ) )
# Negative Class
ntest.log <- -(1 / 2) *
rowSums( apply(ntest.std, c(1, 2), function(x) x ^ 2), na.rm = TRUE ) +
( nrow(train.negative) / ( nrow(train.positive) + nrow(train.negative) ) )
# Evaluate the test set
class.positive <- ptest.log       > ntest.log
matches        <- class.positive == y.test
# Return accuracy for the test set for iteration
test.score[iteration] <- sum(matches) / ( sum(matches) + sum(!matches) )
}
mean(train.score)
mean(test.score)
model <- train(as.factor(df$V9) ~ .,
data = train.set,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
train.set      <- prima.indians.fold[prima.partition.fold, ]
train.features <- prima.train.fold  [ , 1:8]
df <- fread('http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data')
partition <- createDataPartition(df$V9, p = .8, list = FALSE)
train.set      <- prima.indians.fold[prima.partition.fold, ]
partition <- createDataPartition(df$V9, p = .8, list = FALSE)
train.set      <- df[partition, ]
train.features <- df[ , 1:8]
test.set      <- df[-partition, ]
test.features <- df[ , 1:8]
nrow(as.data.frame(test.set))
model <- train(as.factor(df$V9) ~ .,
data = train.set,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
as.factor(df$V9)
df$V9
typeof(df$V9)
typeof(as.factor(df$V9))
model <- train(V9 ~ ., data = train.set,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(df$V9 ~ ., data = train.set,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(train.set$V9 ~ ., data = train.set,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(as.factor(train.set$V9) ~ ., data = train.set,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(as.factor(train.set$V9) ~ ., data = train.features,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(as.factor(train.set$V9) ~ ., data = train.features,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb')
model <- train(as.factor(train.set$V9), data = train.features,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(as.factor(train.set$V9) ~., data = train.features,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
View(train.features)
as.factor(train.set$V9)
train.set$V9
model <- train(as.factor(train.set$V9) ~ ., data = train.features,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
partition <- createDataPartition(df$V9, p = .8, list = FALSE)
train.set      <- df[partition, ]
train.features <- train.set[ , 1:8]
test.set      <- df[-partition, ]
test.features <- test.set[ , 1:8]
model <- train(as.factor(train.set$V9) ~ ., data = train.features,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(as.factor(train.set$V9) ~ ., data = train.features,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb')
train.features
train.set$V9
nrow(as.data.frame(train.set$V9))
model <- train(as.factor(train.set$V9) ~ ., data = train.features,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
r
model <- train(as.factor(train.set$V9) ~ ., data = train.set,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(as.factor(train.set$V9) ~ ., data = train.features,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(as.factor(train.set$V9) ~ ., data = train.features[c(1, 2)],
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(as.factor(train.set$V9) ~ ., data = train.set,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
View(train.set)
partition <- createDataPartition(df$V9, p = .8, list = FALSE)
train.set      <- df[partition, ]
train.features <- train.set[ , 1:8]
test.set      <- df[-partition, ]
test.features <- test.set[ , 1:8]
model <- train(as.factor(train.set$V9) ~ ., data = train.set,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(as.factor(train.set$V9) ~ ., data = train.set[,1:8],
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)))
model <- train(as.factor(train.set$V9) ~ ., data = train.set,
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)),
na.action = na.omit)
model <- train(train.features, as.factor(train.set$V9),
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)),
na.action = na.omit)
predictions <- predict(model, test.features)
confusionMatrix(prima.predictions.fold, prima.class.test.fold)
confusionMatrix(predictions, test.set$V9)
partition <- createDataPartition(df$V9, p = .8, list = FALSE)
train.set      <- df[partition, ]
test.set      <- df[-partition, ]
test.features <- test.set[ , 1:8]
model <- train(train.features, as.factor(train.set$V9),
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)),
na.action = na.omit)
train.features <- train.set[ , 1:8]
predictions <- predict(model, test.features)
confusionMatrix(predictions, test.set$V9)
partition <- createDataPartition(df$V9, p = .8, list = FALSE)
train.set      <- df[partition, ]
train.features <- train.set[ , 1:8]
test.set      <- df[-partition, ]
test.features <- test.set[ , 1:8]
model <- train(train.features, as.factor(train.set$V9),
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)),
na.action = na.omit)
predictions <- predict(model, test.features)
confusionMatrix(predictions, test.set$V9)
predictions <- predict(test.features, model, na.rm)
predictions <- predict(test.features, model)
typeof(test.features)
View(test.features)
View(model)
predictions <- predict(model, test.features, na.rm)
predictions <- predict(model, test.features, na.rm = TRUE)
confusionMatrix(predictions, test.set$V9)
partition <- createDataPartition(df$V9, p = .8, list = FALSE)
train.set      <- df[partition, ]
train.features <- train.set[ , 1:8]
test.set      <- df[-partition, ]
test.features <- test.set[ , 1:8]
model <- train(train.features, as.factor(train.set$V9),
trControl = trainControl(method = 'cv', number = 10),
method = 'nb',
tuneGride = expand.grid(.fl = c(0), .usekernel = c(FALSE)),
na.action = na.omit)
predictions <- predict(model, test.features, na.rm = TRUE)
confusionMatrix(predictions, test.set$V9)
nrow(as.data.frame(train.positive.m))
nrow(as.data.frame(train.positive))
nrow(as.data.frame(train.negative))
ptrain.num
ptrain.std
ptrain.log
View(as.data.frame(ptrain.log))
View(as.data.frame(ptrain.log * ptrain.log))
View(as.data.frame(ptrain.log * ptrain.log / 100))
ptrain.log <- -(1 / 2) *
rowSums( apply(ptrain.std, c(1, 2), function(x) x ^ 2), na.rm = TRUE ) +
( nrow(train.positive) / ( nrow(train.positive) + nrow(train.negative) ) )
ntrain.log <- -(1 / 2) *
rowSums( apply(ntrain.std, c(1, 2), function(x) x ^ 2), na.rm = TRUE ) +
( nrow(train.negative) / ( nrow(train.positive) + nrow(train.negative) ) )
class.predictions
matches
matches           <- class.predictions == y.train
matches
model
model$results
model$finalModel
predictions
test.features
predictions$class
library(pander)
b1.table <- data.frame( c("2620938", "3951334", "2448106", "3116998", "2180755",
"3231652", "2630683", "3441665", "2441042", "4021764") )
pandoc.table(b1.table, style = "rmarkdown", justify = "left", plain.ascii = TRUE,
row.names = c("airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"),
col.names = c("Error"))
b1.table <- data.frame( c(   "0.0000", "1683.6354", "1605.0243", "1905.5353", "2148.7634", "1965.2215", "2445.6797", "1663.6459",  "945.5411", "1449.0949"),
c("1605.0243",    "0.0000",  "886.2367", "1027.6498", "1143.0814", "1216.0794", "1191.1920",  "950.7861", "1303.4665",  "949.9958"),
c("1905.5353",  "886.2367",    "0.0000",  "517.3115",  "601.2503",  "701.4682",  "913.7475",  "418.2763", "1557.7150", "1416.6747"),
c("2148.7634", "1027.6498",  "517.3115",    "0.0000",  "469.7917",  "412.1817",  "677.4920",  "596.3767", "1851.2145", "1676.4679"),
c("1965.2215", "1143.0814",  "601.2503",  "469.7917",    "0.0000",  "617.6971",  "460.5109",  "684.3469", "2065.6217", "1830.7409"),
c("2445.6797", "1216.0794",  "701.4682",  "412.1817",  "617.6971",    "0.0000",  "828.5811",  "843.6721", "1897.5918", "1880.2438"),
c("1663.6459", "1191.1920",  "913.7475",  "677.4920",  "460.5109",  "828.5811",    "0.0000",  "948.7040", "2249.1998", "1913.2409"),
c( "945.5411",  "950.7861",  "418.2763",  "596.3767",  "684.3469",  "843.6721",  "948.7040",    "0.0000", "1660.2681", "1347.3341"),
c("1449.0949", "1303.4665", "1557.7150", "1851.2145", "2065.6217", "1897.5918", "2249.1998", "1660.2681",    "0.0000", "1066.9416"),
c("1449.0949",  "949.9958", "1416.6747", "1676.4679", "1830.7409", "1880.2438", "1913.2409", "1347.3341", "1066.9416",    "0.0000") )
pandoc.table( b1.table, style = "rmarkdown", justify = "left", plain.ascii = TRUE )
b1.table <- data.frame( c(   "0.0000", "1683.6354", "1605.0243", "1905.5353", "2148.7634", "1965.2215", "2445.6797", "1663.6459",  "945.5411", "1449.0949"),
c("1605.0243",    "0.0000",  "886.2367", "1027.6498", "1143.0814", "1216.0794", "1191.1920",  "950.7861", "1303.4665",  "949.9958"),
c("1905.5353",  "886.2367",    "0.0000",  "517.3115",  "601.2503",  "701.4682",  "913.7475",  "418.2763", "1557.7150", "1416.6747"),
c("2148.7634", "1027.6498",  "517.3115",    "0.0000",  "469.7917",  "412.1817",  "677.4920",  "596.3767", "1851.2145", "1676.4679"),
c("1965.2215", "1143.0814",  "601.2503",  "469.7917",    "0.0000",  "617.6971",  "460.5109",  "684.3469", "2065.6217", "1830.7409"),
c("2445.6797", "1216.0794",  "701.4682",  "412.1817",  "617.6971",    "0.0000",  "828.5811",  "843.6721", "1897.5918", "1880.2438"),
c("1663.6459", "1191.1920",  "913.7475",  "677.4920",  "460.5109",  "828.5811",    "0.0000",  "948.7040", "2249.1998", "1913.2409"),
c( "945.5411",  "950.7861",  "418.2763",  "596.3767",  "684.3469",  "843.6721",  "948.7040",    "0.0000", "1660.2681", "1347.3341"),
c("1449.0949", "1303.4665", "1557.7150", "1851.2145", "2065.6217", "1897.5918", "2249.1998", "1660.2681",    "0.0000", "1066.9416"),
c("1449.0949",  "949.9958", "1416.6747", "1676.4679", "1830.7409", "1880.2438", "1913.2409", "1347.3341", "1066.9416",    "0.0000") )
pandoc.table( b1.table, style = "rmarkdown", justify = "left", plain.ascii = TRUE )
b1.table <- data.frame( c(   "0.0000", "1683.6354", "1605.0243", "1905.5353", "2148.7634", "1965.2215", "2445.6797", "1663.6459",  "945.5411", "1449.0949"),
c("1605.0243",    "0.0000",  "886.2367", "1027.6498", "1143.0814", "1216.0794", "1191.1920",  "950.7861", "1303.4665",  "949.9958"),
c("1905.5353",  "886.2367",    "0.0000",  "517.3115",  "601.2503",  "701.4682",  "913.7475",  "418.2763", "1557.7150", "1416.6747"),
c("2148.7634", "1027.6498",  "517.3115",    "0.0000",  "469.7917",  "412.1817",  "677.4920",  "596.3767", "1851.2145", "1676.4679"),
c("1965.2215", "1143.0814",  "601.2503",  "469.7917",    "0.0000",  "617.6971",  "460.5109",  "684.3469", "2065.6217", "1830.7409"),
c("2445.6797", "1216.0794",  "701.4682",  "412.1817",  "617.6971",    "0.0000",  "828.5811",  "843.6721", "1897.5918", "1880.2438"),
c("1663.6459", "1191.1920",  "913.7475",  "677.4920",  "460.5109",  "828.5811",    "0.0000",  "948.7040", "2249.1998", "1913.2409"),
c( "945.5411",  "950.7861",  "418.2763",  "596.3767",  "684.3469",  "843.6721",  "948.7040",    "0.0000", "1660.2681", "1347.3341"),
c("1449.0949", "1303.4665", "1557.7150", "1851.2145", "2065.6217", "1897.5918", "2249.1998", "1660.2681",    "0.0000", "1066.9416"),
c("1449.0949",  "949.9958", "1416.6747", "1676.4679", "1830.7409", "1880.2438", "1913.2409", "1347.3341", "1066.9416",    "0.0000") )
pandoc.table( b1.table, style = "rmarkdown", justify = "left", plain.ascii = TRUE,
row.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
col.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10") )
devtools::install_github("rstudio/rmarkdown")
devtools::install_url("http://cran.r-project.org/src/contrib/rmarkdown0.5.1.tar.gz")
b1.table <- data.frame( c(   "0.0000", "1683.6354", "1605.0243", "1905.5353", "2148.7634", "1965.2215", "2445.6797", "1663.6459",  "945.5411", "1449.0949"),
c("1605.0243",    "0.0000",  "886.2367", "1027.6498", "1143.0814", "1216.0794", "1191.1920",  "950.7861", "1303.4665",  "949.9958"),
c("1905.5353",  "886.2367",    "0.0000",  "517.3115",  "601.2503",  "701.4682",  "913.7475",  "418.2763", "1557.7150", "1416.6747"),
c("2148.7634", "1027.6498",  "517.3115",    "0.0000",  "469.7917",  "412.1817",  "677.4920",  "596.3767", "1851.2145", "1676.4679"),
c("1965.2215", "1143.0814",  "601.2503",  "469.7917",    "0.0000",  "617.6971",  "460.5109",  "684.3469", "2065.6217", "1830.7409"),
c("2445.6797", "1216.0794",  "701.4682",  "412.1817",  "617.6971",    "0.0000",  "828.5811",  "843.6721", "1897.5918", "1880.2438"),
c("1663.6459", "1191.1920",  "913.7475",  "677.4920",  "460.5109",  "828.5811",    "0.0000",  "948.7040", "2249.1998", "1913.2409"),
c( "945.5411",  "950.7861",  "418.2763",  "596.3767",  "684.3469",  "843.6721",  "948.7040",    "0.0000", "1660.2681", "1347.3341"),
c("1449.0949", "1303.4665", "1557.7150", "1851.2145", "2065.6217", "1897.5918", "2249.1998", "1660.2681",    "0.0000", "1066.9416"),
c("1449.0949",  "949.9958", "1416.6747", "1676.4679", "1830.7409", "1880.2438", "1913.2409", "1347.3341", "1066.9416",    "0.0000") )
pandoc.table( b1.table, style = "rmarkdown", justify = "right", plain.ascii = TRUE,
row.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
col.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10") )
library(pander)
pandoc.table( b1.table, style = "rmarkdown", justify = "right", plain.ascii = TRUE,
row.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
col.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10") )
b1.table <- data.frame( c(   "0.0000", "1683.6354",  "1605.0243", "1905.5353", "2148.7634", "1965.2215", "2445.6797", "1663.6459",  "945.5411", "1449.0949"),
c("1605.0243",    "0.0000",  "886.2367", "1027.6498", "1143.0814", "1216.0794", "1191.1920",  "950.7861", "1303.4665",  "949.9958"),
c("1905.5353",  "886.2367",    "0.0000",  "517.3115",  "601.2503",  "701.4682",  "913.7475",  "418.2763", "1557.7150", "1416.6747"),
c("2148.7634", "1027.6498",  "517.3115",    "0.0000",  "469.7917",  "412.1817",  "677.4920",  "596.3767", "1851.2145", "1676.4679"),
c("1965.2215", "1143.0814",  "601.2503",  "469.7917",    "0.0000",  "617.6971",  "460.5109",  "684.3469", "2065.6217", "1830.7409"),
c("2445.6797", "1216.0794",  "701.4682",  "412.1817",  "617.6971",    "0.0000",  "828.5811",  "843.6721", "1897.5918", "1880.2438"),
c("1663.6459", "1191.1920",  "913.7475",  "677.4920",  "460.5109",  "828.5811",    "0.0000",  "948.7040", "2249.1998", "1913.2409"),
c( "945.5411",  "950.7861",  "418.2763",  "596.3767",  "684.3469",  "843.6721",  "948.7040",    "0.0000", "1660.2681", "1347.3341"),
c("1449.0949", "1303.4665", "1557.7150", "1851.2145", "2065.6217", "1897.5918", "2249.1998", "1660.2681",    "0.0000", "1066.9416"),
c("1449.0949",  "949.9958", "1416.6747", "1676.4679", "1830.7409", "1880.2438", "1913.2409", "1347.3341", "1066.9416",    "0.0000") )
pandoc.table( b1.table, style = "rmarkdown", justify = "right", plain.ascii = TRUE,
row.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
col.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10") )
b1.table <- data.frame( c(   "0.0000", "1683.6354", "1605.0243", "1905.5353", "2148.7634", "1965.2215", "2445.6797", "1663.6459",  "945.5411", "1449.0949"),
c("1683.6354",    "0.0000",  "886.2367", "1027.6498", "1143.0814", "1216.0794", "1191.1920",  "950.7861", "1303.4665",  "949.9958"),
c("1905.5353",  "886.2367",    "0.0000",  "517.3115",  "601.2503",  "701.4682",  "913.7475",  "418.2763", "1557.7150", "1416.6747"),
c("2148.7634", "1027.6498",  "517.3115",    "0.0000",  "469.7917",  "412.1817",  "677.4920",  "596.3767", "1851.2145", "1676.4679"),
c("1965.2215", "1143.0814",  "601.2503",  "469.7917",    "0.0000",  "617.6971",  "460.5109",  "684.3469", "2065.6217", "1830.7409"),
c("2445.6797", "1216.0794",  "701.4682",  "412.1817",  "617.6971",    "0.0000",  "828.5811",  "843.6721", "1897.5918", "1880.2438"),
c("1663.6459", "1191.1920",  "913.7475",  "677.4920",  "460.5109",  "828.5811",    "0.0000",  "948.7040", "2249.1998", "1913.2409"),
c( "945.5411",  "950.7861",  "418.2763",  "596.3767",  "684.3469",  "843.6721",  "948.7040",    "0.0000", "1660.2681", "1347.3341"),
c("1449.0949", "1303.4665", "1557.7150", "1851.2145", "2065.6217", "1897.5918", "2249.1998", "1660.2681",    "0.0000", "1066.9416"),
c("1449.0949",  "949.9958", "1416.6747", "1676.4679", "1830.7409", "1880.2438", "1913.2409", "1347.3341", "1066.9416",    "0.0000") )
pandoc.table( b1.table, style = "rmarkdown", justify = "right", plain.ascii = TRUE,
row.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
col.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10") )
3 + 5
3 + 2
x = 1:100
y = x
for (i in 1:length(x))
{
if (i % 2 == 0)
{
y[i] = y[i] - 10
}
else
{
Y[i] = y[i] + 5
}
}
y
x = 1:100
y = x
for (i in 1:length(x))
{
if (i %% 2 == 0)
{
y[i] = y[i] - 10
}
else
{
Y[i] = y[i] + 5
}
}
y
?std()
quiz_list = list(
x = c(1, 2),
y = "Hello Quiz Taker",
z = "z"
)
quiz_list[[3]]
quiz_list[3]
quiz_list['3']
?hist()
setwd(D:\\Documents\\MachineLearning\\searchEngines)
setwd(D:\Documents\MachineLearning\searchEngines)
setwd("D:\\Documents\\MachineLearning\\searchEngines")
bm25 = read.table('bm25.avg_p.txt')$V1
inl2 = read.table('inl2.avg_p.txt')$V1
t.test(bm25, inl2, paired=T)

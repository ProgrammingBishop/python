{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 Linear Model Selection & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Accuracy: least squares will tend to have low bias. If $n >> p$ then the least squares estimate will also have low variance and therefore perform well on test observations. If $n$ is not large then there could be a lot of variability in the least squares fit, which results in overfitting and poor predictions on future observations. If $p > n$ then the variance is infinite and the method cannot be used at all.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Subset Selection\n",
    "\n",
    "###### Best Subset Selection\n",
    "Fit a seperate least squares regression for each possible combination of the $p$ predictors. We will fit all the $p$ models that contain exactly one predictor, all:\n",
    "$$\\frac{p(p - 1)}{2}$$\n",
    "\n",
    "that contain exactly 2 predictors, and so on. Then we select the best model from all of these possible models. \n",
    "\n",
    "###### Algorithm \n",
    "- Let $M_0$ denote the null model, which contains no predictors (predicts sample mean for each observation).\n",
    "- For $k = 1, ..., p$\n",
    "    - Fit all (p choose k) models that contain exactly $k$ predictors\n",
    "    - Pick the best among these and call it $M_k$. Here best is defined as having the smallest RSS, or largest $R^2$.\n",
    "- Select a single best model from among the $M_0, ..., M_p$ using cross-validated prediction error, $C_p$, AIC, BIC, or adjusted $R^2$.\n",
    "\n",
    "It is best to use Best subset selection when $p < 40$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stepwise Selection\n",
    "\n",
    "###### Forward Stepwise Selection\n",
    "- Let $M_0$ denote the null model, which contains no predictors\n",
    "- For $k = 0, ..., p - 1$\n",
    "    - Consider all p - k models that augment the predictors in $M_k$ with one additional predictor\n",
    "    - Choose the best among these p - k models, and call it $M_{k + 1}$. Here the best is defined as having the smallest RSS or highest $R^2$.\n",
    "- Select a single best model from the $M_0, ..., M_p$ using cross-validation prediction error. \n",
    "\n",
    "Doing this we will end up with:\n",
    "$$\\frac{1 + p(p + 1)}{2}$$\n",
    "\n",
    "###### Backward Stepwise Selection\n",
    "-Let $M_0$ represent the full model with all of the predictors $p$.\n",
    "- For $k = p, p-1, ..., 1$\n",
    "    - Consider all $k$ models that contain all but one of the predictors in $M_k$ for a total of $k - 1$ predictors.\n",
    "    - Choose the best among these $k$ models and all it $M_{k - 1}$. Here best is defined as having the smallest RSS or $R^2$.\n",
    "- Select the best model from all models using cross-validation prediction error. \n",
    "\n",
    "Backward requires that the number of $p$ is smaller than $n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $C_p$, AIC, BIC, Adjusted $R^2$\n",
    "$$C_p = \\frac{(RSS + 2d \\hat \\sigma^2)}{n}$$\n",
    "\n",
    "where $\\hat \\sigma^2$ is an estimate of the vsriance of the error $\\epsilon$ associated with each response measurement. Usually this is done using the full model. The penalty $2d \\hat \\sigma^2$ will increase as the number of predictors increase to adjust for the decrease in training RSS. \n",
    "\n",
    "$$AIC = \\frac{(RSS + 2d \\hat \\sigma^2)}{n \\hat \\sigma^2}$$\n",
    "\n",
    "is defined for a large class of models fit by maximum likelihood. It is important to note that Mallow's $C_p$ and AIC are proportional.\n",
    "\n",
    "$$BIC = \\frac{(RSS + log(n) d  \\hat \\sigma^2)}{n \\hat \\sigma^2}$$\n",
    "\n",
    "and BIC tends to take on a small value for a model with low test error (like AIC). BIC places a penalty for models with many variables and favors smaller models. \n",
    "\n",
    "$$Adjusted R^2 = 1 - \\frac{ \\frac{RSS}{(n - d - 1)} }{ \\frac{TSS}{(n - 1)} }$$\n",
    "\n",
    "and a model with a large Adjusted $R^2$ indicates a model with a small test error. This model will also only include the correct variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation and Cross-Validation\n",
    "These methods have a leg up over the above criterion simply because these make less assumptions and evaluate the error directly on the test data. \n",
    "\n",
    "###### One-Standard-Error Rule\n",
    "Calculate the standard error of the estimated test MSE for each model size, then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. If a set of models appear to be equally good, it makes sense to just choose the simpler model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Shrinkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shrink the model coefficients toward 0 minimizes the variance.\n",
    "\n",
    "#### Ridge Regression\n",
    "The coefficient estimates $\\hat \\beta^R$ are the vales that minimize:\n",
    "$$\\sum_{i = 1}^n\\ (y_i - \\beta_0 - \\sum_{j = 1}^p\\ \\beta_j\\ x_{ij})^2 + \\lambda\\ \\sum_{j = 1}^p\\ \\beta_j^2\\ = RSS + \\lambda\\ \\sum_{j = 1}^p\\ \\beta_j^2$$\n",
    "\n",
    "where $\\lambda \\ge 0$ is a tuning parameter. The shrinkage parameter $\\lambda\\ \\sum_{j = 1}^p\\ \\beta_j^2$ is small when the coefficients $\\beta$ are near 0 (null model). When $\\lambda$ is 0 then the tuning parameter has no effect and the ridge regression will produce the least squares estimates. As $\\lambda -> \\infty$ the impact of this shrinkage parameter grows and the coefficient approaches 0.\n",
    "\n",
    "Ridge Regression will produce a seperate set of Regression coefficient estimates $\\hat \\beta^R_\\lambda$ for each value of $\\lambda$.\n",
    "\n",
    "If the variables have been centered with mean set to 0 before ridge regression is performed, then the estimated intercept will take the form $\\hat \\beta_0 = \\bar y = \\sum_{i = 1}^n\\ \\frac{y_i}{n}$\n",
    "\n",
    "In Ridge Regression multiplying a given predictor by a constant can change substantially. THe standard least squares duscussed in Chapter 3 are equivariant: multiplying a predictor by a constant leads to a scaling of the least squares coefficient estimates by a factor of $\\frac{1}{c}$. With this in mind:\n",
    "$$X_j \\hat \\beta_{j, \\lambda}^R$$ will not only depend on the value of $\\lambda$, but also onthe scaling of the $j$th predictor. It could also matter on the scaling of the other predictors. \n",
    "\n",
    "This is why it is important to apply Ridge Regression after standardizing the predictors using the following formula:\n",
    "$$\\tilde x_{ij} = \\frac{x_{ij}}{\\sqrt( \\frac{1}{n}\\ \\sum_{i = 1}^n\\ (x_{ij} - \\bar x_{j})^2 )}$$\n",
    "\n",
    "The denominator is essentially th estimated standard deviation of the $j$th predictor. This will make all of the standardized predcictors have a standard deviation of 1 thus making the final fit not dependent on the scale of the predictors. \n",
    "\n",
    "As $\\lambda$ increases the flexibility of the Ridge Regression fir decreases leading to a decreased variance but increased bias. So small $\\lambda$ leads to high variance, and as $\\lambda$ increases the variance decreases at the expense of slight increase in the bias. \n",
    "\n",
    "Ridge Regression will work wonders where the least squares estimate has high variance, which is when the number of predictors is close to the number of observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso\n",
    "One downfall to Ridge Regression is that it shrinks all the coefficients toward 0, but never set them to 0 (unless $\\lambda = \\infty$). This results in all of the predictors being included in the final model. Lasso overcomes this shortcoming by using the following formula to calculate the lasso coefficients $\\hat \\beta_{\\lambda}^L$:\n",
    "$$\\sum_{i = 1}^n\\ (y_i - \\beta_0 - \\sum_{j = 1}^p\\ \\beta_j x_{ij})^2 + \\lambda\\ \\sum_{j = 1}^p\\ \\mathbf{|\\beta_j|} = RSS + \\lambda\\ \\sum_{j = 1}^p\\ \\mathbf{|\\beta_j|}$$\n",
    "\n",
    "The only difference in this formula from the Ridge Regression is that instead of square of beta we are getting the absolute value of beta (The L1 penalty). This allows some of the coefficients to be exactly 0 when $\\lambda$ is sufficiently large. In other words, removes the predictors entirely. So lasso is used for variable selection and results in sparse models (subset of all the predictors).\n",
    "\n",
    "When performing Lasso we are trying to find the set of coefficients that result in the smallest RSS, subject to a budget $s$ for how large $\\sum_{j = 1}^p\\ \\mathbf{|\\beta_j|}$ can be. When $s$ is large the budget is not very restrictive and so coefficient estimates can be quite large. If $s$ is small, then the weight must be small in order to avoid violating the budget. \n",
    "\n",
    "Consider if the budget was set through an indicator function:\n",
    "$$\\sum_{j = 1}^p\\ I(\\beta_j \\neq 0) \\le s$$\n",
    "\n",
    "In other words the value is 1 if not equal to zero and zero otherwise. This means find the smallest RSS with the constraint that no more than $s$ coefficients can be nonzero. For large enough $p$ though this computation is not feasible. \n",
    "\n",
    "###### Variable Selection Property of Lasso \n",
    "A large $s$ is equivelant to $\\lambda = 0$. It is at this point the Ridge Regression and Lasso will be equal to Least Squares. \n",
    "\n",
    "To visualize this, picture a cartesian map in which $\\beta_1$ is the x-axis and $\\beta_2$ is the y-axis. For Lasso there is a diamond-shaped area around the origin that represents the regression constraints, and for Ridge there is a circle. For a Least Squares estimate $\\hat \\beta$ will lie outside these regions near the origin. Around this point are ellipses that represent regions of constant RSS. As $s$ grows and/or $\\lambda$ goes to 0, this estimate for $\\hat \\beta$ will shrink toward the origin and therefore near the shaded regions. \n",
    "\n",
    "Because Ridge is a circular region, the ellipse will never collide with the circular region on the x or y axis and therefore $\\hat \\beta$ will never shrink to 0. Because Lasso is a diamond, the ellipse can collide with the region on the x or y axis and therefore hit 0. \n",
    "\n",
    "As $\\lambda$ increases the variance decreases and the bias increases. \n",
    "\n",
    "Lasso can perform better when there are a small number of predictors that have a substantial coefficients and the remaining have small coefficients or coefficients that equal 0. Ridge will perform better when the response is a function of many predictors all of which with predictors of equal size. Usually though, this determination cannot be made apriori so cross-validation is used to determine which regression is more optimal. \n",
    "\n",
    "###### Special Case for Ridge Regression and the Lasso\n",
    "Consider $n = p$ and $X$ is a diagonal matrix with 1's on the diagonal and 0's everywhere else. Assume further that we are performing regression without an intercept. \n",
    "\n",
    "Least Squares Solution given by:\n",
    "$$\\hat \\beta_j = y_j$$\n",
    "\n",
    "Ridge Regression finds coefficients so the following is minimized:\n",
    "$$\\sum_{j = 1}^p\\ (y_j - \\beta_j)^2 + \\lambda\\ \\sum_{j = 1}^p\\ \\mathbf{|\\beta_j|}$$\n",
    "\n",
    "Lasso amounts to find the coefficients so that the following is minimized:\n",
    "$$\\sum_{j = 1}^p\\ \\mathbf{|\\beta_j|}\\ (y_j - \\beta_j)^2 + \\lambda\\ \\sum_{j = 1}^p\\ \\mathbf{|\\beta_j|}\\ \\mathbf{|\\beta_j|}$$\n",
    "\n",
    "One can show the Ridge Regression estimates take the form:\n",
    "$$\\hat \\beta_j^R = \\frac{y_j}{(1 + \\lambda)}$$\n",
    "\n",
    "and Lasso takes the form:\n",
    "- $\\beta_j^L =$\n",
    "    - $y_j - \\frac{\\lambda}{2}$ if $y_j > \\frac{\\lambda}{2}$\n",
    "    - $y_j + \\frac{\\lambda}{2}$ if $y_j < -\\frac{\\lambda}{2}$\n",
    "    - $0$ if $\\mathbf{|y_j|} \\le \\frac{\\lambda}{2}$\n",
    "    \n",
    "Ridge shrinks each least square coefficient toward zero by the same proportion. Lasso shrinks based on a constant amount $\\frac{\\lambda}{2}$ and is known as soft-thresholding. Least squares estimates that are less than $\\frac{\\lambda}{2}$ are shrunken entirely to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting a Tuning Parameter\n",
    "Choose a grid of $\\lambda$ values and compute the cross-validation for each. Select the tuning parameter for which the cross-validation error is smallest. Then the model is refit using all available observations and the selected value of the tuning parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Dimension Reduction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Z_1, ..., Z_m$ represent $M < p$ linear combinations of original $p$ predictors:\n",
    "$$Z_m = \\sum_{j = 1}^p\\ \\phi_{jm}\\ X_j$$\n",
    "\n",
    "where $\\phi$ represent some constants $\\phi_{1m}, ..., \\phi_{pm}$. We can fit the linear regression model:\n",
    "$$y_i = \\theta_0 + \\sum_{m = 1}^M\\ \\theta_m\\ z_{im} + \\epsilon_i$$\n",
    "\n",
    "where $i = 1, ..., n$, using least squares. If the values for $\\phi$ are chosen wisely, then dimension reduction approaches can outperform least sqaures. This reduces the $p + 1$ coefficients to a subset of $m + 1$ coefficients. Notice:\n",
    "$$\\sum_{m = 1}^M\\ \\theta_m\\ z_{im} = \\sum_{j = 1}^p\\ \\beta_j\\ x_{ij}$$\n",
    "\n",
    "where:\n",
    "$$\\beta_j = \\sum_{m = 1}^M\\ \\theta_m\\ \\phi_{jm}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principle Component Regression \n",
    "First principle component:\n",
    "$$Z_1 = \\sum_{j = 1}^p\\ \\sum_{i = 1}^n\\ \\phi_{ij}\\ * (y_{ij} - \\bar y)$$\n",
    "\n",
    "where the $\\phi$ are the loadings, which define the direction. The idea is that of every linear combination of the predicotrs that = 1 when summed, the linear combination of interest is what leads to the largest variance. \n",
    "\n",
    "The second principle component $Z_2$ is a linear combination of the variablesthat is uncorrelated to $Z_1$ and has the largest vairance subject to this constraint. in other words, the second principle component must be perpendicular (orthogonal) to the first. \n",
    "\n",
    "###### Principle Component Regression Approach (PCR)\n",
    "Construct the first $M$ components and then using these components as predictors in a linear regression model that is fit using least squares. PCR is very similar to Ridge and the components $M$ are chosen through cross-validation. When performing PCR it is recommended to standardize each predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial Least Squares\n",
    "One drawback of PCA/PCR is that the directions selected to best represent the predictors is sometimes the best to predict the response. \n",
    "\n",
    "Partial Least Squares identifies a linear combination subset of the predictors that are selected via least squaresusing the $M$ new features. PLS is better than PCR because it selects new features based on the response $Y$. Essentially choose directions the explain the both the response and the predictors. \n",
    "\n",
    "First standardize the predictors, then compute the first direction by setting each $\\phi_{j1}$ equal to the coefficient from the simple linear regressionof $Y$ onto $X_j$. This coefficient is related to the correlation between $Y$ and $X_j$. PLS will place the highest weights on the predictors most related to the response. \n",
    "\n",
    "In practice PLS does not perform better than Ridge Regression of PCR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Considerations in High Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Dimensional Data\n",
    "High dimensional are data sets that contain more predictors than observations. The below only applies when $p$ is only slightly larger than $n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What Goes Wrong in High Dimensions?\n",
    "Least squares cannot be performed. Least squares will result in a regression coefficient that is a perfect fit so the residuals are 0 and the data will be overfit because least squares creates a model that has a regression line that is too flexible. \n",
    "\n",
    "More predictors leads to more variance, and higher variance makes it so using Mallow's $C_p$, AIC, BIC, and adjusted $R^2$ are not appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression in High Dimensions\n",
    "Stepwise, Ridge, and Lasso Regression are all valid approaches to selecting predictors in higher dimensions. \n",
    "\n",
    "Three important points for high dimensions:\n",
    "- Regularization of shrinkage plays an important role in high-dimensional problems.\n",
    "- Appropriate tuning parameter selection is crucial for good predictor performance.\n",
    "- Test error increases as dimensionality of the problem increases, unless additional features are truly associated with the response. (Curse of Dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting Results in High Dimensions\n",
    "Selecting a subset of $m$ predictors when the dimensionality $p$ is high does not mean we ave found the best model; it means we have selected one of possibly many models. The other models may even be non-verlapping and possible perform similarly, better, or worse. With high $p$ we can never know which coefficients are truly predictive in the response. Never use Sum-of-Square Errors, p-values, $R^2$, or other traditional measures of model fit on the training data as evidence of a good model fit in a high-dimensional data setting.\n",
    "\n",
    "Instead report results on an independent test set, or cross-validation errors. This makes it so the MSE or $R^2$ is valid for an independent test set, but the same measures on the training set is not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Basics of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Trees\n",
    "- Terminal Nodes (Leaves): endpoints of a tree's branch. \n",
    "- Branches: segments of the trees that connect nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction via Stratification of the Feature Space\n",
    "Steps to building a Regression Tree:\n",
    "- Divide predictor space into $J$ distinct and non-overlapping regions $R_1, ..., R_j$.\n",
    "- For every observation that falls into the region $R_j$, we make the same prediction. This is the mean of the response values for the training observations in $R_j$. These mean response labels will be what label new data takes on should that new data point fall into that corresponding region $R_j$.\n",
    "\n",
    "The goal is to find regions that minimize the RSS given by:\n",
    "$$\\sum_{j = 1}^J\\ \\sum_{i \\in R_j}\\ (y_i - \\hat y_{R_j})^2$$\n",
    "\n",
    "where $\\hat y_{R_j}$ is the mean response for the training observations within the $j$th box. \n",
    "\n",
    "For computational purposes we use a top-down greedy approach known as recursive binary splitting. We spliteach feature $X_j$ so that the regions to be split - $\\{ X | X_j < s \\}$ and $\\{ X | X_j \\ge s \\}$ leads to the grestest reduction in RSS, where the regions equal $R_1$ and $R_2$ respectively. This minimizes the equation:\n",
    "$$\\sum_{i: x_i \\in R_1(j, s)}\\ (y_i - \\hat y_i)^2 + \\sum_{i: x_i \\in R_2(j, s)}\\ (y_i - \\hat y_i)^2$$\n",
    "\n",
    "where $j$ represents the region and $s$ represents the branch for that region (i.e. left or right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Pruning\n",
    "A resulting tree created from a recursive process from above may threaten the possibility for overfitting. One thing we could do is only split when the achieved RSS decrease is above a pre-determined threshold. \n",
    "\n",
    "A strategy to do this is to grow a very large tree $T_0$ and then prune it to obtain a subtree. We want a subtree that results in the lowest test error rate. We can do this with cross-validation or the validation set approach. The most computationally efficient way to do this is through cost complexity pruning (aka weakest link pruning).\n",
    "\n",
    "This considers a sequence of trees indexed by non-negative tuning parameter $\\alpha$. For each value of $\\alpha$ there corresponds a subtree $T \\subset T_0$ such that:\n",
    "$$\\sum_{m = 1}^{\\mathbf{|T|}}\\ \\sum_{i: x_i \\in R_m}\\ (y_i - \\hat y_{R_m})^2 + \\alpha\\ \\mathbf{|T|}$$\n",
    "\n",
    "is as small as possible. $\\mathbf{|T|}$ is the number of terminal nodes of the tree $T$, $R_m$ is the rectangle (subset of predictor space) corresponding to the $m$th terminal node, and $\\hat y_{R_m}$ is the predicted response associated with $R_m$ (mean of the training observations in $R_m$). $\\alpha$ controls the trade-off between the subtree's complexity and its fit to the training data. \n",
    "\n",
    "If $\\alpha = 0$ the the subtree $T$ will equal $T_0$ because then the formula above just measures the training error. As $\\alpha$ increases there is a price to pay for a tree that has many terminal nodes, so the quantity will be minimized for a smaller subtree. So as $\\alpha$ increases branches get pruned from the tree in a nested and predictable fashion. We choose a value of $\\alpha$ using a validation set or using cross-validation. \n",
    "\n",
    "###### Algorithm\n",
    "- Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations. \n",
    "- Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees as a function of $\\alpha$.\n",
    "- Use K-fold cross-validation to choose $\\alpha$. Divide training observations into K folds. For each fold do the following:\n",
    "    - Repeat steps 1 and 2 on all but the $K$th fold of the training data.\n",
    "    - Evaluate the mean squared prediction error on the data in the left-out $K$th fold as a function of $\\alpha$. Average the results for each value of $\\alpha$ and pick an $\\alpha$ to minimize the average error. \n",
    "- Return the subtree from step 2 that corresponds to the chosen value of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Trees\n",
    "In a classification tree we predict that each observation belongs to the most commonly occuring class of training observations in the region to which it belongs. We are interested in:\n",
    "- Class prediction corresponding to a node region\n",
    "- The class proportions among the training observations that fall into that region. \n",
    "\n",
    "Classification Trees are similar to Regression Trees except RSS cannot be used. Instead we use the classification error rate (number of observations in a region that do not belong to the most common class):\n",
    "$$E = 1 - max_k(\\hat p_{mk})$$\n",
    "\n",
    "where $\\hat p_{mk}$ represents the proportion of training observations in the $m$th region that are from the $k$th class. Issue with this approach is that it is not sufficient for tree growing, so instead we use two other alternatives:\n",
    "- Gini Index: $G = \\sum_{k = 1}^K\\ \\hat p_{mk}(1 - \\hat p_{mk})$, which measures total variance across $K$ classes. This takes on a small value if all of the $\\hat p_{mk}$'s are close to 0 or 1; small values indicate that a node contains predominantly observations from a single class.\n",
    "- Entropy: $D = -\\sum_{k = 1}^K\\ \\hat p_{mk}\\ log(\\hat p_{mk})$. Entropy will take on a value near 0 if all of the $\\hat p_{mk}$'s are all near zero or near one (if the $m$th node is pure).\n",
    "\n",
    "These above calculations are used to evaluate the quality of a split. Classification Error Rate is the preferred method if prediction accuracy of the final pruned tree is the goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trees vs Linear Models\n",
    "Regression trees assume the form:\n",
    "$$f(X) = \\sum_{m = 1}^M\\ c_m * 1(X \\in R_m)$$\n",
    "\n",
    "When do you use Linear Regression and when do you use Regression Trees?\n",
    "- Linear Regression when the predictors and the response have a linear relationship\n",
    "- Regression Trees if the predictors and the response do not have a linear relationship.\n",
    "\n",
    "###### Advantages \n",
    "- Trees are easy to explain\n",
    "- Trees resemble human decision making more closely\n",
    "- Trees can be displayed graphically\n",
    "- Trees can handle qualitative predictors without the need to create dummy variables\n",
    "\n",
    "###### Disadvantages\n",
    "- Trees do not have the same level of predictive accuracy as other regression and classification approaches.\n",
    "- Trees are not that robust. Small changes in the data can change the final estimated tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Bagging, Random FOrests, Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "Bagging can help reduce the variance that Tree suffer from. Variance can be reduced by averaging a set of observations. This is vital as that means we take many training sets from the population, build a seperate prediction model using each training set, and average the resulting predictions:\n",
    "$$\\hat f_avg(x) = \\frac{\\sum_{b = 1}^B\\ \\hat f^b{x}}{B}$$\n",
    "\n",
    "The above is not too feasible as we do not have access to multiple training sets. We can instead bootstrap by taking repeated samples from a single training data set. Essentially create $B$ different bootstrap training data sets. We then train our method on the $b$th training set in order to get $\\hat f^{*b}(x)$ and averaging all predictions to obtain:\n",
    "$$\\hat f_bag(x) = \\frac{\\sum_{b = 1}^B\\ \\hat f^{*b}{x}}{B}$$\n",
    "\n",
    "To apply bagging to Regression Trees we create $B$ trees using $B$ bootstrapped training sets and averaging the resulting predictions. These trees are grown deep and not pruned. This creates trees with very high variance, but low bias. THe averaging reduces the high variance. \n",
    "\n",
    "In regard to Classification Trees, given a test observation record the class predicted by each of the $B$ trees, and take majority vote.\n",
    "\n",
    "###### Out-of-Bag Error Estimation\n",
    "On average each bagged tree uses around two-thirds of the observations. THe reminaing one-third not used to fit a bagged tree are referred to as Out-of-Bag (OOB) observations. We can predict the response for the $i$th observation using each of the trees is which that observation was OOB. This yields $\\frac{B}{3}$ predictions for the $i$th observation. To get a single prediction we average all of the predictions (regression) or take the majority vote (classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "Provides an improvement over bagging by adding some decorrelating the trees. Like bagging we build a number of decision trees on bootstrapped trainign samples, but when these are built each time a tree split is considered, a random sample of $m$ predictors are chosen as split candidates from the full set of $p$ predictors. The split is allowed to only use the $m$ predictors. A new set of predictors is chosen for each split. $m$ is:\n",
    "$$m \\approx \\sqrt(p)$$\n",
    "\n",
    "This is vital as it stops a strong predictor from dictating the top split in the forest (All bagged trees will look similar to each other). Doing the random split assures that:\n",
    "$$\\frac{p - m}{p}$$\n",
    "\n",
    "splits will not consider the strong predictor, given the other predictors a chance. This decorrelates the trees and therefore makes the trees less variable (more reliable). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting\n",
    "Boosting grows the trees sequentially: trees are grown using the information from previously grown trees. Boosting also does not involve bootstrap sampling. Rather than fitting a deep tree, boosting will learn slowly. For example, fit a tree based on the current residuals, not on the outcome of $Y$. We then add this decision tree into the fitted function in order to update the residuals.  \n",
    "\n",
    "Each of these tree will be small with few terminal nodes, which is determined by $d$ in the algorithm. The shrinkage parameter $\\lambda$ slows the process down even further allowing more and differently shaped trees to attack the residuals. \n",
    "\n",
    "Boosting Trees have 3 parameters:\n",
    "- Number of trees $B$. Boosting can overfit if $B$ is too large. Use cross-validation to select $B$.\n",
    "- Shrinkage parameter $\\lambda$ (small positive number). Typical values are 0.01, 0.001. A very small $\\lambda$ can result in a very large $B$ for good performance. \n",
    "- The number of splits $d$ in each tree (complexity of boosted ensemble). Usually $d$ is 1 (tree is a stump with a single split). \n",
    "\n",
    "###### Algorithm\n",
    "- Set $\\hat f_=(x) = 0$, $r_i = y$, for all $i$ in the training set\n",
    "- For b = 1, ..., B repeat:\n",
    "    - Fit a tree $\\hat f^b with $d$ splits ($d + 1$ terminal nodes)$ to the training data $(X, r)$\n",
    "    - Update the residuals: $r_i <- r_i - \\lambda\\ \\hat f^b\\ (x)$\n",
    "    - Update the residuals: $r_i <- r_i - \\lambda \\hat f^b\\ (x_i)$\n",
    "- Output the boosted model: $\\hat f(x) = \\sum_{b = 1}^B\\ \\lambda\\ \\hat f^b(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

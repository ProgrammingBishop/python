{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set Approach\n",
    "Randomly divide the available set of observations into two parts:\n",
    "- training set\n",
    "- validation set\n",
    "\n",
    "Model is fit on training set, and the fited model is used to predict responses for observation in validation set. The validation set error rate is MSE and provides estimate of test error rate. An example case is to consider if a term should be a quadratic in regressions. Rather than looking at the p-values we split the data into a training set and a validation set. A smaller MSE for a quadratic term would mean that term is significant. \n",
    "\n",
    "###### Drawbacks\n",
    "- The validation estimate of the test error can be highly variable based on the observations included in each split.\n",
    "- Only a subset of observations are used to fit the model. This can lead to overestimation of the test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave-One-Out Cross-Validation (LOOCV)\n",
    "Unlike the validation set, this only leaves one observation out for the validation. Since one observation is not used for the fitting, MSE is calculated by:\n",
    "$$\\sum_{i = 1}^N\\ (y_i - \\hat y_i)^2$$\n",
    "\n",
    "or, for each observation leave it out as validation and form model on the remaining training points. Then grab the mean of the MSE's for all iterations:\n",
    "$$CV_(n) = \\frac{\\sum_{i = 1}^n}{n}$$\n",
    "\n",
    "###### Advantages\n",
    "- Less bias\n",
    "- Does not lead to overestimation of the test error rate\n",
    "- ALways yield the same results\n",
    "\n",
    "###### Disadvantages\n",
    "- Potential to be expensive to implement\n",
    "\n",
    "With least squares linear or polynomial regression this shortcut can make the cost of LOOCV the same as a single model fit:\n",
    "$$CV_(n) = \\frac{\\sum_{i = 1}^n\\ (\\frac{(y_i - \\hat y_i)}{(1 - h_i)})^2}{n}$$\n",
    "\n",
    "where $h_i$ is the leverage. Essentially the $i$th residual is divided by $(1 - h_i)$. The leverage lies between $\\frac{1}{n}$ and 1 and reflects the amount an observation influences its own fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation\n",
    "Split data into $K$ folds and use the current fold for each iteration as the validation set and the remaining data as the data used to train a model. This is similar to LOOCV except that instead of $N$ folds, the number of folds is determined by the user. Classic folds are 5 or 10. \n",
    "\n",
    "K-Fold has the benefit of being computationally cheaper. Other advantages regard the bias-variance trade-off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias-Variance Trade-off for K-Fold CV\n",
    "Using K-Fold CV will have an intermediate level of bias since it uses only $\\frac{(k - 1)n}{k}$ observations. So if possible use LOOCV since it will have the lowest bias. But it is important to note that LOOCV has a higher variance than K-Fold CV. This is because we are training $n$ models trained on an almost identical set of observation (correlated outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation on Classification Problems\n",
    "Instead of using MSE we can use the number of misclassified examples:\n",
    "$$CV(n) = \\frac{\\sum_{i = 1}^n\\ I(y_i \\neq \\hat y_i)}{n}$$\n",
    "\n",
    "When it comes to deciphering the numbr of polynomial terms to use and their degree, we can us CV and plot the Error Rate on the Y-axis and the Order of the Polynomial on the X-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to minimze:\n",
    "$$Var( \\alpha X + (1 - \\alpha) Y )$$\n",
    "\n",
    "where:\n",
    "$$\\alpha = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\hat \\sigma^2_X + \\hat \\sigma^2_Y - 2 \\hat \\sigma_{XY}}$$\n",
    "\n",
    "where $\\sigma_{XY}$ is the covariance of $X$ and $Y$.\n",
    "\n",
    "The sampling in bootrapping is done with replacement. Compute the standard error of these bootstrap estimates using:\n",
    "$$SE_B(\\hat \\alpha) = \\sqrt( \\frac{1}{(B - 1)}\\ \\sum_{r = 1}^B\\ ( \\hat \\alpha^{*r} - \\frac{1}{B}\\ \\sum_{r' = 1}^B\\ \\hat \\alpha^{*r'} )^2 )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

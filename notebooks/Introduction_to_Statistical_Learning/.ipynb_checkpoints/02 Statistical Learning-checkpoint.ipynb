{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Estimate f\n",
    "\n",
    "###### Prediction\n",
    "$$E(Y - \\hat Y) ^ 2 = [f(X) - \\hat f(X)] ^ 2 + Var(\\epsilon)$$\n",
    "\n",
    "Where $E(Y - \\hat Y) ^ 2$ represents the expected value and $Var(\\epsilon)$ is the irreducible error. This produces an upper bound on the accuracy of prediction for $Y$.\n",
    "\n",
    "###### Inference\n",
    "Less focus on predicting $Y$ and more focus on how $Y$ changes as a function $f(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we Estimate f\n",
    "\n",
    "###### Parametric Models\n",
    "Two-step approach:\n",
    "- Make an assumption about the functional form (shape) of $f$. An example could be: $f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 + X_2$\n",
    "- After a model has been selected we need a procedure that uses the training data to fit or train the model. Using the above example this could involve estimating the coefficients $\\beta_p$. This is essestially finding the coefficients that gives the best estimate for $Y$. A common metric is least squares.\n",
    "\n",
    "Issue with the parametric approach is that the chosen function will not match the unknown form of $f$.\n",
    "\n",
    "###### Non-parametric Models\n",
    "Do not make an assumption about the functional form of $f$. THese models try to find $f$ that fits the data as close as possible without being too rough. The drawback is that non-parametric models do not reduce the the problem of $f$ to a small number of parameters, therefore a very large number of observations is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Assessing Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring Quality of Fit\n",
    "\n",
    "Mean-Squared Error (MSE): $MSE = \\frac{1}{n}\\ \\sum_{i = 1}^n\\ (y_i \\hat f(x_i))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias-Variance Trade-Off\n",
    "\n",
    "The expected MSE can be decomposed into three quantities:\n",
    "- Variance\n",
    "- Squared Bias\n",
    "- Variance of Error Terms\n",
    "\n",
    "$$E(y_0 - \\hat f(x_0)) ^ 2 = Var(\\hat f(x)) + [Bias(\\hat f(x_0)^2)] + Var(\\epsilon)$$\n",
    "- $E(y_0 - \\hat f(x_0)) ^ 2$ is the expected test MSE and derived from the average test MSE obtained through repeated estimations for $f$ using a large number of training sets.\n",
    "- Optimum method has the lowest of the three quantities (Expected MSE can never lie below the $Var(\\epsilon)$).\n",
    "\n",
    "###### Variance\n",
    "Refers to the amount by which $\\hat f$ would change if we estimated it using a different training data set. If high variance is present, then smal changes in the training data will result in high changes in $\\hat f$. High variance is tied to models with higher flexibility.\n",
    "\n",
    "###### Bias\n",
    "Error introduced by approximating a real-life problem. Linear Regression, for example, assumes a linear fit, which will introduce bias.More flexible models result in less bias.\n",
    "\n",
    "As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases, thus the expected test MSE declines (fixing underfitting). Eventually increasing flexibility has little impact on the bias but starts to significantly increase the variance, which casuses the test MSE to increase (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Classification Setting\n",
    "\n",
    "###### Training Error Rate\n",
    "Proportion of mistakes that are made if we apply our estimate $\\hat f$ to the training observations:\n",
    "$$\\frac{1}{n}\\ sum_{i = 1}^n\\ I(y_i \\neq \\hat y|i)$$\n",
    "\n",
    "Where $I(y_i \\neq \\hat y|i)$ is an indicator variable that equals 1 if the actual and predicted values are not equal and 0 if they are equal (classified accurately).\n",
    "\n",
    "###### Test Error Rate\n",
    "$$Ave(I(y_i \\neq \\hat y|i))$$\n",
    "\n",
    "###### Bayes Classifier\n",
    "Test Error Rate is minimized, on average, by a classifier that assigns each observation to the most likely class, given its predictor values. $Pr(Y = j | X = x)$ for which class the probability is largest is the label a vector of variables will receive. The Bayes Decision Boundary is what decides the label. For a two-class problem the decision boundary is 0.5. \n",
    "\n",
    "The Bayes Error Rate at $X = x_0$ will be $1 - E( max_j\\ Pr(Y = j | X ) )$.\n",
    "\n",
    "###### K-Nearest Neighbors\n",
    "$$Pr(Y = j | X = x_0) = \\frac{1}{K}\\ sum_{i \\in N_0}\\ I(y_i = j)$$\n",
    "\n",
    "KNN uses Bayes rule and classifies the test observation $x_0$ to the class with the largest probability. As $\\frac{1}{K}$ increases, the moethod becomes more flexible, thus decreasing the the training error rate and potentially hurting the test error rate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

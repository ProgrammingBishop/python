{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Population Regression Line\n",
    "This is the best linear approximation to true relationship between $X$ and $Y$.\n",
    "$$Y \\approx \\beta_0 + \\beta_1 X + \\epsilon$$\n",
    "\n",
    "#### Estimating the Coefficients\n",
    "\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the slope\n",
    "\n",
    "Residual $\\epsilon_i = (y_i - \\hat y_i)^2$\n",
    "$$RSS = \\sum_{i = 1}^n\\ (y_i - \\hat y_i)^2$$\n",
    "\n",
    "or the sum of $\\epsilon$\n",
    "\n",
    "And the minimizers (least squares coefficients) are:\n",
    "- $\\hat \\beta_1 = \\frac{\\sum_{i = 1}^n\\ (x_i - \\bar x_i)(y_i - \\bar y_i)}{\\sum_{i = 1}^n\\ (x_i - \\bar x)^2}$\n",
    "- $\\hat \\beta_0 = \\bar y - \\hat \\beta_1 \\bar x$\n",
    "\n",
    "While it is not feasible to estimate the true population mean given a sample set, we can achieve a close approximation of the population mean from a collection of sample means. A particular set of observations may have a sample mean $\\bar \\mu$ that underestimates the population $\\mu$. Another set of observations may have a sample mean $\\bar \\mu$ overestimate the population mean $\\mu$. But with enough sample means, we can obtain a very close approximation to the population mean.\n",
    "\n",
    "This approach can also work for estimating the coeffiecient values $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating Accuracy of the Coefficient Estimates\n",
    "\n",
    "To estimate closeness of $\\bar \\mu$ to $\\mu$, a value called Standard Error is calculated:\n",
    "$$Var(\\bar \\mu) = SE(\\mu)^2 = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "where $\\sigma^2$ is the standard deviation for each set of observations. This tells us the average amount that the estimate $\\bar \\mu$ differs from the actual value. Notice that the equation shrinks with $n$, that is in the presence of more observations the standard error decreases. To get the standard errors for the coefficients we use:\n",
    "- $SE(\\hat \\beta_0)^2 = \\sigma^2\\ [\\frac{1}{n} + \\frac{\\bar x^2}{\\sum_{i = 1}^n\\ (x_i - \\bar x)^2}]$\n",
    "- $SE(\\hat \\beta_1)^2 = \\frac{\\sigma^2}{\\sum_{i = 1}^n\\ (x_i - \\bar x_i)^2}$\n",
    "\n",
    "Notice that the $SE(\\hat \\beta_1)$ is smaller when the $x_i$ are more spread out. We have more leverage to estimate a slope when this is the case.\n",
    "\n",
    "The estimate of $\\sigma$ is known as the residual standard error:\n",
    "$$RSE = \\sqrt(\\frac{RSS}{(n - 2)})$$\n",
    "\n",
    "Confidence Intervals, 95% confidence interval is defined as a range of values such that with 95% probability the range will contain the true unknown value of the parameter. This range is defined in terms of lower and upper limits computed from the sample data. \n",
    "- $\\hat \\beta_1 \\pm SE(\\hat \\beta_1)$\n",
    "- $\\hat \\beta_0 \\pm SE(\\hat \\beta_0)$\n",
    "\n",
    "which means there is a 95% chance that the interval above will contain the true value of $\\beta_1$.\n",
    "\n",
    "###### Hypothesis Testing\n",
    "- $H_0: \\beta_1 = 0$ (no relationship)\n",
    "- $H_a: \\beta_1 \\ne 0$ (has relationship)\n",
    "\n",
    "If the $SE(\\hat \\beta_1)$ is small, then small values of $\\hat \\beta_1$ may provide stong evidence that $\\hat \\beta_1 \\ne 0$ and that there is a relationship between $X$ and $Y$. If large, then the coefficient must be large in absolute value in order for us to reject the null hypothesis. \n",
    "\n",
    "T-Statistics\n",
    "$$t = \\frac{\\hat \\beta_1 - 0}{SE(\\hat \\beta_1)}$$\n",
    "\n",
    "which measures number of standard deviations that $\\hat \\beta_1$ is away from 0. If there is no relationship between $X$ and $Y$, then we expect a t-distribution with $n - 2$ degrees of freedom. The t-distribution has a bell shape and for values of $n$ greater than around 30 it is quite similar to a normal distribution. \n",
    "\n",
    "###### P-Value\n",
    "A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance (the association is meaningful), in the absence of any real association between the predictor and the response. Therefore if we see a small p-value, we can infer that there is an association between the predictor and the response. This is why we would reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing the Accuracy of a Model\n",
    "\n",
    "###### Residual Standard Error (RSE)\n",
    "$$RSE = \\sqrt(\\frac{1}{n - 2}\\ \\sum_{i = 1}^n\\ (y_i - \\hat y_i)^2$$\n",
    "\n",
    "$$R^2 = 1 - \\frac{RSS}{TSS}$$\n",
    "- $RSS = \\sum_{i = 1}^n\\ (y_i - \\hat y_i)^2$\n",
    "- $TSS = \\sum(y_i - \\bar y)^2$\n",
    "\n",
    "TSS measures the total variance in the response $Y$, which is the amount of variability inherit in the response before the regression is performed. RSS measures the amount of variability in the response left unexplained after performing regression. Hence TSS - RSS measures the amount of variability in the response that is explained by performing the regression, and $R^2$ measures the proportion of variability in $Y$ that can be explained by using $X$. An $R^2$ statistic close to one means a good majority of the variability in the response is explained by the regression. \n",
    "\n",
    "###### Correlation\n",
    "$$Corr(X, Y) = \\frac{\\sum_{i = 1}^n\\ (x_i - \\bar x_i)(y_i - \\bar y_i)}{\\sqrt \\sum_{i = 1}^n\\ (x_i - \\bar x)^2 \\sqrt \\sum_{i = 1}^n\\ (y_i - \\bar y)^2}$$\n",
    "\n",
    "and represents a linear relationship between $X$ and $Y$. In simple linear regression, $R^2 = r^2$ where $r^2$ represents the correlation between $X$ and $Y$. $R^2$ is the square of the correlation of the response and the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3$$\n",
    "\n",
    "where $\\beta_j$ is the $j$th predictor and $\\beta_j$ is the $j$th predictor's association wth $Y$. View $\\beta_j$ as the averega effect on $Y$ for a one unit increase in $X_j$, holding all other predictors fixed. \n",
    "\n",
    "#### Questions\n",
    "\n",
    "###### One: Relationship Between Response and Predictors?\n",
    "First step in Multpile Linear Regression is to compute the F-Statistic and examine the associated p-value.\n",
    "\n",
    "- $H_0: \\beta_1 = \\beta_p = 0$\n",
    "- $H_a:$ at least one coefficient does not equal 0\n",
    "\n",
    "Compute the F-Statistic:\n",
    "$$F = \\frac{(TSS - RSS) / p}{RSS / (n - p - 1)}$$\n",
    "\n",
    "- $E{RSS / (n - p -1)} = \\sigma^2$\n",
    "- $E{(TSS - RSS) / p} = \\sigma^2$\n",
    "\n",
    "such that if there is no relationship between the response and the predictors, one would expect the F-Statistic to take on a value close to 1. Otherwise if $H_a$ is true, then $E{(TSS - RSS/p} > \\sigma^2$ so we expect $F$ to be greater than 1. When the value of $n$ is large, an F-Statistic that is just a little larger than 1 might still provide evidence against the null. In contrast, a larger F-Statistic is needed to reject the null if $n$ is small. When the null is true and the errors $\\epsilon_1$ have a normal distribution, the F-Statistic follows an F-Distribution. \n",
    "\n",
    "Sometimes we want to estimate if a particular subset of the coefficients are zero:\n",
    "$$H_0: \\beta_{p - q + 1} = \\beta_{p - q + n} = 0$$\n",
    "\n",
    "With this we fit a secnd model that uses all variables except the last $q$. We then compute the RSS for that subset as, for example, $RSS_0$ so that the F-Statistic for that subset is:\n",
    "$$F = \\frac{(RSS_0 - RSS) / q}{RSS / (n - p - 1)}$$\n",
    "\n",
    "This evaluates the partial effect of adding that variable to the model. \n",
    "\n",
    "###### Two: Deciding on Important Variables\n",
    "Once we have confirmed that at least one variable is related to the response, the next step is to determine which predictor that is. \n",
    "\n",
    "It is not viable to test all substes as there are $2^p$ models to consider. With this said, there are three viable approaches:\n",
    "- Forward Selection: begin with null model of only an intercept. Then fit $p$ linear models and add to the null the variable that results in the lowest RSS. Repeat.\n",
    "- Backward Selection: start with all variables in the model, and remove the variable with the largest p-value (least statistically significant). We stop when a threshold has been reached. (Cannot be used if $p > n$).\n",
    "- Mixed Selection: this is a combination of forward and backward where two thresholds have been met (lower and upper bound for the p-value). You start with a null model and add variables like forward selection. The difference is that you remove a variable if the addition of a variable causes a previously added variable to go below a threshold. \n",
    "\n",
    "###### Three: Model Fit\n",
    "In multiple linear regression, the $R^2$ is $Corr(Y, \\bar Y)^2$. One property of the fitted linear model is that it maximizes this correlation among all possible linear models.  One question to ask is how does RSE increase when RSS must decrease? Well, models with more variables can have a higher RSE if the decrease in RSS is small relative to the increase in $p$. This is based on the equation:\n",
    "$$RSE = \\sqrt \\frac{1}{n - p - 1} RSS$$\n",
    "\n",
    "###### Four: Predictions\n",
    "While we can get close to the true prediction for $Y$. there will always be a confidence interval to account for the error term $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Other Considerations in the Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative Predictors\n",
    "\n",
    "Create a dummy variable to represent a variable that takes on several values. Given $x_i$ and it can take on male or female:\n",
    "- $x_i = 1$ if the $i$th variable is female\n",
    "- $x_i = 0$ if the $i$th variable is male\n",
    "\n",
    "This results in two regression models:\n",
    "- $y_i = \\beta_0 + \\beta_1 + \\epsilon_i$ if the $i$th person is female\n",
    "- $y_i = \\beta_0 + \\epsilon_i$ if the $i$th person is male\n",
    "\n",
    "With this $\\beta_0$ is the result of the response given the default value for $\\beta_1$, and $\\beta_0 + \\beta_1$ is the response value given the non-default value (assuming the values have only two possible categories). This also represents the difference between the categories. Of course if the p-value for the inclusion of the additional predictor is high, then we can conclude there is no statistical evidence of a difference in average credit card balance between the genders. \n",
    "\n",
    "We can also code the dummy variable to be 1 and -1 rather than 1 and 0. Doing this makes it not a matter of including or excluding a variable, but a matter of the addition of a dummy variable negatively affecting the response. Or, how the inclusion or exclusion of a dummy variable causes the response to be around the average at the intercept. \n",
    "\n",
    "###### Qualitative Predictors with More than Two Levels\n",
    "Rather than a dummy variable being 0 or 1, with multiple possible values for the dummy variable we will have $x$ be represented as $x_{i1}, x_{i2}, x_{i3}$, etc. and each of these variations of the one variable will be 1 or 0. This creates a One Hot Encoding for a vairable in which a variable can turn on - like a switch - one of multiple values so that on of the possible values gets 1 while all the others get 0. This makes it so that a particular variation is included while all the others are excluded. An example would be a variable representing ethnicity. So:\n",
    "- $y_i = \\beta_0 + \\beta_1 + \\epsilon_i$ if the $i$th is Asian\n",
    "- $y_i = \\beta_0 + \\beta_2 + \\epsilon_i$ if $i$th is Caucasion\n",
    "- $y_i = \\beta_0 + \\epsilon_i$ if the $i$th person is African American\n",
    "\n",
    "Here $\\beta_0$ is the response for African Americans, $\\beta_1$ as the difference between Asian and African American categories, and $\\beta_2$ can be interpreted as the difference in the average balance between Caucasion and African American. There will always be one fewer dummy variables than levels because there will always be one dummy variable value that serves as the default level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extensions of the Linear Model\n",
    "\n",
    "Two important relations between the predictors and the response are:\n",
    "- Additive: predictor effects on $Y$ are independent of the other predictors\n",
    "- Linear: change in $Y$ due to a one unit change in $x_j$ is constant regardless of the value of $x_j$.\n",
    "\n",
    "###### Removing Additive Assumptions\n",
    "Add in interaction terms:\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\epsilon$$\n",
    "becomes\n",
    "$$Y = \\beta_0 + (\\beta_1 + \\beta_3 X_2)X_1 + \\beta_2 X_2 + \\epsilon$$\n",
    "\n",
    "where $(\\beta_1 + \\beta_3 X_2)X_1$ becomes $\\tilde \\beta_1 X_1$ and changes with $X_2$. Changing the value of $X_2$ will change the impact of $X_1$ on $Y$.\n",
    "\n",
    "Dummy variables allow for different lines on the same plot. In other words, the average will change based on the presence or absence of a predictor. Interactive terms allow for a change in the slope. With the interaction term we have:\n",
    "- $Y_i = \\beta_0 + \\beta_1 X_i +$:\n",
    "    - $\\beta_2 + \\beta_3 X_i$ if male\n",
    "    - $0$ if female\n",
    "    \n",
    "Which then becomes:\n",
    "- $(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) X_i$ if male\n",
    "- $\\beta_0 + \\beta_1 X_i$ if female\n",
    "\n",
    "where $(\\beta_0 + \\beta_2)$ is the difference in intercepts (dummy variables) and $(\\beta_1 + \\beta_3)$ is the difference in slopes (interaction terms - how the presence of one variable affects another).\n",
    "\n",
    "###### Non-Linear Relationships\n",
    "Adding polynomial terms can help a model that is not strictly linear. To know when to stop adding the $n$th degree term use a p-value to evaluate the the addition of the next $n$ degree version of a predictor. If not significant then stop; if significant then include the variable and check the next $n$th degree version of that variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following problems can occur when fitting a linear regression:\n",
    "\n",
    "###### Non-linearity of the response-predictor relationships\n",
    "If a residual plot indicates that there is a non-linear associations in the data, then use non-linear transformations of predictors like $log(X)$, $\\sqrt X$, or $X^2$.\n",
    "\n",
    "###### Correlation of error terms \n",
    "If there is correlation between the error terms $\\epsilon$, then the estimated standard errors will tend to underestimate the true standard errors. This results in narrower confidence and prediction intervals than they should be. Correlated error terms lead to unwarrented confidence in our models. Positively correlated variables lead to tracking residuals (adjacent residuals may have similar values).\n",
    "\n",
    "###### Non-constant variance of error terms:\n",
    "Heteroscedasticity can be seen when the linear plot of data is funnel-shaped in the residual plot. To fix this issue, transform the response $Y$ with a concave functions like the log() or the square-root of the response. This leads to a greater amount of shrinkage for larger responses. \n",
    "\n",
    "###### Outliers: \n",
    "To identify the outliers it is beneficial to plot the studentized residuals calculated by dividing each residual $e_i$ by its estimated standard error. Observations whose outliers are greater than 3 in absolute value are possible outliers. It is common to remove outliers, but study should be made to assure that it is not due to some other deficiency like a missing predictor value. \n",
    "\n",
    "###### High-leverage points:\n",
    "High Leverage observations tend to have a sizeable impact on the estimated regression line; sometimes greater than outliers. To quantify an observation's leverage, we compute the leverage statistic. A large value of this statistic indicates an observation with high leverage. For simple linear regression:\n",
    "$$h_i = \\frac{1}{n} + \\frac{(x_i - \\bar x)^2}{\\sum_{i' = 1}^n\\ (x_{i'} - \\bar x)^2}$$\n",
    "\n",
    "with this equation $h_i$ increases with the distance between $x_i$ and $\\bar x$. In regard to multiple linear regression, if an observation has a leverage statistic that greatly exceeds $\\frac{p + 1}{n}$, then we suspect the corresponding point has high leverage. \n",
    "\n",
    "###### Collinearity:\n",
    "If two variables are interact in a collinear fashion, it can be troublesome to identify how the collinear predictors interact with the response (which is responsible). A simple way to detect collinearity is to look at the correlation matrix of the predictors. A large value in this table that is large in absolute value flags the two variables as collinear. While this flags collinearity, but not mutlicollinearity: correlation between three variables. \n",
    "\n",
    "Rather than a correlation matrix, a better method to detect collinearity is the Variance Inflation Factor (VIF). The VIF is the ratio of the variance of $\\bar \\beta_j$ when fitting the full model divided by the variance of $\\bar \\beta_j$ if fit on its own. The smallest value for VIF is 1, which represents the complete absence for collinearity. As a rule of thumb, a VIF greater than 5 or 10 indicates a problematic amount of collinearity. \n",
    "\n",
    "The VIF can be computed as:\n",
    "$$VIF(\\hat \\beta_j) = \\frac{1}{1 - R^2_{X_j | X_{-j}}}$$\n",
    "\n",
    "where $R^2_{X_j | X_{-j}}$ is the $R^2$ from a regression of $X_j$ onto all of the other predictors. If that value is close to one, then the collinearity is present and so the VIF will be large. Two ways to deal with collinearity is:\n",
    "- Drop problematic variables from the regression. \n",
    "- Combine the collinear variables together into one predictor. An example is taking the average of the two predictors to create a new predictor. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

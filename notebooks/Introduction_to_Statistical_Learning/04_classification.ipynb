{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model  \n",
    "The Logistic Function always assures an S-shaped curve that lies between 0 and 1 and should be used for binary responses.  \n",
    "\n",
    "$$p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}$$\n",
    "\n",
    "and  \n",
    "$$e^{\\beta_0 + \\beta_1X} = \\frac{p(X)}{1 - p(X)}$$\n",
    "\n",
    "where results close to 0 and $\\infty$ indicate very low and very high probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating the Regression Coefficients  \n",
    "It is best to use maximum likelihood to fit logistic regression models. The Likelihod Function:  \n",
    "\n",
    "$$l(\\beta_0, \\beta_1) = \\prod_{i:y_i = 1}\\ p(x_i)\\ \\prod_{i':y_{i'}=0}\\ (1 - p(x_{i'}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Logistic Regression\n",
    "$$log( \\frac{p(X)}{1 - p(X)} ) = \\beta_0 + \\sum_{p = 1}^n\\ \\beta_p X_p$$  \n",
    "\n",
    "and  \n",
    "\n",
    "$$p(X) = \\frac{e^{\\beta_0 + \\sum_{p = 1}^n\\ \\beta_p X_p}}{1 + e^{\\beta_0 + \\sum_{p = 1}^n\\ \\beta_p X_p}}$$  \n",
    "\n",
    "Overall we calculate the probability of each $p(X)$ and then use maximum likelihood using those probabilities to decipher which class the observation belongs to. The class is the $P(X)$ with the larget probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to use this method when working with classification that focuses on more than two levels. The process is to model the predictors $X$ seperately in each response class, and then use Bayes' Theorem to flip these into estimates for $Pr(Y = k | X = x)$. When these are assumed to be normal, it turns out that the model is very similar in form to logistic regression. \n",
    "\n",
    "It is best to use LDA over Logistic Regression when:\n",
    "- Classes are well-seperated. In this setting the parameter estimates are very unstable. \n",
    "- If $n$ is small and the distribution of the predictors are approximately normal in each of the classes, LDA is more stable.\n",
    "- Overall LDA is more popular when we have classification that is beyond a two-class response. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Bayes' Theorem for Classification\n",
    "Let $\\pi_k$ represent the overall prior probability that randomly chosen observation comes from the $k$th class. Then let $f(k) = Pr(X = x | Y = k)$ denote the density function of $X$ for an observation that comes from the $k$th class. This way $f_k(X)$ is relatively large if an observation is from the $k$th class.\n",
    "$$Pr(Y = k | X = x) = \\frac{\\pi_k\\ f_k(x)}{\\sum_{l = 1}^K\\ \\pi_l\\ f_l(x)}$$\n",
    "\n",
    "in the case above $\\pi_k$ represent the number of observations in the $k$th class. $p_k(x)$ is the posterior probability that an observation belongs to the $k$th class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA for p = 1\n",
    "Assume that $f_k(x)$ is Gaussian. With only a single predictor the density takes the form of:\n",
    "$$f_k(x) = \\frac{1}{\\sqrt(2 \\pi \\sigma_k)}\\ exp(-\\frac{(x - \\mu_k)^2}{2 \\sigma_k^2})$$\n",
    "\n",
    "where $\\mu_k$ and $\\sigma_k^2$ are the mean and variance parameters for the $k$th class. It is also easy to assume that all classes have equal variance, so use $\\sigma^2$. If we plug this equation into the equation from 4.1, wef ind that:\n",
    "$$p_k(x) = \\frac{ \\pi_k \\frac{1}{\\sqrt(2 \\pi \\sigma)}\\ exp(-\\frac{(x - \\mu_k)^2}{2 \\sigma^2}) }{ \\sum_{l = 1}^K\\ \\pi_l\\ \\frac{1}{\\sqrt(2 \\pi \\sigma)}\\ exp(-\\frac{(x - \\mu_l)^2}{2 \\sigma^2}) }$$\n",
    "\n",
    "This can be simplified by taking the log, so that:\n",
    "$$\\delta_k(x) = x * \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2 \\sigma^2} + log(\\pi_k)$$\n",
    "\n",
    "And the observations is assigned to the class in which $\\delta$ is largest. \n",
    "\n",
    "$$\\hat \\mu_k = \\frac{1}{n_k}\\ \\sum_{i:y_i = k}\\ x_i$$\n",
    "$$\\hat \\sigma^2 = \\frac{1}{n - K}\\ \\sum_{k = 1}^K\\ \\sum_{i:y_i = k}\\ (x_i - \\hat \\mu_k)^2$$\n",
    "\n",
    "where $n_k$ are the number of training observations in the $k$th class. $\\mu_k$ is the mean of all the observations from the $k$th class and $\\hat \\sigma^2$ is the weighted averages of the sample variances for each of the $K$ classes. Also:\n",
    "$$\\hat \\pi_k = \\frac{n_k}{n}$$\n",
    "\n",
    "The decision boundary is the midpoint between the two classes:\n",
    "$$\\frac{(\\hat \\mu_1 + \\hat \\mu_2)}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA for p > 1\n",
    "In a multivariate Gaussian distribution there is an assumption that each individual predictor follows a one-dimensional normal distribution.\n",
    "$$X ~ N(\\mu, \\Sigma)$$\n",
    "\n",
    "where $\\mu$ is the mean of $X$ (vector of $p$ components) and $\\Sigma$ is the $Cov(X)$ ($p\\ x\\ p$ covraince matrix). The multivariate Gaussian density function is:\n",
    "$$f(x) = \\frac{1}{(2 \\pi)^{\\frac{p}{2}}\\ \\mathbf{|\\Sigma|^{\\frac{1}{2}}}}\\ exp( -\\frac{(x - \\mu)^T\\ \\Sigma^{-1}\\ (x - \\mu)}{2} )$$\n",
    "\n",
    "which can be rewritten as:\n",
    "$$\\delta_k(x) = x^T\\ \\Sigma^{-1}\\ \\mu_k - \\frac{1}{2}\\ \\mu_k^T\\ \\Sigma^{-1}\\ \\mu_k + log(\\pi_k)$$\n",
    "\n",
    "and the class that an observation belongs to is the one for which $\\delta_k$ is largest. \n",
    "\n",
    "###### ROC Curve\n",
    "Stands for Reciever Operating Characteristics and measures the overall performance of a classifier summarized over all possible thresholds via the area under the curve (AUC). An ideal ROC curve is one where the curve hugs the top-right corner. Larger AUC is better with the max being 1 and the min being 0. The ROC curve plots the False Positive Rate on X-axis and the True Positive Rate on the Y-axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quadratic Discriminant Analysis (QDA)\n",
    "Assumes results are drawn from a Gaussian distribution and pluggin parameters into Bayes' theorem in order to form predictions. QDA defers from LDA as it assumes each class has its' own covariance matrix. With this assumption the classifier assigns the observation to the class for which\n",
    "$$\\delta_k(x) = -\\frac{1}{2}\\ x^T\\ \\Sigma_k^{-1}\\ x + x^T\\ \\Sigma_k^{-1}\\ \\mu_k\\ - \\frac{1}{2}\\ \\mu_k^T\\ \\Sigma_k^{-1}\\ \\mu_k - \\frac{1}{2}\\ log(\\mathbf{|\\Sigma_k|}) + log(\\pi_k)$$\n",
    "\n",
    "is largest. The decision to use LDA versus QDA depends on the bias-variance tradeoff. When there are $p$ predictors, then estimating a covariance matrix rewuires estimating $$\\frac{p(p + 1)}{2}$$\n",
    "\n",
    "parameters. With this in mind, QDA estimates a seperate covariance matrix for each class for a total of:\n",
    "$$K\\ \\frac{p(p + 1)}{2}$$\n",
    "\n",
    "parameters. LDA has lower variance and less flexible, which could lead to improved predictive performance. The issue is that if LDA's assumption that a common covariance can be used, then the bias will be very high. It is best to use LDA if there are few training observations and so reducing variance is crucial. QDA is recommended if the training set is very large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Comparison of Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between logistic regression and LDA, the true difference lies in the fact that logistic regression $\\beta$ coefficients are estimated using maximum likelihood whereas in LDA the log probabilities are computed using the mean and variance from a normal distribution. This holds also for multidimensional data. LDA will outperform logisitc regression usually when the assumption of observations being drawn from a normal distribution with a common covariance matrix holds true. Otherwise logisitc regression would be better to use.  \n",
    "\n",
    "On the other end, KNN will outperform LDA and logistic regression when the decision boundary is highly non-linear because the classifications are based on a vote-based approach. KNN, however, does not tell us which predictors are important.  \n",
    "\n",
    "QDA is a median of the above approaches because it assumes a quadratic decision boundary. If the decision boundary is linear it is best to use LDA or logistic regression. If the decision boundary is non-linear it is best to use KNN and QDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

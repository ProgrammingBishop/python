{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are intended for binary classifiaction settings.\n",
    "\n",
    "#### Hyperplanes\n",
    "A flat affine subspace of dimension $p - 1$. In 2D this is a line. In 3D this is a square. A hyperplane is defined by the formula:\n",
    "$$\\beta_0 + \\beta_1X_1 + \\beta_2X_2 = 0$$\n",
    "\n",
    "When we say that the above formula defines the hyperplane we say that any $X = (X_1, X_2)^T$ for which the above equation holds is a point on the hyperplane. The above equation is for a line, but we can add additional coefficients and variables X for high p-dimensional planes. \n",
    "\n",
    "What if $X > 0$? That tells us that $X$ lies on one side of the plane. WHat if less than 0? This means $X$ lies on the other side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Using a Seperating Hyperplane\n",
    "Suppose we can create a hyperplane that seperates the training observations perfectly according to their class labels. We classify the points based on whether:\n",
    "$$\\beta_0 + \\sum_{j = 1}^p\\ \\sum_{i = 1}^n\\ \\beta_j x_{ij} > 0$$\n",
    "\n",
    "or equivalently less than 0. This determines if $y_i = 1$ or $y_i = -1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximal Margin Classifier\n",
    "This is the hyperplane that is farthest from the training observations. We can compute the perpendicular distance from each training observation to give a seperating hyperplane. The smallest distance is the minimal distance from the observations to the hyperplane; this is called the margin. Maximal margin hyperplane is the farthest distance. We can classify a test observations based on which side of the maximal marginal hyperplane it sits on. We hope a classifier that has a large margin on the training data will also have a large margin on the test data. \n",
    "\n",
    "This method can lead to overfitting when $p$ is large. Observations that sit on the margin are known as support vectors because they support the maximal margin hyperplane in the sense that if these points were moved then the hyperplane would move as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction of the Maximal Margin Classifier\n",
    "The maximal margin hyperplane is the solution to the optimization problem to maximize $M$ subject to:\n",
    "$$\\sum_{j = 1}^p\\ \\beta_j^2 = 1$$\n",
    "\n",
    "The main constraint is that each point is on the correct side of the hyperplane and at least a distance of $M$ from the hyperplane. Hence $M$ is our hyperplane and we choose the coefficients to maximize $M$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Support Vector Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "The Maximal Marginal Hyperplane is not ideal because the narrower this plane is, which means that training examples can be misclassified and therefore be prone to overfitting on the test data. It would be better to create a hyperplane that does not perfectly fit the data as that:\n",
    "- Offers greater robustness to individual observations\n",
    "- Ofers better classification of most of the training observations\n",
    "\n",
    "We call this a soft margin because some observations can violate the margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details\n",
    "The optimization problem is to maximize the margin $M$ subject to:\n",
    "$$\\sum_{j = 1}^p\\ \\beta_j^2 = 1$$\n",
    "\n",
    "given that $\\epsilon_i \\ge 0$ and:\n",
    "$$\\sum_{i = 1}^n\\ \\epsilon_i \\ge C$$\n",
    "\n",
    "where $C$ is a nonnegative tuning parameter. The $\\epsilon$ are slack variables that allow individual observations to be on the wrong side of the margin or the hyperplane. The $\\epsilon_i$ tell us where the $i$th observation is located relative to the hyperplane and the margin. if $\\epsilon_i = 0$ then the $i$th observation is on the correct side of the hyperplane. If greater then it is on the wrong side of the margin and the observation has violated the margin. If greater than 1 then it is on the wrong side of the hyperplane. \n",
    "\n",
    "$C$ bounds the sum of the $\\epsilon$s and so determines the number and severity of the violations to the margin that will be tolerated. $C$ is the budget for the amount $M$ can be violated by the $n$ observation. If $C > 0$ then we can tolerate $C$ violations of the hyperplane. As $C$ increases the more tolerant the margins and therefore they widen. $C$ is usually chosen via cross-validation and controls the bias-variance trade-off. Small $C$ means small margin which means high variance. Large $C$ is large margin and therefore more bias. \n",
    "\n",
    "points that lie on the margin or on the wrong side of the margin are called support vectors and control the margin. So larger $C$ means wider margin which means more points are influencing the margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with Non-Linear Decision Boundaries\n",
    "Maximize $M$ subject to:\n",
    "$$y_i = ( \\beta_0 + \\sum_{j = 1}^p\\ \\beta_{j1}x_{ij} + \\sum_{j = 1}^p\\ \\beta_{j2}x_{ij}^2 ) \\ge M(1 - \\epsilon_i)$$\n",
    "\n",
    "where:\n",
    "$$\\sum_{i = 1}^n\\ \\epsilon_i \\le C$$\n",
    "\n",
    "given $\\epsilon \\ge 0$\n",
    "\n",
    "so that:\n",
    "$$\\sum_{j = 1}^p\\ \\sum_{k = 1}^2\\ \\beta_{jk}^2 = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine\n",
    "We can expand the feature space using kernels. This allows us to fit a non-linear boundary between the classes. The inner product of two r-vectors $a$ and $b$ is defined as:\n",
    "$$<a, b> = \\sum_{i = 1}^r\\ a_i b_i$$ \n",
    "\n",
    "so the inner product of tw observations $x_i, x_{i'}$ is given by:\n",
    "$$<x_i, x_{i'}> = \\sum_{j = 1}^p\\ x_{ij}x_{i'j}$$\n",
    "\n",
    "It can be shown that:\n",
    "- The linear support vector classifier can be represented as $f(x) = \\beta_0 + \\sum_{i = 1}^n\\ \\alpha_i\\ <x, x_i>$ where there are $n$ parameters $\\alpha_i$, one per training example. \n",
    "- To estimate the parameters $\\alpha$ and $\\beta_0$ all we need are the $\\frac{n(n - 1)}{2}$ inner products between all pairs of training observations. \n",
    "\n",
    "In order to evaluate $f(x)$ we need to compute the inner product between the new point $x$ and each of the training points $x_i$. $\\alpha_i$ is nonzero only for the support vectors. If a training oservation is not a support vector then its $\\alpha_i$ is 0. Based on $S$ being a collection of indices for support points, we can rewrite a more efficient solution:\n",
    "$$f(x) = \\beta_0 + \\sum_{i \\in S}\\ \\alpha_i\\ <x, x_i>$$\n",
    "\n",
    "A kernel is a function that quantifies the similarity of two observations. You can use the following kernel:\n",
    "$$K(x_i, x_{i'}) = (1 + \\sum_{j = 1}^p\\ x_{ij}x_{i'j})^d$$\n",
    "\n",
    "This is known as the polynomial kernel of degree $d$ where $d$ is a positive integer. Using this kernel with $d > 1$ in the SV classifier leads to a much more flexible decision boundary. When a support vector classifier is combined with a non-linear kernel it results in a classifier known as a Support Vector Machine. \n",
    "\n",
    "Radial Kernel:\n",
    "$$K(x_i, x_{i'}) = exp( -\\gamma\\ \\sum_{j = 1}^p\\ (x_{ij} - x_{i'j})^2 )$$\n",
    "\n",
    "where $\\gamma$ is a positive constant. The radial kernel sees if a test observation is far from a training observation $x_i$ in terms of Euclidean Distance. If so then $\\sum_{j = 1}^p\\ (x_{ij} - x_{i'j})^2$ will be large and the $K()$ function will be very tiny. This means $x_i$ will play no rolein the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 SVMs with More than Two Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs do not typically lend itself to aid with more than 2 classes. THere have been a couple of attempts to allow SVMs to extend t $k$ classes.\n",
    "\n",
    "#### One-Versus-One\n",
    "We compare each pair of classes if the number of classes is greater than 2. We classify a test observation sing each of the classifiers and tally number of times test observation is assigned to each of the $k$ classes. Final classification done by assigning test observation to the class it which it is most frequently assigned in the pairise classification. \n",
    "\n",
    "#### One-Versus-All \n",
    "Fit $K$ SVMs. Each time comparing one of the $K$ classes to the remaining $K - 1$ classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Relationship to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the support vector classifier as:\n",
    "$$minimize_{\\beta}\\ { \\sum_{i = 1}^n\\ max[ 0, 1 - y_i\\ f(x_i) ] + \\lambda\\ \\sum_{j = 1}^p\\ \\beta_j^2 }$$\n",
    "\n",
    "where $\\lambda$ is a nonnegative tuning parameter. When $\\lambda$ is large the coefficients are small, which means more violations to the margin are tolerated, and a low variance but high bias classifiet will result. If small then few violations to the margin will occur, which results in high variance and low bias classifier. \n",
    "\n",
    "###### Hinge Loss\n",
    "$$L(X, y, \\beta) = \\sum_{i = 1}^n\\ max[ 0, 1 - y_i( \\beta_0 + \\sum_{j = 1}^p\\ \\beta_{j}X_{ij} ) ]$$\n",
    "\n",
    "When $ y_i( \\beta_0 + \\sum_{j = 1}^p\\ \\beta_{j}X_{ij} )$ is greater than 1 the SVM will loss at 0 since this corresponds to an observation that is on the correct side of the margin. This differs for the loss function for logistic regression as the loss function does not get to zero, but rather very close to 0.\n",
    "\n",
    "SVMs are preferred if the classes have a good seperation, but if there is overlapping of the classification then Logisitc Regression is preferred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 Moving Beyond Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there appears to be two distinct groups in a regression problem, one can begin generating regions in which to split the data. From this a regression line can be fit for each region. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Step Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using polynomial functions of the features imposes a global structure. Step functions help avoid the use of global structures. To do so we break the range of $X$ into bins and then fit a different constant in each bin.This amounts converting a continuous variable into an ordered categorical variable. So we create cutpoints $c_1, ..., c_K$ in the range of $X$, then construct $K + 1$ new variables such that:\n",
    "- $C_0(X) = I(X < c_1)$\n",
    "- $C_1(X) = I(c_1 \\le X \\le c_2)$\n",
    "- ...\n",
    "- $C_k(X) = I(c_k \\le X)$\n",
    "\n",
    "where $I()$ is an indicator function that returns 1 if True, and 0 otherwise. This is similar to the idea of dummy variables. It is important to also note that for any value of $X$, the combination of $C_0(X) + ... + C_k(X) = 1$ since $X$ must be exactly one of the $K + 1$ intervals. We then fit a linear model using the different $C_k(X)$ as predictors:\n",
    "$$y_i = \\beta_0 + \\beta_1 C_1(x_i) + ... + \\beta_K\\ C_K(X_k) + \\epsilon_i$$\n",
    "\n",
    "At most one of the $C_K$ will be non-zero. We exclude $C_0(X)$ as a predictor because it is redundant with the intercept. The logisitc regression fit can be defined as:\n",
    "$$Pr( y_i > threshold | x_i ) = \\frac{ exp(\\beta_0 + \\beta_1 C_1(x_i) + ... + \\beta_K\\ C_K(X_k)) }{ 1 + exp(\\beta_0 + \\beta_1 C_1(x_i) + ... + \\beta_K\\ C_K(X_k)) }$$\n",
    "\n",
    "A downfall of step functions is that they can miss important information, like when there is a rise between two regions of data. There needs to be a smoother transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Basis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial and Stepwise Regression are types of Basis Functions. The idea is to have a family of functions or transformations that can be applied to a variable $X$:\n",
    "$$b_1(X), ..., b_K(X)$$\n",
    "\n",
    "which leads to fitting the model:\n",
    "$$y_i = \\beta_0 + \\beta_1\\ b_1(x_i), ..., \\beta_K\\ b_K(x_i) + \\epsilon_i$$\n",
    "\n",
    "and these basis functions are fixed and known (know ahead of time). We can think of these functions as standard linear model with no predictors, hence we can use least squares to estimate the unknown regression coefficients in the equation above. This also means standard error for coefficient estimates and the F_Statistic for the model's significance are also available. The next section discusses a common choice for a basis function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Regression Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Piecewise Polynomials\n",
    "Fit seperate low-degree polynomials over different regions of $X$. The point where the coefficients change are called knots. For example, a piecewise cubic polynomial with 0 knots is just a regular cubic polynomial ($d = 3$). A piecewise cubic polynomial with a single knot at point $c$ takes the form:\n",
    "- $y_i$\n",
    "    - $\\beta_{01} + \\beta_{11}\\ x_i +\\beta_{21}\\ x_i^2 + \\beta_{31}\\ x_i^3 + \\epsilon$ if $x_i < c$\n",
    "    - $\\beta_{02} + \\beta_{12}\\ x_i +\\beta_{22}\\ x_i^2 + \\beta_{32}\\ x_i^3 + \\epsilon$ if $x_i \\ge c$\n",
    "\n",
    "We fit two different polynomial function to the data, one on the subset of the observations with $x_i < c$, and one on the subset of the observations with $x_i \\ge c$. Each of these polynomials can be fit using least squares applied to simple functions of the original predictors. If we fit $K$ knots throughout the range of $X$, then we end up with $K + 1$ different cubic polynomials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraints and Splines\n",
    "To allow a smoother transition between each region, it is helpful to add two additional constraints (on top of the constraint that the transition must join): include both the first and second derivatives of the piecewise polynomial where the knot occurs. Each constraint we add to the piecewise polynomial frees up a degree of freedom. \n",
    "\n",
    "Cubic splines impose a total of $4 + K$ degrees of freedom. A degree-d spline is a piecewise degree-d polynomial with continuity in the derivatives up to degree $d - 1$ at each knot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spline Basis Regression\n",
    "A cubic spline with $K$ knots can be represented as:\n",
    "$$y_i = \\beta_0 + \\beta_1\\ b_1(x_i) + ... + \\beta_{K + 3}\\ b_{K + 3}(x_i) + \\epsilon_i$$\n",
    "\n",
    "The model can then be fit using least squares. The best way to represent a cubic spline is to start off with a basis function for a cubic polynomial, and then add one truncated power basis function per knot. This truncated power basis function can be defined as:\n",
    "- $h(x, \\xi) = (x - \\xi)^3_+$\n",
    "    - $(x - \\xi)^3$ if $x > \\xi$\n",
    "    - 0 otherwise\n",
    "\n",
    "where $\\xi$ is the knot. In order to fit a cubic spline to a data set with $K$ knots, we perform least squares regression with an intercept and $3 + K$ predictors of the form:\n",
    "$$X, X^2, X^3, h(X, \\xi_1), h(X, \\xi_2), ..., h(X, \\xi_K)$$\n",
    "\n",
    "where the $\\xi$ are the knots. This amounts to estimating a total of $K + 4$ regression coefficients, therefore uses $K + 4$ degrees of freedom. Splines can have high variance on the outer range of the predictors. A natural spline is a spline with additional boundary constraints: function is suppose to be linear at the boundary ($X > \\xi_1$ or $X > \\xi_K$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the Number and Locations of the Knots\n",
    "It is best to place knots in a uniform way. One way to do this is to specifiy the desired degrees of freedom and then have software place the knots at uniform quantiles of the data. \n",
    "\n",
    "How do we choose the degrees of freedom? One way is to try many out and select the best looking curve. ANother way is cross-validation. To do this we remove a portion of the data, fit a spline with a certain number of knots to the remaining data, and then use a spline to make predictionsfor the held-out portion. Repeat this process many times until each observation has been left out once, and then compute the overall cross-validated RSS. This can be repeated for different number of knots $K$. $K$ is ultimately chosen for the number that produced the smallest $RSS$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison to Polynomial Regression\n",
    "Splines outperform polynomial regression fits because they produce flexibility by introducing knots and keeping the degree fixed. It is this reason the splines produce better estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Smoothing Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "We want to find a functin $g(x)$ that fits the observed data well. We want $RSS = \\sum_{i = 1}^n\\ (y_i - g(x_i))^2$ to be small. The issue is if we do not place constraints on the function $g()$ then we can always make the RSS 0 and ultimately overfit the data. A natural way to do this is to find a function $g()$ so:\n",
    "$$\\sum_{i = 1}^n\\ (y_i - g(x_i))^2\\ + \\lambda\\ \\int\\ g''(t)^2\\ dt$$\n",
    "\n",
    "where $\\lambda$ is a nonnegative tuning parameter. The function $g()$ that minimizes the above equation is known as a smoothing spline. The term:\n",
    "$$\\sum_{i = 1}^n\\ (y_i - g(x_i))^2\\$$\n",
    "\n",
    "is a loss functionthat encourages $g$ to fit the data well, and the term:\n",
    "$$\\lambda\\ \\int\\ g''(t)^2\\ dt$$\n",
    "\n",
    "is a penalty that penalizes the variability in $g$. $g''$ indicates the second derivative of $g$. The first derivative $g'(t)$ measures the slope of a function $t$. The second derivative corresponds to the amount by which the slope is changing (measure of roughness). It is large in absolute value if $g(t)$ is very wiggly near $t$, and close to 0 otherwise. (Second derivative of a straight line is 0). $\\int$ is the integral, which we can think of as the summation over the range of $t$. \n",
    "\n",
    "Overall, the penalty is a measure of the total change in the function $g'(t)$, over its entire range. If $g$ is smooth then $g'(t)$ will be close to a constant and the penalty will take on a small value. COnversely if $g$ is jumpy and variable then $g'(t)$ will vary significantly and the penalty will result in a large value. The larger $\\lambda$ the smoother this penalty. When 0 the penalty has no effect and $g$ will be jumpy and exeactly interpolate the training data. When $\\lambda$ approaches $\\infty$ $g$ will be perfectly smooth (straight line that passes as closely as possible to the training point).\n",
    "\n",
    "$\\lambda$ controls the bias-variance tradeoff of a smoothing spline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the Smoothing Parameter \\lambda\n",
    "As the value of $\\lambda$ increases from 0 to $\\infty$, the effective degrees of freedom $df_{\\lambda}$ decrease from $n$ to 2. The effective degree of freedom can be defined as:\n",
    "$$df_{\\lambda} = \\sum_{i = 1}^n\\ \\{ S_{lambda} \\}\\ ii$$\n",
    "\n",
    "which is the sum of the diagonal elements of the matrix $S_{\\lambda}$. In fitting a smoothing spline we do not need to select the number or locations of the knots - there will be a knot at each of the training observations. The issue is choosing the value of $\\lambda$. One solution is cross-validation. The best way to go is to use the LOOCV:\n",
    "$$RSS_{cv}(\\lambda) = \\sum_{i = 1}^n\\ (y_i - \\hat g_{\\lambda}^{-i}\\ (x_i))^2 = \\sum_{i = 1}^n\\ \\frac{ (y_i - \\hat g_{\\lambda}\\ (x_i)) }{ 1 - \\{ S_{lambda}\\ ii \\} }^2$$\n",
    "\n",
    "and $\\hat g_{\\lambda}^{-i}$ is the fitted value for the smoothing spline evaluated at $x_i$ where all the fit uses all the training observations except the $i$th observation $(x_i, y_i)$. In contrast $\\hat g_{\\lambda}\\ (x_i)$ indicates a smoothing spline function fit to all the training observations and evaluated at $x_i$. This formula says that we can compute each of the LOO fits using only $\\hat g_{\\lambda}$, the original fit to all the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Local Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local regression computes the fit at target point $x_0$ using only nearby training observations. To obtain a local regression fit at a new point we need to fit a new weighted least-squares regression model by minimizing for a new set of weights. Local Regression is also referred to as a memore-based procedure (need all training data in order to compute a prediction). \n",
    "\n",
    "The span $s$ plays a role similar to $\\lambda$ in smoothing splines. The smaller $s$ will result in local and wiggly fit. A large $s$ will lead to a global fit. \n",
    "\n",
    "###### Algorithm\n",
    "- Gather the fraction $s = \\frac{k}{n}$ of training points whose $x_i$ are closest to $x_0$\n",
    "- Assign a weight $K_{i0} = K(x_i, x_0)$ to each point in this neighborhood, so that the point furthest from $x_0$ has weight 0 and the closest point has the highest weight. All but the knn get weight zero.\n",
    "- Fit a weighted least squares regression of $y_i$ on the $x_i$ using the aforementioned weights, by finding $\\hat \\beta_0$ and $\\hat \\beta_1$ that minimize:\n",
    "$$\\sum_{i = 1}^n\\ K_{i0}\\ (y_i - \\beta_0 - \\beta_1\\ x_i)^2$$\n",
    "- The fitted value at $x_0$ is given by $\\hat f(x_0) = \\hat \\beta_0 + \\hat \\beta_1\\ x_0$\n",
    "\n",
    "Local regression can perform poorly if $p$ is larger than 3 or 4 because there will generally be few training oservations near $x_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Generalized Additive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extends a linear model by allowing non-linear functions of each of the variables while maintaining additivity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAMs for Regression Problems\n",
    "A way to extend the multplie linear regression model is to replace each linear component with a smooth non-linear function $f_j(x_{ij})$. We then write the model as:\n",
    "$$y_i = \\beta_0 + f_1(x_{i1}) + ... + f_p(x_{ip}) + \\epsilon$$\n",
    "\n",
    "Backfitting fits a model involving multiple predictors by repeatedly updating the fit for each predictor in turn, holding the others fixed. This approach allows each time a function updates we can apply the fitting method for that variable to a partial residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros\n",
    "- GAMs allow us to fit non-linear $f_j$ to each $X_j$ so we can model non-linear relationships that standard linear regression will miss. This means we do not need to try many different transformations on the each variable individually.\n",
    "- The non-linear fits can potentially make more accurate predictions for the response $Y$.\n",
    "- Because the model is additive, we can still examine the effect of $X_j$ on $Y$ individually while holding all of the other variables fixed. \n",
    "- The smoothness of the function $f_j$ for the variable $X_j$ can be summarized via degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cons\n",
    "- Model is restricted to be additive. This can miss important interactions. We can, however, add interaction terms to the GAMs model by including additional predictors of the form $X_j\\ x\\ X_k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

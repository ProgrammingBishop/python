{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 07 - Natural Langage Processing\n",
    "\n",
    "#### 7.1/7.2 - Text Mining Analytics\n",
    "Text mining focuses on the process and analytics focuses more on the result. Text Analytics turns data into high-quality information or actionable knowledge.\n",
    "- Minimize human effort\n",
    "- Supplies knowledge for optimal decision making\n",
    "- Mining focuses on the process\n",
    "- Analytics emphasizes on the result\n",
    "- Text retrieval is preprocessing for text mining\n",
    "- Text retrieval is needed for knowledge provenance; turning text data into actionable knowedge\n",
    "\n",
    "###### Data Mining Problems\n",
    "- Dealing with text/non-text data (numerical, categorical, video, relational).\n",
    "\t- Non-text data collected from sensors\n",
    "\t- Text data collected from humans\n",
    "- Problem is turning all data into actionable knowledge as output to change the world.\n",
    "\n",
    "The real world is percieved by the observer that is human, who will then go and express their perceptions of what they percieved for text data. \n",
    "- You can mine knowledge about a language expressed as text data\n",
    "- You can mine the content of text data observed by the user\n",
    "- You can mine knowledge about the observer\n",
    "- You can infer other real-world variables (predictive analytics)\n",
    "\n",
    "Non-text data can help establish context. We can partition text data into different time periods, different locations, or any metadata that form interesting comparisons. Non-text data can help make context-sensitive analysis of content or language usage or opionions about the observer or authors of text data.\n",
    "\n",
    "###### Course Overview\n",
    "- NLP and Text Representation\n",
    "- Word Association Mining and Analysis\n",
    "- Topic Mining and Analysis\n",
    "- Opinion Mining and Sentiment Analysis\n",
    "- Text-based Prediction\n",
    "\n",
    "#### 7.3/7.4 - Natural Language Content Analysis\n",
    "\n",
    "###### Basic Concepts in NLP\n",
    "- Lexical analysis is Part-of-speech tagging. Which words are nouns, verbs, adjectives, etc. \n",
    "- Syntactical Parsing is finding patterns in lexicon. Verb phrase, noun phrase, prepositional phrase, etc.\n",
    "- Semantic Analysis is binding words and phrases into symbols. A woman being a type of human for example.\n",
    "- Inference is about understanding the world given content. A woman in a sentence about sadness would be sad. Someone doing an action like running from another entity would be in fear.\n",
    "- Pragmatic Analysis is all about the purpose of a sentence.\n",
    "\n",
    "###### What We Can't Do \n",
    "- 100% POS tagging (ambiguity)\n",
    "- General complete parsing (ambiguity)\n",
    "- Precise deep learning semantic analysis (true definition of certain words)\n",
    "\n",
    "###### Computer Steps\n",
    "- Segment the words\n",
    "- Understand the categories (Lexical Analysis; i.e. part-of-speech tagging)\n",
    "- Understand relationships between the words (Syntactical Parsing; i.e. parsing)\n",
    "- Make inference from the context of the above steps\n",
    "\n",
    "The representation of the all of the above through symbols, which is also known as Semantic Analysis. After this has been established, then we can make inference from the context of the sentence. Pragmatic Analysis is the use of language (why someone would say something).\n",
    "\n",
    "#### 7.5/7.6 - Text Representation\n",
    "Take a string of text and transform into a sequence of words. With this word sequence you can then perform parts-of-speech analysis, but we will have two layers at this point: sequence and parts-of-speech. Once we have POS, we generate a syntactic structure for words and their associations. And then from this we can understand the context of words like a name is a person, and place is a location, etc. This is forming entities and relations. After this is logic predicates, which is essentailly inferencing rules.\n",
    "\n",
    "As this process goes from beginning to end generality is lost (requires more human effort and less accurate, but closer to knowledge representation)t. The final step is understanding is speech acts; the why behind why the sentence exists. \n",
    "\n",
    "| Text Representation  | Enabled Analysis  | Examples |\n",
    "|-|-|-|-|-|\n",
    "| String | String Processing | Compression |\n",
    "| Words | Word relations analysis; topic analysis; sentiment analysis | Thesaurus discovery; topic and opinion related discovery |  \n",
    "| Syntactic Structures | Syntactic Graph | Stylistic Analysis; structure-based feature extraction |  \n",
    "| Entities and relations | Knowledge graph analysis; information network analysis  | Discovery of knowledge and opinions about specific entities |  \n",
    "| Logic and Predicates | Integrative analysis of scattered knowledge; logic inference | Knowledge assistant for biologists |\n",
    "\n",
    "#### 7.7/7.8 - Word Association Mining\n",
    "Basic word Relations:\n",
    "- Paradigmatic: A & B have this if they can be substituted for each other (same class; sentence will still make sense). Words with high context similarity will likely have this relation.\n",
    "- Syntagmatic: A & B have this if they can be combined with each other (Noun and a verb). Words with high co-occurences but relatively low individual occurrences likely have this relation.\n",
    "- Joint DIscovery of two relations: paradigmatically related words tend to have syntagmatic relation with the same word.\n",
    "\n",
    "###### Why Mine Word Associations?\n",
    "- Useful for improving accuracy of NLP\n",
    "\t- POS tagging\n",
    "\t- Parsing\n",
    "\t- Entity Recognition\n",
    "\t- Acronym Exapnsion\n",
    "\t- Grammar Learning\n",
    "- Useful for text retrieval and mining\n",
    "\t- Text Retrieval (word associations suggest variation of a query)\n",
    "\t- Automatic construction of topic map for browsing (words as nodes and associations as edges)\n",
    "\t- Compare and summarzie opinions\n",
    "\n",
    "Left context are word/s that appear before the word being analyzed. Right context are for the right of the word/s being analyzed. Even a general context analyzies the text around the word being analyzed. This is the study of paradigmatic relations.\n",
    "\n",
    "Syntagmatic relations studies the words that appear in text given the presence/absence of other words. This can even go as deep as words that appear left and right of the word being analyzed. \n",
    "\n",
    "###### General Ideas of Mining Word Associations\n",
    "- Paradigmatic\n",
    "\t- Represent each word by its context\n",
    "\t- Compute context similarity\n",
    "\t- Words with high similarity likely have paradigmatci relation\n",
    "- Syntagmatic\n",
    "\t- Count how many times two words occur together in a context\n",
    "\t- Compare their co-occurences with their individual occurrences\n",
    "\t- Words with a high co-occurrence but relatively low individual occurences likely have a high syntagmatic relation.\n",
    "\n",
    "Word context are known as a pseudo document. Left context are words that can appear before the word. Right context are the words that appear after the word. Window context are a number of words that appear around the word. This is essentially a bag-of-words. But context may contain adjacent or non-adjacent words.  \n",
    "\n",
    "###### Measuring Context Similarity\n",
    "$$Sim(Cat, Dog) = Sim(Left1(Cat), Left1(Dog)) + Sim(Right1(Cat), Right1(Dog)) + ...$$\n",
    "If this is high then the words a paradigmatically related.\n",
    "\n",
    "Imagine bag-of-words as a Vector Space Model. Each vector will be like a function of the pseudo-document given the target word as a parameter, and then measure the similarity of the word vectors. In the VSM will be a frequency vector representing the context. Each word is a dimension, and track the count of each word.\n",
    "\n",
    "###### Expected Overlay of Words in Context (EOWC)\n",
    "$$x_i = \\frac{ c( w_i, d_1 ) }{ \\mathbf{ | d_1 | } }$$\n",
    "$$y_i = \\frac{ c( w_i, d_2 ) }{ \\mathbf{ | d_2 | } }$$\n",
    "$$sim(X, Y) = \\sum_{i = 1}^N\\ X_i dot Y_i$$\n",
    "\n",
    "is the probability that two selected words picked at random are identical. We want to analyze the similarity between $x_1$ and $y_i$. Each word has a weight that is the probability that a word taken from the vector of words is the target word, given the context. All words in the vector $d_j$ will sum to one, since they are normalized. \n",
    "\n",
    "Problem is that this favors one frequent term very well over matching more distinct terms. It treats every word equally, which is not ideal for common stop words. \n",
    "\n",
    "- Sublinear Transformation of Term Frequency (TF)\n",
    "- Reward Matching a Rare Word (Inverse Document Frequency: IDF)\n",
    "\n",
    "#### 7.9 - Paradigmatic Relation Discovery\n",
    "###### BM25 Transformation\n",
    "Imagine the x-axis is $x = c(w, d)$ and the y-axis is $y = TF(w, d)$.\n",
    "$$y = \\frac{(k + 1)x}{x + k}$$\n",
    "\n",
    "where $k$ is a parameter and $x$ is a raw count of a word. The upper bound is $k + 1$. This puts a strict constraint on high frequency terms.\n",
    "\n",
    "###### IDF Weighting: Penalize Popular Terms\n",
    "x-axis is $k$ and the y-axis is $IDF(w)$. $M$ is the total number of docs in collection and $k$ is the total number of docs containing $w$.\n",
    "$$IDF(w) = log[\\frac{M + 1}{k}]$$\n",
    "\n",
    "this function gives a higher value for a lower $k$. THe lowest value is when $k$ reaches its' maximum, which is $M$: all docs contain the word. THe smallest value is only one document containing the word and therefore the word is rare.\n",
    "\n",
    "###### Adapting BM25 for Paradigmatic Relation Mining\n",
    "$d_1 = (x_1, ..., x_N)$ and $d_2 = (y_1, ..., y_N)$.\n",
    "$$BM25( w_i, d_1 ) = \\frac{ ( k + 1 ) * c( w, d_1 ) }{ c( w_i, d_1 ) + k( 1 - b + ( b * \\frac{ \\mathbf{ | d_1 | } }{ AVDL } ) ) } * log( \\frac{ m + 1 }{ df(  w ) } )$$\n",
    "\n",
    "$$x_i = \\frac{ BM25( w_i, d_1 ) }{ \\sum_{ j = 1 }^N BM25( w_j, d_1 ) }$$\n",
    "\n",
    "where $b \\in [ 0, 1 ]$ and $k \\in [ 0, + \\infty )$. $k$ sets the upper bound and controls the linear transformation. $b$ controls length normalization.\n",
    "$$Sim( d_1, d_2 ) = \\sum_{ i = 1 }^N IDF( w_i )x_iy_i$$\n",
    "\n",
    "$y_i$ is defined similarly. THis assures that high frequency words would obtain a lower weight.\n",
    "\n",
    "BM25 can also discover syntagmatic relations. The highyl weighted terms in the context vector of word $w$ are likely syntagmatically related to $w$. \n",
    "\n",
    "###### BM25 to Discover Syntagmatic Relations\n",
    "$$IDF-weighted\\ d_1 = (x_1 * IDF(w_1), ..., x_N * IDF(w_N))$$\n",
    "\n",
    "$x_i$ only reflects how frequent a word occurs in context. IDF Weighting makes it so that common words will not be the highest weighted terms. The highest weighted terms are the terms frequent in the context, but not in the collection.\n",
    "\n",
    "###### The main idea for discovering paradigmatic relations\n",
    "- Collecting the context of a candidate word to form a pseudo document (bag of words)\n",
    "- Computing similarity of the corresponding context documents of two candidate words\n",
    "- Highly similar word pairs can be assumed to have paradigmatic relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

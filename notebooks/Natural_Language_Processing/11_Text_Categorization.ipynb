{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 11 - Text Categorization Continued\n",
    "\n",
    "#### 11.1 - Discriminative Classifier Part I \n",
    "You will have a binary response variable, where each value maps to one of two categories.\n",
    "\n",
    "###### Conditional Likelihood\n",
    "$$p(T | \\bar \\beta) = \\prod_{i = 1}^{\\mathbf{|T|}}\\ p(Y = Y_i | X = X_i, \\bar \\beta)$$\n",
    "\n",
    "###### Estimation of Parameters (Logistical Function)\n",
    "This will map the range of $x$ to 0 and 1. Training data $T$ and parameters $\\bar \\beta$ leads to the conditional likelihood $p(T | \\bar \\beta) = \\prod_{i = 1}^{\\mathbf{|T|}}\\ p(Y = Y_i | X = X_i, \\bar \\beta)$ where:\n",
    "\n",
    "$$p(Y = 1 | X) = \\frac{e^{\\beta_0 + \\sum_{i = 1}^M\\ x_i\\beta_i}}{e^{\\beta_0 + \\sum_{i = 1}^M\\ x_i\\beta_i} + 1}$$\n",
    "$$p(Y = 0| X) = \\frac{1}{e^{\\beta_0 + \\sum_{i = 1}^M\\ x_i\\beta_i} + 1}$$\n",
    "\n",
    "and Maximum likelihood is:\n",
    "$$\\bar \\beta^{*} = argmax\\ p(T | \\bar \\beta)$$\n",
    "\n",
    "###### KNN\n",
    "- Find $k$ examples in train most similar to text object to be classified\n",
    "- Assign category most common in these neighbor text objects\n",
    "- Improve by considering distance of a neighbor\n",
    "- A way to directly estimate conditional probability of label\n",
    "- Need similarity function to measure similarity between two text objects \n",
    "\n",
    "Assume $p(\\theta_i | d)$ is locally smooth; the same probability for all the $d$ in each region $R$. Estimate $p(\\theta_i | R)$ based on the known categories in the region $R$.\n",
    "$$p(\\theta_i | R) = \\frac{c(\\theta_i, R)}{\\mathbf{|R|}}$$\n",
    "\n",
    "where $\\theta_i$ represents documents with category.\n",
    "\n",
    "#### 11.2 - Discriminative Classifier Part II (Optional)\n",
    "\n",
    "#### 11.3 - Evaluation Part I\n",
    "###### General Evaluation Method (Cranfield Methodology)\n",
    "- Have people create test collection where every document is tagged with desired categories\n",
    "- Generate categorization decisions with people-made categorization decisions and quantify their similarity\n",
    "- Higher similarity the better the results\n",
    "\n",
    "###### Classification Accuracy\n",
    "Measures the percentage of correct decisions. Matrix columns are the categories $c_i$ and the rows are the documents $d_j$. The elements are a binary classification based on whether or not the document has been classified to match that of a person's classification.\n",
    "$$accuracy = \\frac{Number of Correct Decisions}{Number of Decisions}$$\n",
    "\n",
    "###### Problems with Classification\n",
    "Decision error depends on the application; some decisions for some documents or categories are more important than others. To avoid skewed test results, place all instances in a single category.\n",
    "\n",
    "|*|System (y) |System (n)|\n",
    "|-|-|-|\n",
    "|Human (+) |True Positive |False Negative |\n",
    "|Human (-)  |False Positive |True Negative |\n",
    "\n",
    "- Precision: $\\frac{TP}{TP + FP}$; when system says yes how many are correct\n",
    "- Recall: $\\frac{TP}{TP + FN}$ and answers if document has all categories it should have; does the doc have all the categories it should have.\n",
    "\n",
    "###### Per-Category Evaluation\n",
    "- Precision: when system says yes how many are correct\n",
    "- Recall: has the category been assigned to all the docs of this category\n",
    "\n",
    "###### F-Measure (Harmonic Mean)\n",
    "$$F_{\\beta} = \\frac{(\\beta^2 + 1) P * R}{\\beta^2 P + R}$$\n",
    "\n",
    "$$F_1 = \\frac{2PR}{P + R}$$\n",
    "\n",
    "where $F_1$ is more popular to use.\n",
    "\n",
    "#### 11.4 - Evaluation Part II\n",
    "###### Macro Average Over All Categories\n",
    "For each category we will compute precision, recall, and $F_1$. Once computed, then aggregate all the values for all the categories to get overall precision, recall, and $F_1$.\n",
    "- Arithmatic Mean affected by high values\n",
    "- Geometric Mean affected by low values\n",
    "\n",
    "Do this same process for all of the documents, instead of all categories/topics. Macro Averaging is typically more helpful and informative than Micro Averaging.\n",
    "\n",
    "###### Micro Averaging of Precision and Recall\n",
    "Pool all decisions and then compute precision and recall (counting the number of cases in each of the four categories in table above.). Aim to treat all instances equally, but this may be commonly not helpful.\n",
    "\n",
    "###### Ranking\n",
    "Categorization can be passed to people for:\n",
    "- Further editing\n",
    "- Prioritizing a task\n",
    "\n",
    "In such cases we can evaluate results as a ranked list if the system can give scores for the decisions\n",
    "- Discovery of spam emails\n",
    "- Appropriate to frame problem as a ranking problem instead of a categorization problem\n",
    "\n",
    "###### Categorization Evaluation\n",
    "- Commonly used measures for relative comparison\n",
    "\t- Accuracy, precision, recall, F-measure\n",
    "\t- Variation: per-document, per-category, micro, macro averaging\n",
    "- Ranking could be better as ranking\n",
    "\n",
    "#### 11.5 - Sentiment Analysis\n",
    "###### Opinion Representation\n",
    "- Opinion Holder: whose opinion is this? Reviewer $X$\n",
    "- Opinion Target: what opinion is about? Product $P$\n",
    "- Opinion Content: what exactly is the opinion? Review Text $T$\n",
    "\n",
    "###### Enriched Opinion Representation\n",
    "- Opinion Context: what situation was the opinion expressed (time or location)? Context $C$\n",
    "- Opinion Statement: what does the opinion tell us about the opinion holder's feeling? Sentiment $S$\n",
    "\n",
    "###### Variations of Opinions\n",
    "- Opinion Holder: Individual or Group\n",
    "- Opinion Target: one entity, group of entities, one attribute of entity, or someone else's opinion\n",
    "- Opinion Content: \n",
    "\t- Surface Variation: one sentence or phrase, paragraph, article, etc.\n",
    "\t- Sentiment/Emotion Variation: positive or negative\n",
    "- Opinion Context:\n",
    "\t- Simple Context: different time, location, etc.\n",
    "\t- Complex Context: potentially includes entire discourse context of an opinion\n",
    "\n",
    "###### Different Kinds of Opinions in Text Data\n",
    "- Author Opinion\n",
    "- Reported Opinion\n",
    "- Observed Opinion\n",
    "- Indirect/Inferred Opinions\n",
    "\n",
    "###### Opinion Mining Pipeline\n",
    "- Take Text Data\n",
    "- Generate Opinion Representation Set\n",
    "\t- Opinion Holder\n",
    "\t- Opinion Target\n",
    "\t- Opinion Content & Context to create Opinion Sentiment\n",
    "\n",
    "###### Why Opinion Mining\n",
    "- Decision Support\n",
    "\t- Help choose products or services\n",
    "\t- Help decide vote choices\n",
    "\t- Help design new policies\n",
    "- Understand People\n",
    "\t- People preferences to better serve them\n",
    "\t- Advertising\n",
    "- Voluntary Survey\n",
    "\t- Business Intelligence\n",
    "\t- Market Research\n",
    "\t- Data-Driven Social Science Research\n",
    "\t- Gain advantage in any prediction\n",
    "\n",
    "#### 11.6 - Sentiment Classification\n",
    "###### Task Definition\n",
    "- Input is an opinionated text object\n",
    "- Output is a sentiment tag / label\n",
    "\t- Polarity Analysis: Categories like positive or negative or numbered factors\n",
    "\t- Emotion Analysis: Categories based on mood\n",
    "- Any text classification can be used to do sentiment classification\n",
    "- Improvements:\n",
    "\t- More sophisticated features for sentiment tagging\n",
    "\t- Consideration of the order of the categories\n",
    "\n",
    "###### Common Text Features\n",
    "- Character n-grams: general and robust to spelling/recognition errors, but less discriminative than words\n",
    "- Word n-grams: Unigrams are often very effective, but not for sentiment analysis. Long n-grams are discriminative, but may cause overfitting.\n",
    "- Parts-of-Speech Tagging n-grams: tags with categories like noun, verb, adjective, etc.\n",
    "- Word Classes\n",
    "\t- Syntactic\n",
    "\t- Semantic\n",
    "\t- Empirical Word Clusters\n",
    "- Frequent Patterns in Text\n",
    "\t- More specific/discriminative than words\n",
    "\t- May generalize better than pure n-grams\n",
    "- Parse tree-based\n",
    "\t- Even more discriminative, but need to avoid overfitting\n",
    "\n",
    "Optimize the tradeoff between exhaustivity (high coverage) and specificity (discriminative) is the major goal.\n",
    "\n",
    "###### Feature Construction for Text Categorization\n",
    "- Feature design affects categorization accuracy significantly\n",
    "- Combination of ML, error analysis, and domain knowledge is most effective:\n",
    "\t- Domain knowledge -> seed features, feature space\n",
    "\t- ML -> feature selection, feature learning\n",
    "\t- Error analysis -> feature validation\n",
    "- NLP enriches text representation\n",
    "- Optimize tradeoff between exhaustivity and specificity\n",
    "\t- exhaustivity is frequency\n",
    "\t- specificity is discriminative (infrequent)\n",
    "\n",
    "#### 11.7 - Ordinal Logistic Regression (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

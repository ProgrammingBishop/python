{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 08 - Word Association Mining\n",
    "\n",
    "#### 8.1 - Syntagmatic Relation Discovery: Entropy\n",
    "Syntagmatic Relation are words with correlated occurrences. Whenever word $w_i$ occurs, what other words also tend to occur?\n",
    "\n",
    "Word Prediction:\n",
    "$$X_w \\in \\{\\, 0, 1 \\}\\,$$ \n",
    "\n",
    "where 0 or 1 corresponds to present or not present.\n",
    "$$p( X_w = 1 ) + p( X_w = 0 ) = 1$$\n",
    "\n",
    "###### Entropy\n",
    "How do we measure the randomness of $X_w$? Entropy! $H(X)$ measures the randomness of $X$.\n",
    "$$H(X_w) = \\sum_{ v \\in \\{\\, 0, 1 \\}\\, } - p( X_w = v ) log_2 p( X_w = v )$$\n",
    "\n",
    "where $v$ is the value. If $v$ represents a coin, then the $\\sum$ will contain two values. If plotting, y-axis is $H(X_w)$ and x-axis is $P( X_w = 1 )$. When the probability of $X = 1$ is small or large, the value of entropy is small. If $X = 0.5$ then the value of entropy is very large. Smaller entropy is easier to predict. Smaller entropy means the probability of an event is biased, which is why it is easier to predict. So common words and rare words are easy to predict, but words that are neither are harder, and these words have a higher entrpy.\n",
    "\n",
    "The question to ask is: for what $X_w$ does $H(X_w)$ reach its' max or min? Let's use a coin example:\n",
    "$$H( X_{ coin } ) = -p( X_{ coin } = 0 ) log_2 p( X_{ coin } = 0 ) - p( X_{ coin } = 1 ) log_2 p( X_{ coin } = 1 )$$\n",
    "\n",
    "If the coin is fair ( 0.5 Head and 0.5 Tail ), then:\n",
    "$$H(X) = -\\frac{ 1 }{ 2 } log_2 \\frac{ 1 }{ 2 } - \\frac{ 1 }{ 2 } log_2 \\frac{ 1 }{ 2 } = 1$$\n",
    "\n",
    "If the coin was biased - heads always - then $H(X)$ would be 0 because that coin would be easy to predict. If we were to apply this to word prediction, biased words would be common words, which means $H(X) = 0$ for common stop words. High entropy words are harder to predict; smaller entropy is easier to predict.\n",
    "\n",
    "#### 8.2 - Conditional Entropy\n",
    "Let's assume we know more about a text segment, like we know a certain $X_w$ occurs in the segment with another word. Does knowing the presence or absence of a word help in the prediction; does it reduce the uncertainty therefore decreasing entropy. \n",
    "$$p( X_{ w1 } = 1 | X_{ w2 } = 1 )$$\n",
    "$$p( X_{ w1 } = 0 | X_{ w2 } = 1 )$$\n",
    "\n",
    "helps lead to the conditional entropy equation:\n",
    "$$H( X_{ w1 } | X_{ w2 } ) = \\sum_{ u \\in \\{\\, 0, 1 \\}\\, } [ p( X_{ w2 } = u ) \\sum_{ v \\in \\{\\,0, 1 \\}\\, } [ -p( X_{ w1 } = v | X_{ w2 } = u ) log_2 p( X_{ w1 } = v | X_{ w2 } = u ) ] ]$$\n",
    "\n",
    "The max value is the entropy of $X$. The min would be 0; $H( X_{ w1 } | X_{ w1 } )$. This is because there is a smaller entropy for terms that related to other terms. So the more related a word is, the lower the entropy and therefore the higher the ease of predicting. The less related a word is the higher the entropy and therefore a lower ease of predicting.\n",
    "\n",
    "###### Conditional Entropy for Mining Syntagmatic Relations\n",
    "For each word $W_1$\n",
    "- For every other word $W_2$, compute the conditional entropy $H( X_{ w1 } | X_{ w2 } )$\n",
    "- Sort all candidate words in ascending order of entropy. Lower entropy first, which means the words that are easier to predict come first.\n",
    "- Take the top-ranked candidate words as words that have potential syntagmatic relations for each word $W_1$\n",
    "- Need to use a threshold for each $W_1$\n",
    "\n",
    "#### 8.3 - Mutual Information: Measuring Entropy Reduction\n",
    "Answers the question: how much reduction in the entropy of $X$ can be obtained by knowing the entropy of $Y$?\n",
    "$$I( X; Y ) = H(Y) - H( Y | X )$$\n",
    "$$I( X; Y ) = H(X) - H( X | Y )$$\n",
    "\n",
    "While the actual entropy of the two above equations are different, the reduction of entropy from the above equations are equal. Some properties:\n",
    "- Non-negative\n",
    "- Symmetric\n",
    "- Is equal to 0 iff $X$ & $Y$ are independent (Knowing the condition does not help in prediction)\n",
    "\n",
    "Which word $W_2$ will have a high mutual information with word $W_1$?\n",
    "\n",
    "###### Rewriting Mutual Information using KL-Divergence\n",
    "KL-Divergence measures the divergence between two distributions.\n",
    "$$I(X_{w1}; X_{w2}) = \\sum_{ u \\in \\{\\, 0, 1 \\}\\, } \\sum_{ v \\in \\{\\,0, 1 \\}\\, } p( X_{ w1 } = u, X_{ w2 } = v ) log_2 \\frac{ p( X_{ w1 } = u, X_{ w2 } = v ) }{ p( X_{ w1 } = u ) p(  X_{ w2 } = v ) }$$ \n",
    "\n",
    "The numerator is the joint distribution of $X_{ w1 }$ and $X_{ w2 }$ and the denominator is the expected joint distribution of the terms is the terms $X_{ w1 }$ and $X_{ w2 }$ were independent. If the numerator and denominator are different then the two variables are not independent. The larger the divergence the higher the Mutual Information will be.\n",
    "\n",
    "The presence and absence of words sum to 1 and the co-occurrences of $W_1$ and $W_2$ all sum to 1 and involve:\n",
    "- Both occur\n",
    "- Only $W_1$ occurs\n",
    "- Only $W_2$ occurs\n",
    "- None of them occur\n",
    "\n",
    "Constraints:\n",
    "$$p( X_{ w1 } = 1, X_{ w2 } = 1 ) + p( X_{ w1 } = 1, X_{ w2 } = 0 ) = p( X_{ w1 } = 1 )$$\n",
    "$$p( X_{ w1 } = 0, X_{ w2 } = 1 ) + p( X_{ w1 } = 0, X_{ w2 } = 0 ) = p( X_{ w1 } = 0 )$$\n",
    "$$p( X_{ w1 } = 1, X_{ w2 } = 1 ) + p( X_{ w1 } = 0, X_{ w2 } = 1 ) = p( X_{ w2 } = 1 )$$\n",
    "$$p( X_{ w1 } = 1, X_{ w2 } = 0 ) + p( X_{ w1 } = 0, X_{ w2 } = 0 ) = p( X_{ w2 } = 0 )$$\n",
    "\n",
    "These constraints make it so we only need to calculate 3 probabilities as they will lead to calculating the other probabilities. These are:\n",
    "- $p( X_{ w1 } = 1 )$\n",
    "- $p( X_{ w2 } = 1 )$\n",
    "- $p( X_{ w1 } = 1, X_{ w2 } = 1 )$\n",
    "\n",
    "#### 8.4 - Mutual Information Part II\n",
    "Estimating Probabilities\n",
    "$$p(X_{w1} = 1) = \\frac{count(w1)}{N}$$\n",
    "$$p(X_{w2} = 1) = \\frac{count(w2)}{N}$$\n",
    "$$p(X_{w1} = 1, X_{w2} = 1) = \\frac{count(w1, w2)}{N}$$\n",
    "\n",
    "Normalize the 3 probabilities by: \n",
    "- Getting the count of the word $W_i$ within each segment\n",
    "- Dividing that count by $N + 1$, which is the total number of segments plus the total of all of the pseudo segment values. This is a smoothing constant to avoid 0 probability.\n",
    "\n",
    "To accomodate 0 counts it is best to add a small constant to the counts. These are pseudo counts. Pseudo counts set, for each segment, $\\frac{ 1 }{ N }$. So is a word is present in only one segment then it gets $\\frac{ 1 }{ N }$.\n",
    "\n",
    "###### Summary\n",
    "- Syntagmatic Relation discovers the correlations between occurrences of two words\n",
    "- Information Theory Concpets\n",
    "\t- Entropy: measures the uncertainty of a random variable X\n",
    "\t- Conditional Entropy: entropy of X given we know Y\n",
    "\t- Mutual Information: entropy reduction of X due to knowing Y\n",
    "- Mutual Information provides principled way for discorvering syntagmatic relations.\n",
    "\n",
    "#### 8.5 - Motivation and Task Definition\n",
    "Topic is a subject of a discussion or conversation. They tell us something about the world, like opinions of a product or an event in current events. Applications include:\n",
    "- What are Twitter users talking about today?\n",
    "- What are current research topics in data mining?\n",
    "- What do people like about a product?\n",
    "\n",
    "Non-text data can supply other context like time, locations, authors, etc. This metadata can be associated with the topics and these can further assist in finding patterns. \n",
    "\n",
    "Tasks of topic mining is to take text data, discover the range of topics, and then decide which documents are associted with which topics. The formal definition is as follows:\n",
    "- Input\n",
    "\t- Collection $C$ of $N$ text documents\n",
    "\t- Number of topics $k$\n",
    "- Output \n",
    "\t- $k$ topics $\\{\\, \\theta_1, ..., \\theta_k \\}\\,$\n",
    "\t- Coverage of topics in each $d_i$: $\\{\\, \\pi_{ i1 }, ..., \\pi_{ ik } \\}\\,$\n",
    "\t- $\\pi_{ ij }$ is the probability of $d_i$ covering topic $\\theta_j$\n",
    "\n",
    "$$\\sum_{ j = 1 }^k \\pi_{ ij } = 1$$\n",
    "\n",
    "#### 8.6 - Term as Topic\n",
    "Term can be a word or a phrase, and these are the $\\theta_k$. Then for each document you want to calculate what percent of each document covers each term. Topics are the rows, documents are the columns. and the elements of a matrix are the $pi$s, which represent the percentage of the document that is covered by the topic.\n",
    "\n",
    "Process:\n",
    "- Parse text in $C$ to obtain candidate terms\n",
    "- Design scoring function to measure how good each term is as a topic\n",
    "\t- Favor terms with high frequency\n",
    "\t- Avoid words too frequent like stop words\n",
    "\t- TF-IDF (Term Frequency - Inverted Index Frequency) weighting from retrieval can be useful\n",
    "\t- Favor title words, hashtags in tweets, tags from posts in Pinterest or Instagram, etc.\n",
    "- Pick $k$ terms (via greedy algorithm) - become $\\theta_k$ - with the highest scores but try to minimize redundancy. So if multiple terms are similar/related, pick one of them and ignore the others\n",
    "\n",
    "Compute topic coverage $\\pi_{ ij }$\n",
    "$$\\pi_{ ij } = \\frac{ count( \\theta_j, d_i ) }{ \\sum_{ L = 1 }^k count( \\theta_L, d_i ) }$$\n",
    "\n",
    "Which is just the count of the term in each document over the sum of all just so the value is normalized.\n",
    "\n",
    "One issue with this approach is that it does not consider related words. So there needs to be a method to count similar words. Also, some words in document can be ambiguous (two meanings).\n",
    "- Lack of expressive power; represent simple topics. Solve by adding more words to describe a topic\n",
    "- Incomplete in vocabulary coverage; does not capture variations in vocabulary. Solve by adding weights\n",
    "- Word sense ambiguity. Solve by splitting ambiguous words\n",
    "\n",
    "#### 8.7 - Probabilistic Topic Models\n",
    "Improve by representing a topic as a word distribution.\n",
    "$$\\sum_{ w \\in V } p( w | \\theta_i ) = 1$$\n",
    "\n",
    "with all words having non-zero probabilities. These probabilities is the likelihood a word is associated with $\\theta_i$. This approach allows an association of many words to represent a topic. A word distribution allows multiple words to describe a complex topic and allows the addition of weights to words (model semantic variations).\n",
    "\n",
    "Inputs:\n",
    "- Collection of $N$ text documents\n",
    "- Vocabulary $V$\n",
    "- Number of topics $k$, and $k$ is a word distribution\n",
    "\n",
    "Output:\n",
    "- $k$ topics with each a word distribution that sums to 1\n",
    "- Coverage of topics in each document\n",
    "- Probability of document covering each top: this sume to 1\n",
    "\n",
    "For the computation task, the inputs are $C$, $k$, and $V$ and the output are the topics $\\theta_k$ and the coverage of topics in each document $\\pi_{ ij }$.\n",
    "\n",
    "###### Generative Model for Text Mining\n",
    "- Design model for data $P( Data | Model, \\Lambda )$ where $\\Lambda$ are the $\\theta$ and $\\pi$. Since we have $N$ documents we will have $N$ sets of $\\pi$. Each set of $\\pi$ values will sum to 1. The total number of parameters is $N x k$. $N$ will be the rows, $\\theta$ will be the columns, and $\\pi$ will be the elements.\n",
    "\n",
    "Estimate the parameters given the data by generating a plot:\n",
    "- y-axis is $P(Data | Model, \\Lambda)$\n",
    "- x-axis is $\\Lambda^*$\n",
    "\n",
    "and the goal is to adjust the parameter values to return the highest probability. So again, generative model is creating a model with parameters, fit the model, and then adjust the parameters so to achieve the highest probability.\n",
    "\n",
    "Constraints:\n",
    "- Each worded distribution must have probabilities sum to 1\n",
    "- A document is never allowed to recover a topic outside the set of topics discovered\n",
    "\n",
    "#### 8.8 - Statistical Language Models\n",
    "This is a probability distribution over word sequences. This can also be a probabilistic mechanism for generating text, thus called a generative model.\n",
    "\n",
    "###### Unigram Langauage Model is the Simplest Model\n",
    "Generates text by generating each word independently. Thus the probability of a word sequence is the product of the word sequence terms. Text is a sample drawn according to the word distribution. Draw a word one at a time and eventually we will generate text. \n",
    "\n",
    "The probability of generating a text sequence is the product of each term's probability in that sequence. Two problems to think about:\n",
    "- Sampling process $p( d | \\theta )$: given a model, how likely are we to observe a certain kind of data points?\n",
    "- Estimation process $p( w |\\theta )$: think about parameters of model given. What language model is most likely responsible for generating the text. Each term probability is the count of that term over the total number of words in the document. Is this optimal?\n",
    "\n",
    "#### 8.9 - Statistical Language Models Part II\n",
    "###### Maximum Likelihood Estimation\n",
    "$$\\hat \\theta = argmax_{ \\theta } P( X | \\theta )$$\n",
    "\n",
    "where best is the data likelihood has reached the maximum. It's not the function but rather the argument that has allowed the function to reach its maximum, which is $\\theta$ in this case. This can be an issue if the sample is too small as bias comes into play.\n",
    "\n",
    "###### Bayesian Estimation\n",
    "$$P( X | Y ) = \\frac{ P( Y | X )P( X ) }{ p( Y ) }$$\n",
    "\n",
    "where best means being consistent with our prior knowledge and explaining data well and $P(X)$ is our belief before observing any data. $P( X | Y )$ is our posterior belief about $X$. Look at data and prior knowledge $P( \\theta )$.\n",
    "$$\\hat \\theta = argmax_{ \\theta } p( \\theta | X ) = argmax_{ \\theta } P( X | \\theta ) P( \\theta )$$\n",
    "\n",
    "$P(\\theta)$ is the posterior, $P(X | \\theta)$ is the maximum likelihood, and $p(X | \\theta)\\ p(\\theta)$ is the posterior mode.\n",
    "\n",
    "This is a more general estimate than maximum likelihood estimator.  So the prior + Maximum Likelihood estimator will give the posterior, which is essentially the average of the two.\n",
    "$$\\hat f( \\theta ) = \\sum_{ \\theta } f( \\theta ) p( \\theta | X )$$\n",
    "\n",
    "posterior mean:\n",
    "$$\\hat \\theta = \\sum_{ \\theta } \\theta * p( \\theta | X )$$\n",
    "\n",
    "#### 8.10 - Mining One Topic\n",
    "Language Model Setup:\n",
    "- Data id document $d = x_1 * ... * x_{ \\mathbf{ | d | } }$ and $x_i \\in V = { w_1, ..., w_M }$ is a word.\n",
    "- Model is a Unigram Language Model where $\\theta(topic) : \\theta_i = p( w_i | \\theta )$, $i = 1, .., M$\n",
    "- Likelihood function is $\\prod_{ i = 1 }^ M \\theta_i^{ c( w_i, d ) }$ where this is the product of all the unique words. This helps fix the issue of words occurring more than once in a document.\n",
    "- Compute the Maximum Likelihood. It is best to do log likelihood so product becomes a sum. Also, sum is easier to take the derivitive: $\\sum_{ i = 1 }^M c( w_i, d ) log\\ \\theta_i$\n",
    "\n",
    "Solve this problem with a lagrange multiplier approach:\n",
    "$$f( \\theta | d ) = \\sum_{ i = 1 }^M c( w_i, d )\\ log\\ \\theta_i + \\lambda( \\sum_{ i = 1 }^M \\theta_i - 1)$$\n",
    "\n",
    "where the term to the left of + encodes the constraint that all term probabilities must sum to 1:\n",
    "- When taking the partial derivitive in respect to $\\theta_i$ we get: $\\theta_i = - \\frac{ c( w_i, d ) }{ \\lambda }$\n",
    "- $\\lambda = - \\sum_{ i = 1 }^N c( w_i, d )$  \n",
    "- Ultimately, the normalized counts is: $\\frac{ c( w_i, d ) }{ \\mathbf{ | d | } }$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 10 - Text Clustering / Categorization\n",
    "\n",
    "#### 10.1 - Clustering Motivation\n",
    "Clustering Bias states that any two objects can be similar based on how you view them. Must define the perspective of assessing similarity. What can be clustered:\n",
    "- Documents\n",
    "- Terms\n",
    "- Passages/Sentences/Segments\n",
    "- Websites\n",
    "- Text Clusters can also be clustered\n",
    "\n",
    "Why cluster? It is useful for exploratory analysis and text mining:\n",
    "- Get sense of overall content in collection\n",
    "- Link text objects\n",
    "- Create structure on text data\n",
    "- Induce additional features for classification\n",
    "\n",
    "#### 10.2 - Clustering Generative Probabilistic Model Part I\n",
    "Two ways to do text clustering:\n",
    "- Generative Probabilistic Models\n",
    "- Similarity-Based Approaches\n",
    "\n",
    "In text clustering we only allow a document to cover one topic, topic representing a cluster. \n",
    "\n",
    "###### Generative Model for CLustering\n",
    "\n",
    "Input $C, k, v$ with the ouput $\\{\\, \\theta_1, ..., \\theta_k \\}\\,$ and $\\{\\, c_1, ..., c_n \\}\\, \\in [1, k]$ where $c_i$ represents a cluster the topic belongs to and each document can only be defined by one topic. This is similar to topic mining, except of using $\\pi$ there is the use of $C$. So instead of topic coverage for each document we will get the likelihood that a document belongs to a cluster. \n",
    "\n",
    "Within the topic mining method think of the $\\pi$ as being elements of a matrix where the columns represent the documents and the rows are the topics. $\\pi$ is the topic coverage for the document.\n",
    "\n",
    "Mine one topic:\n",
    "- Input $C = \\{\\, d \\}\\ , v$\n",
    "- Output $\\{\\, \\theta \\}\\,$\n",
    "\n",
    "Why can generative topic model not be used for clustering? Because it has allowed multiple topics to contribute words to document and therefore makes it complex to identify which topic/cluster the document is from. All words in the document must be generated from a single distribution of a topic model defined by the cluster. \n",
    "\n",
    "###### Mixture Model for Document Clustering\n",
    "Once you choose a topic you will stay with distribution to generate all words in document. Only make choice of distribution once for each document. \n",
    "\n",
    "How does the mixture model differ from the topic model?\n",
    "- Choice of distribution is made just once in mixture model\n",
    "- One distribution generates all words for document in mixture model\n",
    "\n",
    "Likelihood Function:\n",
    "$$p(d) = p(\\theta_1)\\ \\prod_{i = 1}^L\\ p(x_i | \\theta_1) + p(\\theta_2) \\prod_{i = 1}^L\\ p(x_i | \\theta_2)$$\n",
    "$$p(C|\\Lambda) = \\prod_{j = 1}^N\\ p(d_j | \\Lambda)$$\n",
    "\n",
    "which is the probability of choosing a distribution plus the probability of observing a document from that distribution.\n",
    "\n",
    "#### 10.3 - Clustering Generative Probabilistic Model Part II\n",
    "How to generalize using $k$ clusters? Use a model that is a mixture of $k$ unigram language models. $\\Lambda = ({\\theta_i}; {p(\\theta_i)}), i \\in [1, k]$. Likelihood:\n",
    "$$p(d | \\Lambda) = \\sum_{i = 1}^k [p(\\theta_i)\\ \\prod_{w \\in V} p(w | \\theta_i)^{c(w,d)}]$$\n",
    "\n",
    "Maximize likelihood by selecting the max parameters for $\\Lambda$. This essentially assigns document to cluster with highest probability of generating document.\n",
    "\n",
    "Parameters:\n",
    "- Each $\\theta_i$ represents the content of cluster $i : p(w | \\theta_i)$\n",
    "- $p(\\theta_i)$ indicates the size of cluster $i$. NOTE this does not depend on $d$\n",
    "\n",
    "Which cluster should document belong to? Assign $d$ to the cluster correpsonding to the topic $\\theta_i$ that most likely has been used to generate $d$. Likelihood + prior $p(\\theta_i)$ is the Bayesian approach and favor large clusters. Want to choose a cluster that is both large and has the highest probability.\n",
    "\n",
    "#### 10.4 - Clustering Generative Probabilistic Model Part III\n",
    "Use the EM algorithm for document clustering:\n",
    "- randomy set the parameters for $\\Lambda$\n",
    "- Repeat until likelihood $p(C | \\Lambda)$ converges\n",
    "\n",
    "\n",
    "- E-Step infers which distribution has been used to generate document $d$ using hidden variables $z_d$.\n",
    "$$p^{(n)}(Z_d = i | d) \\propto p^{(n)}(\\theta_i)\\ \\prod_{w \\in V}p^{(n)}(w | \\theta_i)^{c(w, d)}$$\n",
    "- M-Step is the re-estimation of the parameters. (Choose the distribution)\n",
    "$$p^{(n + 1)}(\\theta_i) \\propto \\sum_{j = 1}^N\\ p^{(n)}(Z_{d_j} = i | d)$$\n",
    "\n",
    "is the probability of selecting a specific distribution.\n",
    "\n",
    "$$p^{(n + 1)}(w | \\theta_i) \\propto \\sum_{j = 1}^N\\ c(w, d_j)\\ p^{(n)}(Z_{d_j} = i | d)$$\n",
    "\n",
    "is the probability of words within each distribution / cluster. \n",
    "$p(Z_d = 1 | d)$ is the Bayesian formula: numerator is probabily of selecting topic/cluster  times the probability of generating words from that topic. Denominator is the sum of all probabilities (normalize). We add a normailizer to avoid underflow. $\\bar \\theta$ is the average of generating a word form a topic.\n",
    "\n",
    "To add a normalizer, for the terms in the numerator and denominator divide by the average word distribution.\n",
    "\n",
    "#### 10.5 -Clustering  Similarity-Based Approaches\n",
    "General Idea:\n",
    "- Explicitly define a similarity function to measure similarity between two text objects\n",
    "- Find an optimal partitioning of data to maximize intra-group similarity (similar within groups ) and minimize inter-group similarity (different between groups)\n",
    "- Strategies\n",
    "\t- Progressively construct a hierarchy of clusters via bottom-up (group) or top-down (partition) (Hierarchal Agglomerative Clustering)\n",
    "\t- Start with initial clustering and iteratively improve it (K-Means)\n",
    "\n",
    "###### Agglomerative Hierarchal Clustering\n",
    "Given a similarity function to measure similarity between two objects, gradually group similar objects together in a bottom-up fashion to form a hierarchy. Stop when a threshold has been achieved.  \n",
    "\n",
    "Similarity-induced Structure: gradually group objects one-by-one based on which two are most similar, which essentially results in a binary tree. Cutoff/threshold is like pruning this binary tree. Mehtods used to group similar objects include:\n",
    "- Single-link algorithm: similarity of closest pair. Loose clusters that is sensitive to outliers.\n",
    "- Complete-link algorithm: similarity of farthest pair. Tight clusters that are also sensitive to outliers. Even the unlikely connections are feasible.\n",
    "- Average-link algorithm: average of similarity of all pairs. In-between, group decision, not sensitive to outliers.\n",
    "\n",
    "###### K-Means\n",
    "Represent each text object as a term vector and assume a similairty function on two objects.\n",
    "- Start with $k$ randomly selected vectors and assume they are the centroids of $k$ clusters. \n",
    "- WIth these $k$ centroids, assign a vector to a cluster whose centroid is the closest to the vector\n",
    "- Recompute the centroid based on the objects within the cluster\n",
    "- Repeat until function converges to a local minima (within cluster sum-of-squares)\n",
    "\n",
    "This process is very similar to the EM Algorithm clustering. K-Means differs in the E-Step because a probabilistic decision is made, but rather a user choice based on the distance of points from the centroids.\n",
    "\n",
    "#### 10.6 - Clustering Evaluation\n",
    "###### Direct Evaluation of Text Clusters\n",
    "Questions to answers: how close are system-generated clusters to the ideal clusters generated by humans. Closeness can be assessed in different ways and can be quantified. Clustering bias is imposed by the human assessors.\n",
    "\n",
    "Evaluation procedure:\n",
    "- Given test set, have humans create an ideal clustering result\n",
    "- Use a system to produce clusters from the same test\n",
    "- Quantify similarity between the system-generated clusters and the human-generated clusters. \n",
    "- Similarity can be measured from multiple perspectives (purity, normalized mutual information, F measure, etc.)\n",
    "\n",
    "###### Indirect Evaluation of Text Clusters\n",
    "How useful are the clustering results for the intended applications\n",
    "- Usefulness is application-specific\n",
    "- Clustering bias is imposed by the intended application\n",
    "\n",
    "Evaluation Proceedure\n",
    "- Create a test set for intended application to quantify the performance of any system for this application\n",
    "- Choose baseline system to compare with\n",
    "- Add clustering algorithm to the baseline system\n",
    "- Compare the performance of the clustering system and the baseline in terms of any performance measure for the application.\n",
    "\n",
    "###### Approaches\n",
    "- Strong clusters tend to show up no matter what method is used\n",
    "- Effectiveness of a method highly depends on whether the desired clustering bias is captured appropriately \n",
    "- Deciding the optimal number of clusters is generally more difficult method due to the unsupervised nature\n",
    "\n",
    "#### 10.7 - Categorization Motivation\n",
    "###### Text Categorization\n",
    "Given a set of predefined categories and often a training set of labeled text objects. The goal is to classify a text object into one or more of the categories. Internal categories characterize a text object. External categories characterize an entity associated with the text object.\n",
    "\n",
    "###### Examples of Text Categorization\n",
    "- News categorization\n",
    "- Literature article categorization\n",
    "- Spam email detection\n",
    "- Sentiment categorization of product reviews or tweets\n",
    "- Automatic email sorting\n",
    "\n",
    "###### Variants\n",
    "- Binary categorization\n",
    "- K-Category categorization\n",
    "- Hierarchal categorization\n",
    "- Joint categorization: multiple related categorization tasks done in a joint manner\n",
    "\n",
    "###### Why Text Categorization\n",
    "- Enrich text representation. Text can now be represented in multiple levels like keywords and categoried.\n",
    "- Semantic categories assigned and can facilitate aggregation of text content (positive or negative opinions)\n",
    "- Infer properties of entities associated with text data.\n",
    "- As long as an entity can be associated with text data, we can use text data to help categorize associated entities\n",
    "\n",
    "#### 10.8 - Categorization Methods\n",
    "###### Works well when:\n",
    "- Categories are well defined\n",
    "- Categories are easily distinguished based on surface features in text\n",
    "- Sufficient domain knowledge is available to suggest many effective rules\n",
    "\n",
    "###### Problems\n",
    "- Does not scale well\n",
    "- Cannot handle uncertainty in rules\n",
    "- Rules ay be inconsistent\n",
    "\n",
    "###### Automatic Categorization\n",
    "Use humans to:\n",
    "- Annotate data sets with category labels\n",
    "- Provide a set of features to represent each text object that can potentially provide a clue about the category\n",
    "\n",
    "Use machine learning to learn soft rules for categorization from the training data:\n",
    "- Figure out which features are most useful\n",
    "- Optimally combine features to minimize the errors of categorization on training data\n",
    "- The trained classifier can then be applied to new text object to predict the most likely category a human export would assign to it\n",
    "\n",
    "###### Discriminative vs. Generative Classifiers\n",
    "Generative classifiers attempts to model the probability of features and label to compute the label using Baye's Rule. Objective function is likelihood. What data looks like each category.\n",
    "\n",
    "Discriminative classifiers attempt to model the labels and features directly; what features seperate categories. Function measures errors of categorization on training data. This includes Logistic Regression, SVMs, K-Nearest Neighbors, etc.\n",
    "\n",
    "#### 10.9 - Categorization  Generative Probabilistic Model\n",
    "Text clustering is to understand the categories, and text categorization places the documents into predefined categories.\n",
    "$$cluster(d) = argmax \\prod_{w \\in V}\\ p(w | \\theta_i)^{c(w, d)}\\ p(\\theta_i)$$\n",
    "$$p(\\theta_i | d) = \\frac{p(d | \\theta_i)\\ p(\\theta_i)}{\\sum_{j = 1}^k\\ p(d | \\theta_j)\\ p(\\theta_j)}$$\n",
    "\n",
    "###### Text Categorization with Naive Bayes Classifier\n",
    "$$category(d) = argmax_i\\ log_p(\\theta_i) + \\sum_{w \\in V}\\ c(w, d)\\ log\\ p(w | \\theta_i )$$\n",
    "\n",
    "How to assure $\\theta_i$ represents category $i$? Use the training data which are documents with assigned categories assigned by human experts. To estimate $p(w | \\theta_i)$ and $p(\\theta_i)$ use the training data. Use the training data to get the prior $\\theta_i$ and the likelihood $p(w | \\theta_i)$. \n",
    "\n",
    "$$p(\\theta_i) = \\frac{N_i}{\\sum_{j = 1}^k N_j} \\propto \\mathbf{|T_i|}$$\n",
    "\n",
    "will estimate which category is most popular. $N_i$ is the number of documents in each category.\n",
    "\n",
    "$$p(w | \\theta_i) = \\frac{\\sum_{j = 1}^{N_i}\\ c(w, d_{ij})}{\\sum_{w' \\in V}\\ \\sum_{j = 1}^{N_i}\\ c(w', d_{ij})}\\ \\propto c(w, T_i)$$\n",
    "\n",
    "determines which word is most frequent.\n",
    "\n",
    "Why smooth in Naive Bayes? This addresses data sparseness, incorporates prior knowledge, and achieves discriminative weighting (IDF Weighting). We do this by adding pseudocounts to words:\n",
    "$$p(\\theta_i) = \\frac{N_i + \\delta}{\\sum_{j = 1}^k\\ N_j + k \\delta}$$\n",
    "\n",
    "where $\\delta \\ge 0$.\n",
    "\n",
    "$$p(w | \\theta_i) = \\frac{\\sum_{j = 1}^{N_i}\\ c(w, d_{ij}) + \\mu p(w | \\theta_B)}{\\sum_{w' \\in V}\\ \\sum_{j = 1}^{N_i}\\ c(w', d_{ij}) + \\mu}$$\n",
    "$$p(w | \\theta_B) = \\frac{1}{\\mathbf{|v|}}$$\n",
    "\n",
    "As $\\mu$ approaches infinity the model approaches the background language model. $\\theta_B$ is the background model.\n",
    "\n",
    "###### Anatomy of Naive Bayes Classifier\n",
    "Assume we have two categories:\n",
    "$$score(d) = log(\\frac{p(\\theta_i)}{p(\\theta_2)}) + \\sum_{w \\in V}\\ c(w, d)\\ log(\\frac{p(w | \\theta_i)}{p(w | \\theta_2)})$$\n",
    "\n",
    "The larger the score the more likely the document is in category one $\\theta_1$. Left of + is the log of the prior and therefore the category bias and can be represented as $\\beta_0$. Right of + is the sum of all of the words: count of word times the weight of each word. This can be represented as $\\beta_i$. Essentially comapring the probability of the word given two distributions. This can be generalized as:\n",
    "$$score(d) = \\beta_0 + \\sum_{i = 1}^M\\ f_i \\beta_i$$\n",
    "\n",
    "where $f_i$ is the count of the word. This is close to logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

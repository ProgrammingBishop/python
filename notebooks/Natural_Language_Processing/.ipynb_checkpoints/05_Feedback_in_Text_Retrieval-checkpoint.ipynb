{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 05 - Feedback in Text Retrieval\n",
    "\n",
    "##### 5.1 - Feedback in Text Retrieval\n",
    "Relevance Feedback is when users make explicit relevance judgments on the initial results. The document collection is updated from the Feedback module.\n",
    "\n",
    "Implicit Feedback is when the user-clicked docs are assumed to be relevant; skipped ones non-relevant. Observe how user interacts with the results. Used in Google and Bing. \n",
    "\n",
    "Pseudo/Blind/Automatic Feedback is when the top-k initial results are assumed to be relevant. This assumption is used to improve the query. This allows additional related words to be learned, and then forwarded to be used in the query. This also does not involve the user.\n",
    "\n",
    "##### 5.2 - Feedback in Vector Space Model - Rocchio\n",
    "Modify the query vector by adding new weighted terms and also adjusting weights of old terms.\n",
    "\n",
    "Query vector is a nucleus within a radius of related documents in vector space. These related docs are either relevant or irrelevant, so to increase the number of relevant documents in relation to the query, move the query in vector space to the centroid of relevant documenats.  \n",
    "$$q_m = \\alpha q + \\frac{\\beta}{\\mathbf{|D_r|}} \\sum_{d_j \\in D_r} d_j - \\frac{\\gamma}{\\mathbf{|D_n|}} \\sum_{d_j \\in D_n} d_j$$\n",
    "\n",
    "$\\alpha$, $\\beta$, and $\\gamma$ are parameters that control the movement. $q$ is the original query. $\\alpha$ controls the original query weight. $\\beta$ control the inference of positive centroid. $\\gamma$ controls the inactive weight of the inactive centroid.\n",
    "\n",
    "$D_r$ are the related doucments, $D_n$ are the non-related documents.\n",
    "\n",
    "$$ \\frac{\\beta}{\\mathbf{|D_r|}} \\sum_{d_j \\in D_r} d_j$$\n",
    "\n",
    "Is the centroid of the relevant documents.\n",
    "\n",
    "$$\\frac{\\gamma}{\\mathbf{|D_n|}} \\sum_{d_j \\in D_n} d_j$$\n",
    "\n",
    "Is the centroid of the non-relevant documents.\n",
    "\n",
    "We want to truncate the vector so we only have a small number of words that have the heighest weights in centroid vector.\n",
    "\n",
    "To avoid overfitting, keep relatively high weight on the original query weights. This is sowe can use relevance feedback and pseudo feedback. For pseudo feedback the $\\beta$ should be set to a smaller value becuase the assumption of relevance and therefore less reliable than relevance feedback (which uses a larger value for $\\beta$)\n",
    "\n",
    "##### 5.3 - Feedback in Language Model\n",
    "Query likelihood cannot support relevance feedback. Kullback-Leibler (KL) divergence retrieval.\n",
    "\n",
    "KL-Divergence (Cross Entropy)\n",
    "$$f(q, d) = \\sum_{w \\in d, p(w | \\theta_Q) > 0} [p(w | \\hat \\theta_Q) log \\frac{P_{seen}(w | d)}{\\alpha_d p(w | C)}] + log \\alpha_d$$\n",
    "\n",
    "Query LM\n",
    "$$p(w | \\hat \\theta_Q) = \\frac{c(w | Q)}{\\mathbf{|Q|}}$$\n",
    "\n",
    "By plugging the QLM into the KL-Divergence, we get the Query Likelihood equation from last week. Since the denominator is a constant we can drop that value.\n",
    "\n",
    "###### Feedback Model Interpretation\n",
    "Solve the Document $D$ and Query $Q$ in $D(\\theta_Q || \\theta_D)$ to get results. These results give the feedback documents $\\theta_F$, which plugs into the equation:\n",
    "$$\\theta_Q' = (1 - \\alpha) \\theta_Q + \\alpha \\theta_F$$\n",
    "\n",
    "- If $\\alpha = 0$ we get no feedback\n",
    "- if $\\alpha = 1$ then we get full feedback\n",
    "\n",
    "###### Generative Mixture Model to Get $\\theta_f$\n",
    "The background langauge model assists the topic word model in identifying which words are stopwords. These distributions - background and topic word - are controlled by a source that decides which to use.\n",
    "- $\\lambda$ is background model $P(w | C)$\n",
    "\t- Generates the common words\n",
    "\t- Helps reduce the probability of common words in the topic word model\n",
    "\t- Will be used if $\\lambda$ is very large\n",
    "- $1 - \\lambda$ is the topic word model $P(w | \\theta)$\n",
    "\t- Will use if $\\lambda$ is very small\n",
    "\t- Words given a high probability are words that are rare in the background model, but common in the overall distribution constructed by the topic word model\n",
    "\n",
    "$\\lambda$ is the noise feedback in documents. The above process is part of the Mixture Model because there are two distributions mixed together, and we do not know when each distribution will be used.\n",
    "$$log_p (F | \\theta) = \\sum_i \\sum_w c(w;d_i) log[(1 - \\lambda) p(w | \\theta) + \\lambda p(w | C)]$$\n",
    "\n",
    "##### 5.4 - Web Search & Web Crawlers\n",
    "Web search challenges involve:\n",
    "- Scalability\n",
    "\t- Solved by parallel indexing and searching with MapReduce\n",
    "- Low quality information and spams\n",
    "- Dynamics of the web: nw pages and page updates constantly \n",
    "\t- Both above solved by spam detection and robust ranking\n",
    "\n",
    "###### Basic Search Engine Technologies - Crawling\n",
    "Web would be crawled by a crawler to generate cached pages. These cached pages are then indexed like an inverted index in which the user can retrieve the relative information. The user interacts with the inverted index through a browser using queries.\n",
    "- Crawler: Building a \"Toy\" Crawler\n",
    "\t- Start with a set of seed pages in a priority queue\n",
    "\t- Fetch pages form the web\n",
    "\t- Parse fetched pages for hyperlinks and add them to the queue\n",
    "\t- Follow hyperlinks in the queue\n",
    "- A real Crawler is tougher because there are various obstacles to consider\n",
    "\t- Server Failure and Traps\n",
    "\t- Server Load Balancing and Robot Exclusion (Crawler Courtesy)\n",
    "\t- Handling of various file types\n",
    "\t- URL scripts like cgi script, internal references, etc.\n",
    "\t- Recognize redundant pages due to identical or duplicate pages\n",
    "\t- DIscover hidden URLs that are truncated in longer URLs\n",
    "- Strategies\n",
    "\t- Breadth-First to balance server load\n",
    "\t- Parallel Crawling\n",
    "\t- Incremental Crawling\n",
    "\t\t- Need to minimize resource overhead\n",
    "\t\t- Learn from past experience \n",
    "\t\t- Target at:\n",
    "\t\t\t- Frequently updated pages: Daily or monthly depending on how frequently the page updates\n",
    "\t\t\t- Frequently accessed pages: More important that a page is fresh than a page users do not visit\n",
    "\t- Variation\n",
    "\t\t- Target a subset of pages\n",
    "\t\t- Typically given in a query\n",
    "\n",
    "##### 5.5 - Web Indexing \n",
    "###### Google FIle System\n",
    "Uses a centralized management system (GFS Master) to manage locations of file namespace and locations (using a lookup table), which are stored in chunks that are replicated to ensure reliability. Once this information has been obtained, Master communicates with the actual servers to locate where the files actually exist.\n",
    "\n",
    "###### Map Reduce\n",
    "Helps parallel processing with fault tolerance and automatic load balancing. Keys are the document IDs, and the values are the strings representing to document. The strings are then broken down into tokens (mapping). Reduce counts how many occurrences there are for each unique token. Key are the words, and value are the counts of the word frequency.\n",
    "\n",
    "##### 5.6 - Link Analysis\n",
    " Hub page are pages with many outgoing links and authority pages are pages with many incoming links.\n",
    "\n",
    "Anchor text is what a query would match for a page. It is additional text to define a document. Links indicate the utility of a document.\n",
    "- Hub page has outgoing links\n",
    "- Authority page as incoming links\n",
    "\n",
    "We are checking for the probability that a page is a hub or an authority page. Indirect Citation is when a page is cited by another highly cited source. This would boost the authority of a page. Smoothing of citations assumes every page to have non-zero pseudo citation count.\n",
    "\n",
    "###### Page Rank Algorithm (Capture a Pages Authority)\n",
    " Random surfing model: At any page\n",
    "- With prob $\\alpha$, randomly jumping to another page\n",
    "- With prob $(1 - \\alpha)$, randomly picking a link to follow\n",
    "\n",
    "The internet can be represented as a directed graph with nodes $d$. This directed graph can be further represented as a matrix $M$ where each row's columns $ij$ are equal to 1, and each column in the row represents if a node directs to the other node. The number in the column represents the probability of a link going to the new page. The probability of going from $d_i$ to $d_j$ is:\n",
    "\n",
    "$p(d_i)$: PageRank score of $d_i$ = average probability of visiting page $d_i$\n",
    "$$\\sum_{j = 1}^N M_{ij} = 1$$\n",
    "\n",
    "The Equilibrium Equation:\n",
    "$$P_{t + 1}(d_j) = (1 - \\alpha) \\sum_{i = 1}^N M_{ij} p_t(d_i) + \\sum_{i = 1}^N \\frac{1}{N} p_t(d_i)$$\n",
    "- $P_{t + 1}(d_j)$ is probability of visiting page $d_j$ at time $t + 1$\n",
    "- $(1 - \\alpha) \\sum_{i = 1}^N M_{ij} p_t(d_i) + \\sum_{i = 1}^N \\frac{1}{N} p_t(d_i)$ is probabilioty a surfer was at page $d_i$ at time $t$\n",
    "\t- $(1 - \\alpha) \\sum_{i = 1}^N M_{ij} p_t(d_i)$ is reaching $dj$ by following a link\n",
    "\t- $\\sum_{i = 1}^N \\frac{1}{N} p_t(d_i)$ is reaching $dj$ via random jumping. This is a smoothing mechanism to assure non-zero entries.\n",
    "\n",
    "$M$ is the transition matrix. The row of $M$ represents the current page and all of the other pages the current page can link to. The sum of the row is 1. The column represents the page itself. So the diagonal would be 0, for example, if none of the pages linked to themselves.\n",
    "\n",
    "$M_{ij} p_t(d_i)$ is the transition probability from $d_i$ to $d_j$.\n",
    "\n",
    "After dropping the time index we get:\n",
    "$$p(d_j) = \\sum_{i = 1}^N [\\frac{1}{N} \\alpha + (1 - \\alpha)M_{ij}] p(d_i)$$\n",
    "$$\\bar p = (\\alpha I + (1 - \\alpha) M)^t \\bar p$$ \n",
    "\n",
    "and $I_{ij} = \\frac{1}{N}$. Initial value $p(d) = \\frac{1}{N}$ and we iterate until convergence. Iteratively update the $p$ vector of $d$ nodes via matrix multiplication.\n",
    "\n",
    "Computation is efficient because $M$ can be sparse, normalization does not affect ranking. One issue is the zero-outlink problem where the $p(d_i)$s do not sum to 1. One option is to have $\\alpha = 1$ for a page with no outlink.\n",
    "\n",
    "###### HITS (Hypertext-Induced Topic Search) Algorithm - Capture Authorities and Hubs\n",
    "The intuition is that pages that are widely cited are good authorities and pages that cite many other pages are good hubs. The idea of HITS is to say that good authorities are cited by good hubs and good hubs point to good authorities. Essentially iterative reinforcement.\n",
    "\n",
    "Process:\n",
    "- Build an adjacency matrix with initial values $a(d_i) = h(d_i) = 1$\n",
    "- Iteratively: ($h$ = Hub Score and $a$ = Authority score)\n",
    "\t- $h(d_i) = \\sum_{d_j \\in OUT(d_i)} a(d_j)$\n",
    "\t- $a(d_i) = \\sum_{d_j \\in IN (d_i)} h(d_j)$\n",
    "- Normalize: $\\sum a(d_i)^2 = \\sum h(d_i)^2 = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

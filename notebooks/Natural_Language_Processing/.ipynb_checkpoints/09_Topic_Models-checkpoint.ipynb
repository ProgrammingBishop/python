{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 09 - Topic Models\n",
    "\n",
    "#### 9.1 - \tMixture of Unigram Models\n",
    "###### Two-Word Distribution\n",
    "Given document $D$:\n",
    "- $P( w | \\theta_d )$ represents the uncommon words\n",
    "- $P( w | \\theta_B )$ represents the background, which are the common words\n",
    "- $P( \\theta_d ) + ( \\theta_B ) = 1$\n",
    "\n",
    "First step is to decide which of the two distributions to use. Then the probability of observing a word - given multiple distributions - is the sum of the word's probability for all of the distributions.\n",
    "$$P( word ) = p( \\theta_d ) p( word | \\theta_d ) + p( \\theta_B ) p( word | \\theta_B )$$\n",
    "\n",
    "which ends up being the sum of the the probability of choosing a model multipled by the probability of observing the word from that model. A mixture model aims to congregate the multiple models together into one model. It is possible to think of this model as a generative model:\n",
    "$$p( w ) = p( \\theta_d )p( w | \\theta_d ) + p( \\theta_B ) p( w | \\theta_B )$$\n",
    "\n",
    "And if we are finding the probability of a phrase we multiply all of the words in the phrase together along with the probability of the model distribution.\n",
    "\n",
    "where the sum represents the different ways a word can be generated. Within each sum the multiplication represents the product of the probability for selecting a component and the probability for selecting a word from that model. The estimate model will discover the multiple topic models and the coverage of each topic model. This sum of probabilities sums to 1. The sum of the probabilites within each model must sum to 1 and the sum of the choice of models must sum to 1.\n",
    "\n",
    "#### 9.2 - Mixture Model Estimation Part I\n",
    "Words of the document are drawn from a mixture of topics where the mixing weight depends on different documents.\n",
    "\n",
    "Behavior of the Mixture Model:\n",
    "- Start off with $P( \\theta_d )$ and $P( \\theta_B )$ having probability of 0.5\n",
    "- Within the background model also assume we know the probability of the words - assume 0.9 and 0.1; stop words will have a higher probability than the uncommon target words\n",
    "\n",
    "\n",
    "- Find the probability of $\\theta_d$ by using the formula: \n",
    "\t- $P( word_1 ) = 0.5 * p( word_1 | \\theta_d ) + 0.5 * 0.1$\n",
    "\t- $P( word_2 ) = 0.5 * p( word_2 | \\theta_d ) + 0.5 * 0.9$\n",
    "\n",
    "\n",
    "- Then: $p( d | \\Lambda ) = p( word_1 | \\Lambda ) p( word_2 | \\Lambda )$ which is the product of the above equations.\n",
    "\t- Remember that $p( word_1 | \\theta_d ) + p( word_2 | \\theta_d ) = 1$\n",
    "\t- This becomes an algebra problem\n",
    "\t- Consider the law that if $x + y = constant$ then $xy$ reaches maximum when $x = y$\n",
    "\t- Then $0.5 * p( word_1 | \\theta_d ) + 0.5 * 0.1 = 0.5 * p( word_2 | \\theta_d ) + 0.5 * 0.9$\n",
    "\t- And $P( word_1 ) = 0.9$ and $P( word_2 ) = 0.1$\n",
    "\n",
    "###### Behaviors\n",
    "- If $p( word_1 | \\theta_B ) > p( word_2 | \\theta_B )$, then $p( word_1 | \\theta_d ) < p( word_2 | \\theta_d )$, which essentially translates to larger probabilities in the background will result in smaller probabilities in the topic model.\n",
    "- Higher frequency words get higher $p( w | \\theta_d )$\n",
    "\n",
    "\n",
    "\n",
    "#### 9.3 - Mixture Model Estimation Part II\n",
    "Consider data frequency: if common words appear multiple times in the document. This is the exponent of $c(w, d)$ that appears in some formulas.  Based on behavior two above, the more frequent that a term appears the higher the probability that term will have. Terms that appear less will have a smaller probability if they appear less. But based on the above formulas, high frequency terms are encouraged by the unknown distribution of $\\theta_d$ to assign a somehwhat higher probability to the high refrequency term.\n",
    "\n",
    "Naturally every component model attempts to assign high probabilities to the highly frequent terms (to collaboratively maximize likelihood). Also, different component models attempt to bet high probabilities on different words to avoid competition or waste probability. Lastly, havimg a background model helps prevent overfitting.\n",
    "\n",
    "Ultimately, the probability of a term in regard to frequency is regularized by the coefficient for choosing a background model versus a topice model. The larger the coefficient is on the background, the less important it is to increase the frequency of a term. So fixing a component to a background word distribution:\n",
    "- Helps get rid of background words in other component\n",
    "- Is an example of imposing a prior on the model parameters (prior = one model must be equal tot he background language model)\n",
    "\n",
    "###### General Behavior of Mixture Model\n",
    "- Every component model attemps to assign high probabilities to highly frequent words in the data\n",
    "- Different component models tend to bet high probabilities on different words\n",
    "- The probability of choosing each component regulates the collaboration/competition between the component models.\n",
    "\n",
    "###### Fix one component to background word distribution\n",
    "- Helps get rid of background words in other component\n",
    "- Example of imposing prior on the model parameter\n",
    "\n",
    "#### 9.4 - Expectation Maximization (EM Algorithm) Part I\n",
    "Start by inferring which distribution a word can be from; distributions represent the various topics covered in a collection. This is why we use a gaussian mixture model because each distribution is a uniqu gaussian distribution specific to each topic. This can be handled with a Bayesian Distribution where the prior is the guess regarding which distribution a word is from before observing a word. Consider $z$ being the probability for a word being from the topic model or the background model: $\\{\\, 0, 1 \\}\\,$\n",
    "$$p( z = 0 | w = word_1 ) = \\frac{ p( \\theta_d ) p( word_1  | \\theta_d ) }{ p( \\theta_d ) p( word_1  | \\theta_d) + p( \\theta_B ) p( word_1  | \\theta_B ) }$$\n",
    "\n",
    "which is essentially the Bayesion Normalization. This helps get an initial value for the words in a probabilistic manner rather than randm guessing.\n",
    "\n",
    "#### 9.5 - Expectation Maximization (EM Algorithm) Part II\n",
    "###### The EM Algorithm\n",
    "- Initialize the $p( w | \\theta_d )$ with random values - best to use a uniform intialization (that is all words have the same probability), then iteratively improve these random values with an E-step and a M-step. Stop at a threshold for when the likelihood does not change a certain amount.\n",
    "- Hidden variable: $z \\in \\{\\, 0, 1 \\}\\,$: all words have this hidden value attached to them.\n",
    "- E-Step: \n",
    "$$p^{(n)}( z = 0 | w ) = \\frac{ p( \\theta_d ) p^{ ( n ) }( w | \\theta_d ) }{  p( \\theta_d ) p^{ ( n ) }( w | \\theta_d ) +  p( \\theta_B ) p^{ ( n ) }( w | \\theta_B ) }$$\n",
    "\n",
    "E-Step augments the data with additional information like the z-value.\n",
    "\n",
    "- M-Step:\n",
    "$$p^{ ( n + 1 ) }( w | \\theta_d ) = \\frac{ c( w, d ) p^{ ( n ) }( z = 0 | w ) }{ \\sum_{ w' \\in V } c( w', d ) p^{ ( n ) }( z = 0 | w' ) }$$\n",
    "\n",
    "where $w'$ represents the word's probability updated from the E-Step. Notice that the M-Step utilizes the $p^{ ( n ) }$ from the E-Step. M-Step takes this info to split data and then collect the right data to reestimate the parameters. THis is repeated until the change does not surpass a preset threshold. Ultimately figures out how likely a word is from the topic model versus the background model.\n",
    "\n",
    "#### 9.6 - Expectation Maximization (EM Algorithm) Part III\n",
    "Convergence to Local Maximum can be plotted:\n",
    "- x-axis is $\\theta$\n",
    "- y-axis is likelihood $p( d | \\theta )$\n",
    "\n",
    "Goal is to obtain the maximum on the likelihood curve. The starting point will begin somehwere randomly on the x-axis, and with each iteration of the EM steps the random guess will slowly converge toward this maxima, whether it be moving in a positive direction or a negative direction. \n",
    "\n",
    "To do so, the E-Step will fit a lower bound around our current guess, and this is because it is easier to optimize a lower bound. the M-Step will then maximize this lowal bound. It is important to start the EM algorithm from different points because there is a chance - with only one point - in which the algorithm starts at or around a local maxima and will never achieve the global maxima. By having multiple starting locations there is a higher likelihood that the algorithm can achieve the global maxima.\n",
    "\n",
    "#### 9.7 - Probabilistic Latent Semantic Analysis (PLSA) Part I\n",
    "PLSA is a mixture model with $k$ unigram language models where $k$ represents the topics. By adding a pre-determined background language model we can discover discriminative topics. The most challenging part of applying maximal likelihood to PLSA is that the objective function needs to sum over all topics for each word.\n",
    "\n",
    "Take a blog article and break it down into different topics $\\theta_1$; each of which have their own distribution of words. The goal of topic analysis is to decode the words behind the topics. Figure out which words are from which distribution and what the topics actually are. \n",
    "\n",
    "Input is $C$, $k$, $V$ and the output are the $\\theta_k$s and the $\\pi_{ ik }$s; $\\theta$ are the various word distributions and $\\pi$ is the topic covered in each document. Choosing a topic is multiway switch as there are $k$ topics. \n",
    "- The probability of choosing a topic is $1 - \\lambda_B$ against the probability of choosing a background $\\lambda_B$. \n",
    "- After choosing whether the word is a background word or not, the next step is to choose which of the $k$ topics the word belongs to. This is $p( \\theta_1 ) = \\pi_{ d, 1 }$ where $\\sum_{ i = 1 }^k \\pi_{ d, i } = 1$. This switch decides which distribution to use.\n",
    "- Third step is to get the likelihood that a word belongs to a distribution.\n",
    "\t- Probability word is generated from the background model is $\\lambda_B\\ p( w | \\theta_B )$; the probability is having to actually obtain the word from the background.\n",
    "\t- Probability of word from a topic is $( 1 - \\lambda_B )\\ p( \\theta_k )\\ p( w | \\theta_k )$; the probability is having to actually obtain the word from the topic.\n",
    "\t- Probability of observing the word is the sum of all probabilities from each topic switch and the background component\n",
    "\n",
    "###### Probabilistic Latent Semantic Analysis (PLSA)\n",
    "$$p_d( w ) = \\lambda_B\\ p( w | \\theta_B ) + ( 1 - \\lambda_B ) \\sum_{ j = 1 }^k \\pi_{ d, j } p( w | \\theta_j )$$\n",
    "\n",
    "- Where $\\lambda_B$ is the percentage of background words (known)\n",
    "- $p( w | \\theta_B )$ is the background language model (known)\n",
    "- $\\pi$ represents the coverage of topic $\\theta_j$ in document $d$\n",
    "- $p( w | \\theta_j )$ is the probability of the word being in topic $\\theta_j$\n",
    "- Unknown parameters: $\\Lambda = ( { \\pi_{ d, j } }, { \\theta_j } )$ where $j = 1, ..., k$, wich is coverage and word distributions respectively\n",
    "\n",
    "When using PLSA to mine topics from a text collection, the number of parameters of the PLSA model does not stay the same as we keep adding new documents into the text collection.\n",
    "\n",
    "$$log\\ p( d ) = \\sum_{ w \\in V }\\ c( w, d )\\ log[ \\lambda_B\\ p( w | \\theta_B ) + ( 1 - \\lambda_B ) \\sum_{ j = 1 }^k\\ \\pi_{ d, j }\\ p( w | \\theta_j ) ]$$\n",
    "\n",
    "$$log\\ p( C | \\Lambda ) = \\sum_{ d \\in C } \\sum_{ w \\in V }\\ c( w, d )\\ log[ \\lambda_B\\ p( w | \\theta_B ) + ( 1 - \\lambda_B ) \\sum_{ j = 1 }^k\\ \\pi_{ d, j }\\ p( w | \\theta_j ) ]$$\n",
    "\n",
    "Constraints:\n",
    "- Sum of word probabilities must sum to 1\n",
    "- A document must cover a topic in the $k$ topics so the probability of covering some topic must sum to 1\n",
    "\n",
    "#### 9.8 - Probabilistic Latent Semantic Analysis (PLSA) Part II\n",
    "###### Updated E-Step (Because Hidden Variable Takes More Than 2 Values)\n",
    "Hidden Variable: $z_{ d, w } \\in \\{\\, B, 1, 2, ..., k \\}\\,$. Notice that $z$ covers more than 2 topics in PLSA, therefore there are more hidden variables.\n",
    "\n",
    "- Probability that a word in the document is generated from a certain topic in the topic model\n",
    "$$p( z_{ d, w = j } ) = \\frac{ \\pi_{ d, j }^{ ( n ) }\\ p^{ ( n ) }\\ ( w | \\theta_j ) }{ \\sum_{ j' = 1 }^k\\  \\pi_{ d, j' }^{ ( n ) }\\ p^{ ( n ) }\\ ( w | \\theta_{ j' } ) }$$\n",
    "\n",
    "- Probability that a word in the document is generated from the background model\n",
    "$$p( z_{ d, w } = B ) = \\frac{ \\lambda_B\\ p( w | \\theta_B ) }{ \\lambda_B\\ p( w | \\theta_B ) + ( 1 - \\lambda_B ) \\sum_{ j = 1 }^k\\ \\pi_{ d, j }^{ ( n ) }\\ p^{ ( n ) }( w | \\theta_j ) }$$\n",
    "\n",
    "Using $d$ to index because the probability that a word belongs to a topic depends on the document, which is through the $\\pi$s - They affect the prediction Each document can have a potentially different $\\pi$. Bayes Rule is used in both cases.\n",
    "\n",
    "###### Updated M-Step\n",
    "Hidden Variable: $z_{ d, w } \\in \\{\\, B, 1, 2, ..., k \\}\\,$\n",
    "\n",
    "Re-estimate the probability of document covering topic\n",
    "$$\\pi_{ d, j }^{ ( n + 1 ) } = \\frac{ \\sum_{ w \\in V }\\ c( w, d )\\ ( 1 - p( z_{ d, w } = B ) )\\ p( z_{ d, w } = j ) }{ \\sum_{ j' }\\ \\sum_{ w \\in V }\\ c( w, d )\\ ( 1 - p( z_{ d, w } = B ) )\\ p( z_{ d, w } = j' ) }$$ \n",
    "\n",
    "Re-estimate the probability of a word for a topic\n",
    "$$p^{ ( n + 1 ) }( w | \\theta_j ) = \\frac{ \\sum_{ d \\in C }\\ c( w, d )\\ ( 1 - p( z_{ d, w } = B ) )\\ p( z_{ d, w } = j ) }{ \\sum_{ w' \\in V }\\ \\sum_{ d \\in C }\\ c( w', d )\\ ( 1 - p( z_{ d, w' } = B ) )\\ p( z_{ d, w' } = j ) }$$ \n",
    "\n",
    "###### Steps\n",
    "- Initialize all unknown parameters randomly\n",
    "- Then, repeat until likelihood converges\n",
    "\t- E-Step\n",
    "\t\t- $p( Z_{ d, w } = j ) \\propto \\pi_{ d, j }^{ ( n ) }\\ p^{ ( n ) }( w | \\theta_j )$ where the sum is equal to 1. This is the prediction of selecting a topic multiplied by the probability of observing a word from that distribution.\n",
    "\t\t- $p( Z_{ d, w } = B ) \\propto \\lambda_B\\ p( w | \\theta_B )$\n",
    "\t\t- The above steps will augment the data by predicting the hidden random variable values $z_{ d, w }$, which is the probability a word from document is from a one of the topics or background. \n",
    "\t- M-Step\n",
    "\t\t- $\\pi_{ d, j }^{ ( n + 1 ) } \\propto \\sum_{ w \\in V }\\ c( w, d )\\ ( 1 - p( z_{ d, w } = B ) )\\ p( z_{ d, w } = j)$\n",
    "\t\t- $p^{ ( n + 1 ) }( w | \\theta_j ) \\propto \\sum_{ d \\in C }\\ c( w, d )\\ ( 1 - p( z_{ d, w } = B ) )\\ p( z_{ d, w } = j )$\n",
    "\t\t- We can normalize based on:\n",
    "\t\t\t- All of the topics to achieve to get a re-estimate of the coverage $\\pi$\n",
    "\t\t\t- Or renormalize based on the words to get a word distribution \n",
    "\t\t\t- Normalization is vital because it establishes a distribution\n",
    "\n",
    "Maximum Likelihood estimates discover topical knowledge from text data\n",
    "- $k$ word distributions where $k$ are the topics\n",
    "- Proportion of each topic in each document\n",
    "\n",
    "The output of the PLSA model can be used for:\n",
    "- clustering of terms and documents (each topic is a cluster)\n",
    "- Further associate topics with different contexts (time, locations, authors, sources)\n",
    "\n",
    "If we have a large collection of documents to train PLSA with, the best process is to train PLSA on a small subset collection of documents and use the model to initialize, and for other documents randomly initialize the documents' topic weights\n",
    "\n",
    "#### 9.9 - Latent Dirichlet Allocation (LDA) Part I\n",
    "\n",
    "###### PLSA with Prior Knowledge\n",
    "PLSA blindly tackles data with maximum likelihood without any prior data.\n",
    "\n",
    "Users may have expectations on what topics analyze, what topics are/ are not covered in a document (a document can only be generated using topics corresponding to the tags assigned to the document). You can incorporate this knowledge as priors.\n",
    "\n",
    "###### Maximum a Posteriori (MAP) Estimate\n",
    "This aims to maximize the postrior probability. We may use $p( \\Lambda )$ to encode all kinds of preferences and constraints. The MAP estimate (with conjugate prior) can be computed using a similar EM algorithm to ML estimate with smoothing to reflect prior preferences. $p( \\Lambda )$ can encode preferences such as:\n",
    "- $p( \\Lambda ) > 0$ iff topic is precisely background\n",
    "- $p( \\Lambda ) > 0$ iff for a particular document we force it to topic with a set probability threshold\n",
    "- $p( \\Lambda )$ favors $\\Lambda$ with the topics that assign high probabilities to some particular words, which is favoring certain kinds of distributions.\n",
    "\n",
    "###### EM Algorithm with Conjugate Prior on $p( w | \\theta_i )$\n",
    "Prior says that there should be one distribution that will assign high probability to certain words given a collection.\n",
    "\n",
    "$$p( z_{ d, w = j } ) = \\frac{ \\pi_{ d, j }^{ ( n ) }\\ p^{ ( n ) }\\ ( w | \\theta_j ) }{ \\sum_{ j' = 1 }^k\\  \\pi_{ d, j' }^{ ( n ) }\\ p^{ ( n ) }\\ ( w | \\theta_{ j' } ) }$$\n",
    "\n",
    "$$p( z_{ d, w } = B ) = \\frac{ \\lambda_B\\ p( w | \\theta_B ) }{ \\lambda_B\\ p( w | \\theta_B ) + ( 1 - \\lambda_B ) \\sum_{ j = 1 }^k\\ \\pi_{ d, j }^{ ( n ) }\\ p^{ ( n ) }( w | \\theta_j ) }$$\n",
    "\n",
    "$$\\pi_{ d, j }^{ ( n + 1 ) } = \\frac{ \\sum_{ w \\in V }\\ c( w, d )\\ ( 1 - p( z_{ d, w } = B ) )\\ p( z_{ d, w } = j ) }{ \\sum_{ j' }\\ \\sum_{ w \\in V }\\ c( w, d )\\ ( 1 - p( z_{ d, w } = B ) )\\ p( z_{ d, w } = j' ) }$$ \n",
    "\n",
    "Updated re-estimation step\n",
    "$$p^{ ( n + 1 ) }( w | \\theta_j ) = \\frac{ \\sum_{ d \\in C }\\ c( w, d )\\ ( 1 - p( z_{ d, w } = B ) )\\ p( z_{ d, w } = j ) + \\mu p( w | \\theta'_j ) }{ \\sum_{ w' \\in V }\\ \\sum_{ d \\in C }\\ c( w', d )\\ ( 1 - p( z_{ d, w' } = B ) )\\ p( z_{ d, w' } = j ) + \\mu }$$ \n",
    "\n",
    "where $\\mu p( w | \\theta'_j )$ represent the pseudocounts of $w$ from prior $\\theta'$ and $\\mu$ represents the sum of all pseudo counts. $\\mu$ at zero would remove the prior. If $\\mu$ was infinity then the data would be disregarded. We are essentially adding additional counts to reflect our prior. So the $\\mu$ component in the updated equation above will give high counts to the words of interest because pseudocounts are based on the probability of words in the prior. All words not in prior will have a pseudocount of 0. Other than the prior parameters involving $\\mu$, we can set any other parameter to a constant (even 0) as needed. \n",
    "\n",
    "#### 9.10 - Latent Dirichlet Allocation (LDA) Part II\n",
    "Deficiency of PLSA\n",
    "- Not a generative model because it cannot compute probability of a new document. This is because the $\\pi$ are tied to the document we have in the training data, so we cannot compute the $\\pi$ of future data.\n",
    "- Many parameters leads to highly complex models, which would contain many local maxima and therefore prone to overfitting\n",
    "- This is fine for text mining because we are only interested in fitting the training documents.\n",
    "\n",
    "LDA\n",
    "- Makes PLSA a generative model by imposing a Dirichlet prior on the model parameters. LDA is essentially a Bayesian version of PLSA and the parameters are regularized.\n",
    "- Can achieve the same goal as PLSA for text mining purposes like topic coverage and word distributions, which can be inferred through Bayesian Inference.\n",
    "\n",
    "Both word distributions and topic choices are free in PLSA. Consider:\n",
    "- $\\bar \\theta_i = ( p( w_1 | \\theta_i ), ..., p( w_M | \\theta_i ) )$, which is a vector representing the word distributions (one vector for each topic)\n",
    "- $\\bar \\pi_d = ( \\pi_{ d, 1 }, ..., \\pi_{ d, k } )$, which is also a vector and represents a more convenient representation of LDA (one vector for each document). The $\\pi$s tell us the probabilities of which topic to use. The topic distributions are drawn from the Dirichlet distribution $( \\bar \\beta )$, and then from this we will sample a word. \n",
    "\n",
    "The difference between LDA and PLSA is that LDA does not allow the parameters to freely change, instead we will force them to be chosen from another distribution; two Dirichlet distributions:\n",
    "- $p( \\bar \\theta_i ) =$ Dirichlet $( \\bar \\beta )$ where $\\bar \\beta = ( \\beta_1, ..., \\beta_M ), \\beta_i > 0$, which controls the words in the vocabulary\n",
    "- $p( \\bar \\pi_d ) =$ Dirichlet $( \\bar \\alpha )$ where $\\bar \\alpha = ( \\alpha_1, ..., \\alpha_k ), \\alpha_i > 0$, which tells us which vector of $\\pi$ is more likely and with different values of $\\alpha$ we can characterize distributions in different ways. An example would be allowing certain values of $\\pi$ to be more likely than others. Favor a uniform distribution or favor generating a distribution of skewed topics.\n",
    "\n",
    "$( \\bar \\alpha )$ to tell us about the distribution of topics; $( \\bar \\beta )$ tells us about the distribution of words. \n",
    "\n",
    "###### Likelihood Functions for LDA\n",
    "$$p_d( w | \\{\\, \\theta_j \\}\\, , \\{\\, \\pi_{ d, j } \\}\\, ) = \\sum_{ j = 1 }^k\\ \\pi_{ d, j }\\ p( w | \\theta_j )$$\n",
    "\n",
    "Is the probability of generating a word from multiple distributions and is a sum of all possibilites of generating a word with the product within the sum being the probability of choosing a topic multiplied by the probability of observing a word from that topic.\n",
    "\n",
    "$$log\\ p( d | \\bar \\alpha, \\{\\, \\theta_j \\}\\, ) = \\int \\sum_{ w \\in V } c( w, d )\\ log\\ [  \\sum_{ j = 1 }^k\\ \\pi_{ d, j }\\ p( w | \\theta_j ) ]\\ p( \\bar \\pi_d | \\bar \\alpha )\\ d \\bar \\pi_d$$\n",
    "\n",
    "Notice the component of the PLSA in the LDA formula. This is a sum integrals and serves to show that the $\\pi$s are not fixed, but rather drawn from a Dirichlet distribution.\n",
    "\n",
    "$$log\\ p( C | \\bar \\alpha, \\bar \\beta ) = \\int \\sum_{ d \\in C }\\ log\\ p( d | \\bar \\alpha, \\{\\, \\theta_j \\}\\, ) \\prod_{ j = 1 }^k\\ p( \\theta_j | \\bar \\beta )\\ d \\theta_1, ..., d \\theta_k$$\n",
    "\n",
    "Parameters can be estimated using Maximum Likelihood estimator. LDA has fewer parameters (just $\\alpha$ and $\\beta$). Notice that means $\\theta_j$ and $\\pi_{ d, j }$ are no longer parameters and therefore must be computed using posterior inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 03 - Evaluating Text Retrieval Systems\n",
    "\n",
    "#### 3.1 - Evaluating Text Retrieval Systems\n",
    "Important to measure:\n",
    "- Effectiveness/Accuracy\n",
    "- Efficieny\n",
    "- Usability\n",
    "\n",
    "###### Cranfield Evaluatio Methodology\n",
    "Idea is to build a reusable test collection and define measures:\n",
    "- Sample collection of documents (simulate real documents)\n",
    "- Sample a set of queries/topics\n",
    "- Relevance judgments are made by users who formulate the queries, which are binary 0/1 based on relevant or not relevant.\n",
    "- Measures to quantify how well a system's result matches the ideal ranked list\n",
    "\n",
    "This would be reused many times to compare different systems. Run each system on the queries and documents to receive eahc system's results.\n",
    "\n",
    "#### 3.2 - Basic Measures of Text Retrieval Systems\n",
    "Precision measures the total number of relevant documents returned proportionate to all documents returned. Answer the question of whether the retrieved documents are relevant.\n",
    "$$\\frac{ a }{ a + c }$$\n",
    "\n",
    "Recall measure the total number of relevant documents returned proportionate to all documents. High recall tends to be related to low precision. Answers the question of whether all relevant documents have been retrieved.\n",
    "$$\\frac{ a }{ a + b }$$\n",
    "\n",
    "The set that $a, b, c$ belong to can be related to a portion of the whole of a ranked list.\n",
    "\n",
    "###### F-Measure\n",
    "- $F_b = \\frac{ ( B^2 + 1 ) * pr }{ B^2p + r }$\n",
    "- $F_1 = \\frac{ 2pr }{ p + r }$\n",
    "\n",
    "where $p$ is precision, $r$ is recall, and $B$ is a parameter often set to 1. Do not use $0.5p + 0.5r$ because sums tend to be dominated by large numbers, which means the sum will be high because low $p$ or low $r$ will result in a high number of the other.\n",
    "\n",
    "#### 3.3 - Evaluated Ranked List\n",
    "###### Precision-Recall Curve\n",
    "- Y is the precision\n",
    "- X is the Recall\n",
    "- Points from each ranking measure will form the curve\n",
    "- Best to compare multiple curves, with the ideal system being a horizontal line. \n",
    " \n",
    "Always pick the higher curve, and if the curve crosses then the choice will depend on the user's task. Does the user care about high recall and low precision, like news in the day. Or vice versa like researching a new topic.\n",
    "\n",
    "It is best to have one number, which could be the area under the curve. This area is the average precision, which is the sum of the precision proportion for each point over the number of relevant documents in all. By having the $n$ points we are including the recall into the calculation.\n",
    "\n",
    "The order of the relevant documents returned affects the overall calculation. If the fifth item is relevant then the score would be $\\frac{3 }{ 5 }$. If is was instead the fourth then it wuld be $\\frac{ 3 }{ 4 }$. When summing together the larger fractions will leads to a larger total. \n",
    "\n",
    "###### Mean Average Precision (MAP)\n",
    "Average Precision is the average at every cutoff where the new relevant document is retrieved. Normalizer is the total number of documents in the collection. This score is sensitive to the rank of each relevant document.\n",
    "\n",
    "Mean Average Precision is the arithmatic mean of the average precision over a set of queries. $\\frac{ a + b }{ 2 }$. The sum is dominated by large values and is used for popular queries.\n",
    "\n",
    "GMAP is the geometric mean of the averge precision over a set of queries. $\\sqrt{ a * b }$. This is affected by low values (precision is low) and is preferred for difficult queries.\n",
    "\n",
    "###### Mean Reciprocal Rank\n",
    "Use this when there is only one relevant document in the collection.\n",
    "- Average Precision = Reciprocal Rank = $\\frac{ 1 }{ r }$ where $r$ is the rank position of a single relevant document. If the rank is 3 then the formula is $\\frac{ 1 }{ 3 }$. This measures the amount of effort to locate a document.\n",
    "- MAP = Mean Reciprocal Rank (MRP), which is the reciprocal rank over a set of topics.\n",
    "\n",
    "#### 3.4 - Mulit-level Judgments\n",
    "Measure the search engine system using relevance judgments. Set relevance on three levels with 1 being lowest and 3 being the highest; levels are called gain, which represents how much help a user gets from utility of the document. MAP, average precision, and recall will not work because they are based on binary judgments.\n",
    "\n",
    "Cumulative Gain measures the total gain, which is the sum of the gain for the currently ranked document.\n",
    "\n",
    "Discounted Cumulative Gain:\n",
    "- Divide gain by factor of rank\n",
    "- First in rank gets no division\n",
    "- Second in rank gets divided by $log( 2 )$\n",
    "- Third gets divided by $log( 3 )$\n",
    "- Total discounted cumulative gain: $\\sum_{g = 1}^r \\frac{ g }{ log( r ) }$\n",
    "\n",
    "Normalized Discounted Cumulative Gain (DCG)\n",
    "- $\\frac{ DCG }{ Ideal\\ DCG }$\n",
    "- This maps DCG of each document between0 and 1\n",
    "\n",
    "#### 3.5 - Practical Issues\n",
    "Challenges in creating a test collection\n",
    "- Queries and documents must be representative and high enough $n$\n",
    "- Judgments must be complete for all documents and queries, but involve minimal human effort\n",
    "- Measures must capture the perceived utility by users\n",
    "\n",
    "Statistical significance test is to test the variance between experiments\n",
    "- Large variance is not great because resutls are more variable\n",
    "- Start with the assumption that there is no difference between the experiments (mean = 0)\n",
    "\n",
    "Pooling means to avoid judging all documents\n",
    "- Choose diverse set of ranking mehtods (TR Systems)\n",
    "- Have each return the top-k documents \n",
    "- Combine all the top-k documents to form a pool for human assessors to judge\n",
    "- Okay for systems that contribute to the pool, but problematic for evaluating new systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

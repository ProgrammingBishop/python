{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 04 - Language Models\n",
    "\n",
    "#### 4.1 - Probabilistic Retrieval Methods\n",
    "$$f( d, q ) = p( R = 1 | d, q )$$ \n",
    "\n",
    "where $R \\in [ 0, 1 ]$ and equals\n",
    "$$\\frac{ c( q, d, R = 1 ) }{ c( q, d ) }$$\n",
    "\n",
    "and essentially is measuring if the document is relevant to the query. If the user likes the document $d$, then how likely will the user enter the query $q$ to retrieve $d$\n",
    "\n",
    "Three Columns:\n",
    "- Query ID\n",
    "- Document ID\n",
    "- Relavent measure of 0/1\n",
    "\n",
    "Use a Query Likelihood Retrieval Model for unseen documents and queries.\n",
    "\n",
    "#### 4.2 - Query Likelihood Retrieval Model\n",
    "$$f( q, d ) = p( R = 1 | d, q )$$\n",
    "\n",
    "approximates to $p( q | d, R = 1 )$, which assumes the user likes the document and measures how likely it is the user enters query $q$. The user forms a query based on an imaginary relevant document that aims to answer which document is most likely tthe imaginary relevant document.\n",
    "\n",
    "#### 4.3 - Statistical Language Model\n",
    "Language Model \n",
    "- A probability distribution over word sequences\n",
    "- Context dependent\n",
    "- Also a mechanism for generating text (generative model)\n",
    "\n",
    "Why Language Model is useful:\n",
    "- Quantify uncertainties in natural language\n",
    "- Likelihood of seeing a word given the word exists\n",
    "- Likelhiood of the word seen represent a specific topic\n",
    "- Likelihood a user uses a query given what the user is interested in\n",
    "\n",
    "#### 4.4 - Simplistic Language Model: Unigram LM\n",
    "Generate text by generating each word independently\n",
    "$$p( w_1, w_2, ... ) = p( w_1 ) * p( w_2 ) * ...$$\n",
    "\n",
    "Models as $N$ parameterswhrre $N$ is the model size. \n",
    "\n",
    "Maximum likelihood model:\n",
    "$$p( w | \\theta ) = p( w | d ) = \\frac{ c( w, d ) }{ \\mathbf{ | d | } }$$\n",
    "\n",
    "- Background Language Model represents the frequency of words in the language in general. $p( w | B )$ where $B$ is general background in a language, which includes common stopwords\n",
    "- Collection Language Model is $p( w | C )$ where $C$ includes words related to the topic and will normalize the frequencies.\n",
    "- Document Language Model is $p( w | d )$ where $d$ represents the document. THe higher the probabilities for words most related to the topic.\n",
    "- Normalized Topic Language Model will give stopwords very small weights:\n",
    "\n",
    "$$\\frac{ p( w | topic ) }{ p( w | B ) }$$\n",
    "\n",
    "#### 4.5 - Query Likelihodd Retrieval Function\n",
    "Unigram Query Likelihood assumes each query word is generated independently:\n",
    "$$p( q | d ) = \\frac{ c( q_{w1} | d ) }{ \\mathbf{ | d | } } * \\frac{ c( q_{w2} | d ) }{ \\mathbf{ | d | } }$$\n",
    "\n",
    "Improve the model by sampling words from a document model:\n",
    "- How likely would we observe a query from a document model? Do this by computing the model for $p( w | d_n )$ and get the likelihood of a query by getting the probability using each query word.\n",
    "$$f( q, d ) = log( p( q | d ) ) = \\sum c( w, q ) * log( p( w | d ) )*$$\n",
    "\n",
    "where the $log()$ guards against underflow for multiplying with small values. $w \\in V$ is word in entire vocabulary.\n",
    "\n",
    "#### 4.6 - Statistical Language Model\n",
    "Maximum likelihood by normalizing the frequencies\n",
    "$$p_{ ML( w | d ) } = \\frac{ c( w | d ) }{ \\mathbf{ | d | } }$$\n",
    "\n",
    "will produce a step function that will contain the words with 0 probability, but this is sub-optimal. To optimize:\n",
    "$$p( w | d ) > 0$$\n",
    "\n",
    "even if $c( w | d ) = 0$\n",
    "\n",
    "###### Smoothing Options\n",
    "-$p( w | d ) = p_{seen}( w | d )$ if $w \\in d$\n",
    "- $\\alpha_d * p( w | C )$ otherwise where $C$ is collection of documents\n",
    "\n",
    "$$log( p( q | d ) ) = \\sum_{ w \\in V, c( w | d ) > 0 } c( w, q ) * log( p_{seen}( w | d ) ) + \\sum_{ w \\in V, c( w | d ) > 0 } c( w, q ) * log( \\alpha * p( w | C ) ) $$\n",
    "\n",
    "where the first component is the sum of query words matched in the document and the second component is the sum of query words not matched in the document.\n",
    "\n",
    "#### 4.7 - Smoothing Methods\n",
    "Linear Interpolation\n",
    "- Fixed coefficient linera interpolation\n",
    "- Words are assigned a probability based on their relevance\n",
    "- Words that are absent get a very small probability that is $> 0$\n",
    "$$p( w | d ) = ( 1 - \\lambda ) * \\frac{ c( w, d ) }{ \\mathbf{ | d | } } + \\lambda p( w | C )$$\n",
    "\n",
    "where $\\lambda \\in [ 0, 1 ]$.\n",
    "\n",
    "Dirichlet Prior \n",
    "- Adaptive interpolation\n",
    "$$p( w | d ) = \\frac{ \\mathbf{ | d | } }{ \\mathbf{ | d | } + \\mu } * \\frac{ c( w | d ) }{ \\mathbf{ | d | } } + \\frac{ \\mu }{ \\mathbf{ | d | } + \\mu } * p( w | C )$$\n",
    "\n",
    "where $\\mu \\in [ 0, \\infty ]$ and if $\\mu$ is a constant, longer documents get a smaller coefficient (less smoothing). This formula can also be written as:\n",
    "$$c( w | d ) + \\frac{ \\mu p( w | C ) }{ \\mathbf{ | d | } + \\mu }$$\n",
    "\n",
    "where every word gets a pseudocount and $\\mu$ denominator is the total number of pseudocounts added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 01  - Introduction\n",
    "\n",
    "#### 1.1 - Natural Language Content Analysis\n",
    "Lexical Analysis is parts-of-speech tagging\n",
    "- Noun\n",
    "- Verb\n",
    "- Preposition\n",
    "\n",
    "Syntactical Analysis is a parser to find connections between symbols\n",
    "- Prepositional Phrases\n",
    "- Verb Phrase\n",
    "- Noun Phrase\n",
    "\n",
    "Semantic Analysis are symbols to denote different words\n",
    "- Japan (J1)\n",
    "- Woman (W1)\n",
    "- Going (J1, W1) where Going is a connector \n",
    "\n",
    "Inferencing is where a computer makes assumptions based on the context of the text. \n",
    "\n",
    "Pragmatic Analysis understands why someone says a sentence: why would someone say \"the woman is going to Japan\"\n",
    "\n",
    "Ambiguity and common-sense is an issue for computers because one word can have multiple meanings and cannot assume like people do in regard to others understanding the language.\n",
    "\n",
    "Types:\n",
    "- Word-Level\n",
    "- Syntactical: different interpretations and ambiguity\n",
    "- Anaphora Resolution: what do certain words refer to when multiple options exist\n",
    "- Presupposition: assumptions made from a statement\n",
    "\n",
    "Entity Extraction is about extracting relationships between different word symbols\n",
    "\n",
    "Word Disambiguation is about extracting the context of how a word symbol is used. One word can identify the context of another word. For example \"Java\" and \"Applet\" can mean \"Java\" is the programming language.\n",
    "\n",
    "Sentiment Analysis analyzes a sentence's mood. Is the sentence positive or negative.\n",
    "\n",
    "General Natural Language Processing is shallow while deep NLP doesn't scale. Shallow NLP needs to be general, robust, and efficient.\n",
    "\n",
    "Bag-of-Words:\n",
    "- Text data is a collection of words without ay order\n",
    "- Duplicates are maintained\n",
    "- Loses a lot of context because there is no order\n",
    "- Good for search engines\n",
    "\n",
    "#### 1.2 - Text Access\n",
    "Push (Recommender System) is where the system takes initiative. The stable system has a good knowledge about the user's need (not temporary). Preferences are collected over time.\n",
    "\n",
    "Pull (Search Enines) is where the user takes initiative. Ad hoc information is needed (temporary information). Querying is where the user enters a keyword and the system returns relevant documents. Works best when the user knows the right keywords to use. Browsing is an alternative to querying (although not as developed). User navigates to relevant info via a path enabled by structures on document. Works well when the user wants to explore info and doesn't know the right keywords to use.\n",
    "\n",
    "#### 1.3 - Text Retrieval Problem\n",
    "Text Retrieval is a search technology that utilizes a collection of documents for a user's query to obtain relevant documents.\n",
    "\n",
    "Differences from database retrieval\n",
    "\n",
    "TR:\n",
    "- Unstructured and ambiguous\n",
    "- Does not really specify on what is returned \n",
    "- Returns relevant documents\n",
    "- Empirically defined problem. Can't mathematically prove a method is better than another. Involve users to evaluate empirically if one algorithm is better than another.\n",
    "\n",
    "DB:\n",
    "- Structured based on a schema and well-defined\n",
    "- Complete specification on what is returned\n",
    "- Matched records\n",
    "\n",
    "Formal Formulation:\n",
    "- Vocabulary: $V = \\{\\, w_1, ..., w_3 \\}\\,$\n",
    "- Query: $q = q_1, ..., q_m$\n",
    "- Document: $d_i = d_{i1}, ..., d_{im_j}$, which is a word in the vocabulary\n",
    "- Collection: $C = \\{\\, d_1, ..., d_m \\}\\,$\n",
    "- Set of relevant documents: $R(q)$ is from $C$\n",
    "\n",
    "The task is to compute $R'(q)$ as an approximation of $R(q)$.\n",
    "\n",
    "###### Strategies:\n",
    "Document Selection:\n",
    "$$R'(q) = \\{\\, d \\in C | f( d, q ) = 1 \\}\\,$$\n",
    "\n",
    "where $f( d, q )$ is between $\\{\\, 0, 1 \\}\\,$ as an indicator function $I$. Essentially a binary classifier regarding whether or not the document is relevant.\n",
    "\n",
    "Document Ranking (Preferred)\n",
    "$$R'(q) = \\{\\, d \\in C | f( d, q ) > \\theta \\}\\,$$\n",
    "\n",
    "where $f( d, q ) \\in R$ is a relevance measure function $\\theta$ is a cutoff determined by the user. Determine the number of documents returned.\n",
    "\n",
    "Over-constrained query means no relevant documents are returned. Under-constrained query over delivers. Relevance is a measure of degree.\n",
    "\n",
    "###### Probability Ranking Principle\n",
    "Returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions:\n",
    "- Utility of the document to the user is independent of the utility of any other document. This is not true because identical documents with identical content. This leads to collective relevance: one doc is not useful, but multiple may be helpful.\n",
    "- A user would browse the results sequentially. Users may not go sequentially in results like search results.\n",
    "\n",
    "#### 1.4 - Retrieval Methods\n",
    "Probabilistic Model\n",
    "$$f( d, q ) = p( R = 1 | d, q )$$\n",
    "\n",
    "where $R$ is in $\\{\\, 0, 1 \\}\\,$\n",
    "\n",
    "Common ideas in retrieval models:\n",
    "- How often does a word occur in the query: Term Frequency $TF = c( word, d )$\n",
    "- How long is the document: Document Length $DF = \\mathbf{ | d | }$\n",
    "- How often do we see a word in entire collection: $P( word | Collection )$\n",
    "\n",
    "#### 1.5 - Vector Space Model\n",
    "Every dimension corresponds to a term/word. So $N$ terms in an n-dimensional space. A query vector $q = \\{\\, x_1, ..., x_n \\}\\,$ where $x_i$ is in $R$ and is a term weight (measure of term importance). A document vector $d = ( y_1, ..., y_n )$ where $y_j \\in R$ is a document term weight. Overall goal is to find similarity between query vector and document vectors.\n",
    "\n",
    "#### 1.6 - Vector Retrieval Model\n",
    "Similarity Instantiation is a dot product (Retrieval Function)\n",
    "$$sim( q, d ) = q . d = x_1y_1 + ... + x_ny_n$$\n",
    "\n",
    "Dot product will end up adding may 0s and 1s. $1 x 1$ means a match for a word and $1 x 0\\ or\\ 0 x 1$ will mean the word only exists in the query or doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 02 - Text Retrieval Systems\n",
    "\n",
    "#### 2.1 - Vector Space Model\n",
    "Term Frequency (TF):\n",
    "- $x_i$ is count of word $W_i$ in query\n",
    "- $y_i$ is count of word $W_i$ in document\n",
    "\n",
    "$$\\sum (q, d) = q . d = x_1 * y_1 + ... + x_n * y_n$$\n",
    "\n",
    "Document Frequency (DF) are the count of the documents that contain a particular term.\n",
    "\n",
    "Inverse Document Frequency (IDF):\n",
    "$$y_i = c( W_i, d ) * IDF(W_i)$$\n",
    "\n",
    "Common words have a low IDF and less common words have a higher IDF. IDF weighting penalizes popular terms.\n",
    "$$IDF(w) = log( \\frac{ M + 1 }{ k } )$$\n",
    "\n",
    "where $M$ is the total documents in the collection and $k$ is the total number of docs containing $W_i$. So the more documents that contain the word, the smaller the IDF. The less documents that contain the word, the higher the IDF.\n",
    "\n",
    "#### 2.2 Term Frequency Transformation\n",
    "$$f( q, d ) = \\sum ( x_i * y_i ) = \\sum_{ w \\in q \\cap d } c( w, q ) * c( w, d ) * log( \\frac{ M + 1 }{ k } )$$\n",
    "\n",
    "That is, the sum of all words that match in the query and the document. $M$ is still the total number of documents in the collection and $df(w)$ is the document frequency.\n",
    "\n",
    "###### Term Frequency Transformation\n",
    "- y-axis is the $TF( w, d )$: term frequency weight\n",
    "- x-axis is $c( w, d )$\n",
    "- There exists a 0/1 bit vector switch\n",
    "- $y = x$ is frequency weight count  and is a linear function\n",
    "- $y = log( 1 + x )$ is a curved function\n",
    "\n",
    "The curve enables a plateau after so many occurrences (diminishing returns), and you can double $log()$ to increase the occurence of the plateau\n",
    "\n",
    "###### BM25 Transformation\n",
    "$$y = \\frac{ ( k + 1 ) * x }{ x + k }$$\n",
    "\n",
    "where $k + 1$ is an upper bound in the y-axis. $k = 0$ is 0/1 bit transformation. Larger $k$ replicates a linear function and the upper bound controls the influence of a term.\n",
    "$$\\sum_{ w \\in q \\in d }\\ c( w, d ) * \\frac{ ( k + 1 ) * c( w, d ) }{ c( w, d ) * k } * log( \\frac{ m + 1 }{ df(  w ) } )$$\n",
    "\n",
    "#### 2.3 Document Length Normalization\n",
    "The Vector Space Model and the document must be represented in the same format, which is a vector. Relevance $( q, d )$ = Similarity $( q, d )$. We are going to penalize the length of a document with a normalizer because a longer document will have a better chance to match a query because it will have a higher probability of containing a word that is being queried.\n",
    "\n",
    "Document can be long because:\n",
    "- It uses more words, and therefore should receive more penalization.\n",
    "- It has more contents, and therefore should receive less penalization. Contents could be chapters or sections within the document.\n",
    "\n",
    "###### Pivoted Length Normalization:\n",
    "$$f( q, d ) = \\sum_{ w \\in q \\in d }\\ c(w, q) * \\frac{ log( 1 + log( 1 + c( w, d ) ) ) }{ 1 - b + ( b * \\frac{ \\mathbf{| d |} }{ AVDL } ) } * log( \\frac{ m + 1 }{ df( w ) } )$$\n",
    "\n",
    "This formula penalizes long documents where the denominator in the middle term is the document length normalizer.\n",
    "\n",
    "###### BM25/Okapi\n",
    "$$\\sum_{ w \\in q \\in d }\\ c( w, d ) * \\frac{ ( k + 1 ) * c( w, d ) }{ c( w, d ) + k( 1 - b + ( b * \\frac{ \\mathbf{ | d | } }{ AVDL } ) ) } * log( \\frac{ m + 1 }{ df(  w ) } )$$\n",
    "\n",
    "where $b \\in [ 0, 1 ]$ and $k_1, k_3 \\in [ 0, \\infty ]$. This forumal will do a transformation with an upper bound.\n",
    "\n",
    "###### Further Improvement of the Vector Space Model\n",
    "Improved instantiation of dimension:\n",
    "- stemming words, stop word removal, phrases, latent semantic indexing (word clusters), character n-grams\n",
    "- Bag-of-Words with phrases\n",
    "- Language-specific domain-specific tokenization is important in the normalization of words\n",
    "\n",
    "Improved instantiation of similarity function\n",
    "- cosine of angle between two vectors\n",
    "- Euclidean distance\n",
    "- but the dot product still seems to perform best.\n",
    "\n",
    "###### Further Improvement of BM25\n",
    "Use BM25 for documents with structures $(f = fields)$\n",
    "- Combine the frequency counts of terms in all fields and then apply BM25\n",
    "- Fields could be title fields, anchor tags, or an abstract or body\n",
    "\n",
    "BM25+ \n",
    "- Address the problem of over penalizing of long documents by BM25 by adding a small constant to TF\n",
    "- Shown to be better than regular BM25\n",
    "\n",
    "#### 2.4 - Implementation of Text Retrieval\n",
    "Text Retrieval Architecture:\n",
    "- Documents are processed by tokenizer\n",
    "- Tokens will then be processed by an indexer to create a data structure index\n",
    "- Then a query wil be sent through the tokenizer\n",
    "- The query representation is sent off to the scorer, which uses an index to anser the user's query and scores documents and then ranks them\n",
    "- Results are sent back to the user\n",
    "- And the user previews the results and provides feedback to return back to the scorer.\n",
    "\n",
    "Overall Parts:\n",
    "- Indexer is offline\n",
    "- Scorer is online\n",
    "- Feedback is both online and offline\n",
    "\n",
    "Tokenization\n",
    "- Normalize Lexical Units: words with similar meanings should be mapped to the same indexing term.\n",
    "- Stemming: mapping all inflectional forms of words to the same root.\n",
    "- Some languages pose a chellenge in word segmentation\n",
    "\n",
    "Indexing:\n",
    "- Convert documents into data structures that enable a fast search\n",
    "- An inverted index is dominating this method; essentially a hash table.\n",
    "\n",
    "Dictionary (Lexicon)\n",
    "- One columns contains the words\n",
    "- Second column is the number of documents the word in first column appears in\n",
    "- Third column is the total frequency\n",
    "- A pointer is used to point to the Postings\n",
    "\n",
    "Postings\n",
    "- First column is the document ID. Vector where each word is a segment of the vector. Each segment of the vector represents the document ID in which the term exists.\n",
    "- Frequency is the second column  and lists the number of times the term is found in a spcific document ID's text.\n",
    "- Optional third column are the positions of the word in which it is found in the document. If the document has multiple positions then we have info on the length between the occurrences of the word in the doc.\n",
    "\n",
    "Boolean Query:\n",
    "- Must match $A \\cap B$ or\n",
    "- Must match $A \\cup B$\n",
    "\n",
    "Multi-term Query is similar to a disjunctive Boolean query $( A \\cup B )$. This will aggregate the term weights.\n",
    "\n",
    "###### Empirical Distribution of Words\n",
    "Stable language-independent patterns in how people use natural languages. Few words occur frequently, but most appear rarely. Most frequent words in one corpus may be rare in another.\n",
    "\n",
    "Zipf's Law:\\\n",
    "- Rank x Frequency is an approximate constant\n",
    "$$F( w ) = \\frac{ C }{ r(w)^a }$$\n",
    "\n",
    "where $a$ is approximately 1 and $C$ is approximately 0.1\n",
    "\n",
    "Given a graph, Y is the word frequency, X is the word rank, and the curve is the fit segmented into 3 parts:\n",
    "- High frequency words (stop words) are in the left-most segment. These are not useful and will end up being removed.\n",
    "- Intermediate frequency words are in the central segment\n",
    "- Rare words inwill be in the right-most segment (these words are not useful)\n",
    "\n",
    "###### Data Structures for Inverted Index\n",
    "Dictionary is a modest size that needs random access and is preferred to be in memory: hash tale, b-tress, trie, etc.\n",
    "\n",
    "Postings will be huge and needs to have sequential access that can stay on the disk because the CPU is faster than I/O. COmpression is desireable. \n",
    "\n",
    "#### 2.5 - Inverted Index Construction\n",
    "Sort-based methods:\n",
    "- Collect local tuples (term ID, doc ID, frequency) and parse the documents and record the above values. Before running out of memory and writing to the disk\n",
    "- you want to sort the tuples by term IDs. After, write to disk as a temporary file\n",
    "- Pairwise merge runs will essentially emulate map-reduce.\n",
    "\n",
    "###### Term Frequency Compression\n",
    "Small number tends to occur more frequently than large numbers. Fewer bits are needed for small integers at the cost of more buts for larger integers.\n",
    "\n",
    "###### Document ID Compression\n",
    "D-Gap will stor ethe differences and is feasible because of sequential access.\n",
    "\n",
    "###### Methods\n",
    "- Binary Code is equal-length coding (uniform code)\n",
    "- Unary Code has a number of bits equal to the vaue of the number\n",
    "- Gamma Code is a unary code for $1 + floor[ log( x ) ]$ followed by uniform code for $x * 2^{ floor[ log( x ) ] }$. This only uses the $floor[ log( x ) ]$ bits.\n",
    "- Delta Code is the same as gamma code, but replaces unary prefix with gamma code.\n",
    "\n",
    "###### Uncompress Inverted Index\n",
    "Decoding of Encoded integers\n",
    "- Unary decoding will count the 1s until a 0 is found\n",
    "- Gamma decoding will first decode the unary part with value $k + 1$. Then it will read k more bits and decode them as binary code: value = $r$. Value of the encoded number is $2^k + r$.\n",
    "\n",
    "Decode document IDs encoding using D-Gap\n",
    "- Let encoded ID list be $x_1, ..., x_n$\n",
    "- Decode $x_1$ to obtain the document ID 1\n",
    "- Decode $x_2$ and add recovered value to document ID 1 just obtained \n",
    "- Repeat for $x$ until $x_n$\n",
    "\n",
    "#### 2. 6 - Fast Search\n",
    "Scoring Function:\n",
    "$$f( q, d ) = f_a( h( g( t_1, d, q ), ..., g( t_n, d, q ) ), f_d(d), f_q(q) )$$\n",
    "\n",
    "where $f_a, f_q,\\ and\\ f_d$ are adjustment factors of the document and query to adjust the score further. $g()$ function computes the weights of a query term in $d$ and $h()$ aggregates these weights.\n",
    "\n",
    "###### General Algorithm\n",
    "- $f_d$ and $f_q$ are pre-computed\n",
    "- Maintain the score accumulator for each $d$ to compute $h$\n",
    "- For each query term $t_i$ fetch the inverted list, and for each entry in the inverted liste compute the function $g()$ and update the score accumulator for the document for document $d_i$ to incrementally compute $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

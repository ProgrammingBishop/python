{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 12 - Text Mining\n",
    "\n",
    "#### 12.3 - Prediction\n",
    "Goal is to make predictions of the world based on the data gathered through sensors.\n",
    "\n",
    "###### Data Mining Loop:\n",
    "- Real world data comes in\n",
    "- The data is perceived by sensors, which could be people\n",
    "- This data comes in the form of non-text data and text data\n",
    "- Perform joint mining of data to generate multiple predictors \n",
    "- These predictor variables will be added to a predictive model\n",
    "- Predict values of real world variables\n",
    "- Cycle repeats with the machine determining what data should be collected next   \n",
    "\n",
    "###### People:\n",
    "- Involved in data mining and generation of the features\n",
    "- Involved in the predictive model building and testing\n",
    "- Involved in applying the predictions to make decisions or take action\n",
    "- Involved in controlling the sensors of data collection\n",
    "\n",
    "###### Subtasks of Text-Based Prediction\n",
    "- Mining content of text data\n",
    "- Mining knowledge about the observer\n",
    "\n",
    "###### Joint Analysis of Text and Non-Text Data\n",
    "Non-text data help text-mining\n",
    "- Provides context; provide way to partition data in different ways\n",
    "- Contextual Text Mining: mine text in the context defined by non-text data\n",
    "\n",
    "Text data help non-text data mining\n",
    "- Text data determines patterns discovered from non-text data\n",
    "- Pattern Annotation: Using text data to interpret patterns found in non-text data\n",
    "\n",
    "#### 12.4 - Motivation\n",
    "Text has rich context information\n",
    "- Direct context: time, location, authors, source (meta data)\n",
    "- Indirect context: social network of authors, author's age, etc.\n",
    "- Any related data can be regarded as context\n",
    "\n",
    "Context can be used to:\n",
    "- Partition data for comparative analysis\n",
    "- Provide meaning to discovered topics\n",
    "\n",
    "Once given context the data can be partitioned in many different ways. Context could be time, location, etc. Then we could compare topics between years or between locations. We could also partition that data based on the author's location or by a specific topic.\n",
    "\n",
    "#### 12.5 - Contextual Probabilistic Latent Semantic Analysis (CPLSA)\n",
    "Idea: \n",
    "- Add context variables into generative model (enable discovery contextualized topics)\n",
    "- This inlfuences both coverage and content variation of topics\n",
    "\n",
    "Add Extension of PLSA\n",
    "- Model conditional likelihood of text given context\n",
    "- Assume context-dependent views of a topic; allow discovery variations of same topic in different contexts\n",
    "- Assume context-dependent topic coverage; cover topics differently based on time, location, etc.\n",
    "- EM algorithm still used for parameter estimation\n",
    "- Estimated parameters naturally contain context variables, enabling contextual text mining\n",
    "\n",
    "###### Generation  Process of CPLSA\n",
    "Assume multiple topics in the context of an event in time. Also assume different views for each topic. Each view is a different version of word distribtuion and tied to some context variables like location, time, author, etc. Think of this like a matrix. Theme coverage also varies according to contexts. People of a location, time, etc would be interested in certain topics over the other topics.\n",
    "\n",
    "So first choose coverage, then use coverage to choose topic, then draw word from the topic. Then next time may choose a different a topic to generate a different word. This is essentially allowing the context to dictate the choice of the words. \n",
    "\n",
    "#### 12.6 - Mining Topics with Social Networking\n",
    "The context of text article can form a network\n",
    "- Authors of research articles may form collaboration networks\n",
    "- Authors of social media context may form social networks\n",
    "- Locations associated with text can be connected to form a geographic network\n",
    "\n",
    "Benefit of Joint Analysis of text and its network context\n",
    "- Network imposes constraints on topics in text (authors connected in a network tend to write about similar topics)\n",
    "- Text helps characterize the content associated with each subnetwork\n",
    "\n",
    "###### Network Supervised Topic Modeling\n",
    "$$\\Lambda^{*} = argmax_{\\Lambda}\\ p(TextData | \\Lambda)$$\n",
    "\n",
    "where utilization of maximum likelihood can return the parameters.\n",
    "\n",
    "View as solving an optimization problem via maximum likelihood given a set of parameters. Constraints on model parameters $\\Lambda$ include:\n",
    "- Text at two adjacent nodes of network tends to cover similar topics\n",
    "- Topic distribution are smoothed over adjacent nodes\n",
    "- Add network-induced regularizers to the likelihood objective function\n",
    "\n",
    "$$\\Lambda^{*} = argmax_{\\Lambda}\\ f(p(TextData | \\Lambda), r(\\Lambda | Network))$$\n",
    "\n",
    "Optimize function $f$ combines the likelihood function with a regularizer function $r$ using params $\\Lambda$ and the Network. This is like imposing a prior to the model. Text Data is any generative model for text. Network can be any graph that connects text objects.\n",
    "\n",
    "###### NetPLSA\n",
    "This is a Network-Induced Prior, which essentially translates to neighbors having a similar topic distribution. THis gives more meaningful topics.\n",
    "$$O(C, G) = (1 - \\lambda) * (\\sum_{d} \\sum_{w} c(w, d) log\\ \\sum_{j = 1}^k\\ p(\\theta_j | d) p(w | \\theta_j)) + \\lambda * (-\\frac{1}{2} \\sum_{(w, v) \\in E} w(u, v) \\sum_{j = 1}^k (p(\\theta_j | u) - p(\\theta_j | v))^2)$$\n",
    "\n",
    "where:\n",
    "- $O(C, G)$ is the text collection in the network graph\n",
    "- $(\\sum_{d} \\sum_{w} c(w, d) log\\ \\sum_{j = 1}^k\\ p(\\theta_j | d) p(w | \\theta_j))$ is the PLSA log-likelihood, which we want to maximize for the parameters.\n",
    "- $\\sum_{j = 1}^k (p(\\theta_j | u) - p(\\theta_j | v))^2)$ quantifies the difference in the topic coverage at node $u$ and $v$. This computes the square of their difference and we want to minimize this difference. THis makes it possible to find the parameters to maximize the PLSA likelihood and respect the constrant of difference. The negative sign allows minimization when maximizing the objective function.\n",
    "- $w(u, v)$ is the weight of the edge between $u$ and $v$. This weight indicates the relationship between nodes. High wieght means topic coverage is similar.\n",
    "- $\\lambda$ controls the influence of network constraint. When 0 the equation essentially reverts back to the PLSA log-likelihood. If a large number, then the network model will have a greater effect.\n",
    "\n",
    "###### Text Information Network\n",
    "We can view text data that naturally lives in a rich information network with all other related data. Text data can be associated with:\n",
    "- Nodes of the network\n",
    "- Edges of the network\n",
    "- Paths of the network\n",
    "- Subnetworks\n",
    "\n",
    "#### 12.7 Mining Causal Topics with TIme Series Supervision\n",
    "Input:\n",
    "- Time series\n",
    "- Text data produced in similar time period\n",
    "\n",
    "Output:\n",
    "- Topics whose coverage in text stream has strong correlations with time series (causal topics)\n",
    "\n",
    "###### Iterative Casual Topic Modeling\n",
    "Idea is to have iterative adjustment of topic, discovered by topic models using time series to induce a prior. Take text stream as input, apply topic modeling to generate a collection of topics, then use external time series to understand which topics are more causal related to external time series.\n",
    "\n",
    "Look into the words of the top-ranked list/topic, that is the topic related to the time series. Then figure out which words are correlated with the time series. Seperate the words based on positive and negative correlations with the time series. These correlation categories become the prior to be sent back into the topic model. This process repeats in a cycle. \n",
    "\n",
    "This process is a heuristic way to optimize causality and coherence. Measue causailty will use time series $X_t$ and time series of external information $Y_t$. Does $X_t$ cause $Y_t$? This is causality. Correlation measures the relationship between the two. \n",
    "\n",
    "Imagine a plot where the topic coherence is the x-axis and the topic-time series causaility is the y-axis. the ideal causal topics would be an equally high coherence and causality. The extreme value of coherence is a pure topic model. THe extreme value for topic-time series is a causal model without much context.\n",
    "\n",
    "###### Granger Causality Test \n",
    "Uses the history information of $Y$ to predict itself. Then add $X$ to see if it improves $Y$. If so, then we can say $X$ has a casual relation with $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

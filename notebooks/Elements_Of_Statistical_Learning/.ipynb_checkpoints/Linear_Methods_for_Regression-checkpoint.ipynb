{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Methods for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Models and Least Squares\n",
    "\n",
    "RSS can be represented as:\n",
    "$$RSS(\\beta) = (y - X\\beta)^T(y - X\\beta)$$\n",
    "$$\\hat \\beta = (X^TX)^{-1}X^Ty$$\n",
    "$$\\hat y = X\\hat \\beta$$\n",
    "\n",
    "The hat matrix is given by:\n",
    "$$H = X(X^TX)^{-1}X^T$$\n",
    "\n",
    "and is called the hat matrix because it is what makes the response for $y$ a prediction, and thus giving the predicted $\\hat y$. This computes the orthogonal projection between the subspace of the inputs and the real value $y$. This assumes $X$ is linearly independent and of full rank.\n",
    "\n",
    "If there are variables that are considered redundant, then the projection of $y$ onto the subspace formed by the inputs $x$ can be represented in more than one way (more than one solution).\n",
    "\n",
    "The predictors create a hyperplane, and the response creates an orthogonal projection onto this hyperplane in which the predictors span. This projection is $\\hat y$ and represents the vector of least squares predictions. We minimize the $RSS(\\beta)$ so that the residual vector $y = \\hat y$ is orthogonal to the subspace derived from the input vectors $1, ..., p$.\n",
    "\n",
    "The variance can be estimated by:\n",
    "$$\\sigma^2 = \\frac{1}{N - p - 1}\\ \\sum_{i = 1}^N\\ (y_i - \\hat y_i)^2$$\n",
    "\n",
    "To test if a coefficient $\\beta_j = 0$, use a Z-score:\n",
    "$$z_j = \\frac{\\hat \\beta}{\\hat \\sigma \\sqrt(v_j)}$$\n",
    "\n",
    "where $v_j$ is the $j$th diagonal element of $(X^TX)^{-1}$. A large absolute value of $z_j$ will lead to a rejection of the null hypothesis. To test if a categorical variable with $k$ levels can be excluded from a model it will need to be proven that none of the levels are important. The F-statistic can be used to determine this:\n",
    "$$F = \\frac{(RSS_0 - RSS_1) / (p_1 - p_0)}{RSS_1 / (N - p_1 - 1)}$$\n",
    "\n",
    "where $RSS_1$ is the least squares fit for the bigger model and $RSS_0$ is the least squares fit for the smaller, nested model. This means that $p_1 - p_0$ cannot be less than $0$.\n",
    "\n",
    "The F-statistic measures the change in RSS per additional parameter in the bigger model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Selection\n",
    "\n",
    "Reasons why least squares estimates are not satisfactory:\n",
    "- the least squares estimates often have low bias and large variance. Prediction accuracy can be improved by shrinking some coefficients to zero. \n",
    "- With a large number of predictors, it is better to have a smaller subset that exhibit the strongest effects. Sacrifice small details to understand the bigger picture. \n",
    "\n",
    "Forward-stepwise may be computationally more efficient, have lower variance than best subset selection, but perhaps have a higher bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage Methods\n",
    "\n",
    "Subset selection methods can often exhibit high variance, but shrinkage methods do not suffer as much from high variability. \n",
    "\n",
    "###### Ridge Regression\n",
    "$\\lambda$ is a shrinkage parameter; the larger this value the more shrinkage as it brings the coefficients closer to zero. Many correlated variables can cause high variance. Correlation can be seen when one variable is wildly positive and then another is similarly wildly negative and thus cancelling each other out. One solution around this from occurring is by imposing a size constraint:\n",
    "$$\\hat \\beta_{ridge} = argmin_{\\beta}\\ \\sum_{i = 1}^N\\ (y_i - \\beta_0 - \\sum_{j = 1}^p\\ x_{ij}\\beta_j)^2$$\n",
    "\n",
    "subject to:\n",
    "$$\\sum_{j = 1}^p\\ \\beta_j^2 \\le t$$\n",
    "\n",
    "There is a one-to-one correspondance to $\\lambda$ and $t$ found in tha above, simplified equation. \n",
    "\n",
    "It is best practice to normalize the ridge solutions as they are not equivariant under scaling of the inputs. It is also important to NOT penalize the intercept as doing so would make the procedure depend too much on the origin chosen for $Y$. So subtract the mean from the actual non-intercept coefficients. In matrix notation this would be:\n",
    "$$RSS(\\lambda) = (y - X\\beta)^T(y - X\\beta) + \\lambda \\beta^T \\beta$$\n",
    "\n",
    "so the Ridge Regression solutions are seen to be:\n",
    "$$\\hat \\beta = (X^TX + \\lambda I)^{-1}X^Ty$$\n",
    "\n",
    "where $I$ is the $p\\ x\\ p$ identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Singular Value Decomposition\n",
    "The centered input matrix $X$ that is $N\\ x\\ p$ can be defined as:\n",
    "$$X = UDV^T$$\n",
    "\n",
    "where $U$ is an $N x p$ matrix, $V$ a $p\\ x\\ p$ matrix - both orthogonal - with columns of $U$ spanning the column space of $X$ and the columns of $V$ are spanning the row space. $D$ is a $p\\ x\\ p$ diagonal matrix with the diagonal elements increasing from top-left ot bottom-right. Using the SVD, the least squares fitted vector can be represented as:\n",
    "$$UU^Ty$$\n",
    "\n",
    "With the equation written above, the ridge solution becomes:\n",
    "$$X \\hat \\beta^{ridge} = \\sum_{j = 1}^p\\ u_j\\ \\frac{d_j^2}{d_j^2 + \\lambda}\\ u_j^Ty$$\n",
    "\n",
    "where $u_j$ are the columns of $U$. Also note that the division in the equation is less or equal to $1$ and $\\lambda$ is greater than 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression computes the coordinates of $y$ with respect to the orthonormal basis $U$. It then shrinks these corrdinates by the factors $\\frac{d^2_j}{d^2_j + \\lambda}$. This means that greater shrinkage is applied to coordinates of basis vectors with smaller $d^2_j$. \n",
    "\n",
    "Sample covariance matrix is given by:\n",
    "$$S = \\frac{X^TX}{N}$$\n",
    "$$X^TX = VD^2V^T$$\n",
    "\n",
    "where the second equation is the eigen decomposition of $X^TX$. The eigenvectors $v_j$ are the principal components directions of $X$. The first principal component direction $v_1$ has the property that $z_1 = Xv_1$ has the largest variance amongst all normalized linear combinations of the columns of $X$.\n",
    "$$Var(z_1) = Var(Xv_1) = \\frac{d^2_1}{N}$$\n",
    "\n",
    "where $z_1 = u_1d_1$. Subsequent principal components have maximum variance subject to being orthogonal to the earlier ones. The last component has minimum variance and ridge regression shrinks these directions the most. \n",
    "\n",
    "The $df(\\lambda) = p$ when $\\lambda = 0$ (no regularization) and $df(\\lambda) -> 0$ as $\\lambda -> \\infty$. As always there is an additional degree of freedom for the intercept, which is removed apriori. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decomposing a matrix means we want to find a product of matrices that is equal to the initial matrix. In regard to Eigendecomposition, we decompose the initial matrix into a product of its eigenvectors and eigenvalues.\n",
    "\n",
    "The inner product (dot product) between a matrix and a vector is equivelant to transforming the vector to new coordaniates whereas the rules of the transformation are defined in the matrix. \n",
    "\n",
    "###### Eigen Stuffs\n",
    "Eigenvector is a vector in which, after a transformation, the magnitude may change, but the direction does not. Magnitude can include the negative direction; as long as the vector reamins in the same span. So think of eigenvector being a vector where $v$ and $Av$ are parallel. The output vector is just a scaled version of the input vector where $\\lambda$ is the factor by which the input is scaled to produce the output. \n",
    "$$Av = \\lambda v$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 2.]\n",
      "[[ 0.70710678 -0.31622777]\n",
      " [ 0.70710678  0.9486833 ]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[5, 1], [3, 3]])\n",
    "A = np.linalg.eig(A)\n",
    "# This function only works for square matrices\n",
    "# For non-square use np.linalg.svd()\n",
    "\n",
    "# First array of the output are the eigenvalues\n",
    "# Second array of the output are the eigenvectors\n",
    "print( A[0] )\n",
    "print( A[1] )\n",
    "\n",
    "# Eigenvalue A[0, 0] corresponds to eigenvector A[1, 0]\n",
    "# Inner product of original A and the eigenvector will give a scaled version of v\n",
    "# where v = [1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigendecomposition is given by:\n",
    "$$A = V\\ dot\\ diag(\\lambda)\\ dot\\ V^{-1}$$\n",
    "\n",
    "where $V$ is a matrix where the columns are the eigenvectors, $\\lambda$ is a diagonal matrix where the eigenvalues lay on the diagonal.  \n",
    "\n",
    "###### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 1.],\n",
       "       [3., 3.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eigenvector matrix\n",
    "eig_vec = np.array( [ [1,  1],\n",
    "                      [1, -3] ] )\n",
    "\n",
    "# Eigenvalue vector transformed into a diagonal matrix\n",
    "eig_val      = np.array( [6, 2] )\n",
    "eig_val_diag =  np.diag( eig_val )\n",
    "\n",
    "# Inverse of the eigenvector matrix\n",
    "eig_vec_inv = np.linalg.inv( eig_vec )\n",
    "\n",
    "# Calculate decomposed matrix\n",
    "# A = V\\ dot\\ diag(\\lambda)\\ dot\\ V^{-1}\n",
    "eig_vec.dot( eig_val_diag ).dot( eig_vec_inv )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Another Example\n",
    "When symmetric, eigendecomposition can be expressed as:\n",
    "$$A = Q\\ \\Lambda\\ Q^T$$\n",
    "\n",
    "where $Q$ is the matrix with eigenvectors as columns and $\\Lambda$ is $diag(\\lambda)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 2.],\n",
       "       [2., 3.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note a symmetric matrix\n",
    "A = np.array( [ [6, 2],\n",
    "                [2, 3] ] )\n",
    "\n",
    "eig_val, eig_vec = np.linalg.eig( A )\n",
    "eig_val          = np.diag( eig_val )\n",
    "\n",
    "# Note the below result is A\n",
    "eig_vec.dot( eig_val ).dot( eig_vec.T )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = UDV^T$$\n",
    "\n",
    "- $U$ and $V$ are orthogonal matrices (The transpose is equal to the inverse).\n",
    "    - $U$ are the left singular vectors of $A$\n",
    "    - $V$ are the right singular vectors of $A$\n",
    "- $D$ is a diagonal matrix, but not necessarily square.\n",
    "    - Values along the diagonal of $D$ are the singular values of $A$\n",
    "    \n",
    "Dimensions:\n",
    "- $A$ is mxn and shares the shape with $D$\n",
    "- $U$: is mxm\n",
    "- $D$: is mxn and shares the shape with $A$\n",
    "- $V^T$: is nxn\n",
    "\n",
    "$A$ is a matrix that can be seen as a linear transformation that can be decomposed in three subtransformations:\n",
    "- rotation $U$\n",
    "- rescaling $D$\n",
    "- rotation $V$\n",
    "\n",
    "A transformation associated with diagonal matrices imply a rescaling of each coordinate without transforming the coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array( [ [3, 7], \n",
    "                [5, 2] ] )\n",
    "\n",
    "# V is returned already transposed\n",
    "U, D, V = np.linalg.svd ( A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First rotation $U$ is meant to position the eigenvectors about the axes.\n",
    "- Scaling $D$ is meant to squish the inputs about the axes\n",
    "- Second rotation $V^T$ is meant to unrotate the transformed inputs to the original coordinates \n",
    "\n",
    "Singular values are ordered in descending order whereas the first feature explains most of the variance and the last the least. Based on the equation above, the first left singular vector $u_1$ and its norm will be the first singular value $\\sigma_1$, which will also be the feature explaining most of the variance. \n",
    "\n",
    "The steps to find $U, D, V$ can be found by transforming $A$ in a square matrix and by computing the eigenvectors of this square matrix. The square matrix can be obtained by multiplying $A$ by its transpose:\n",
    "- $U$ corresonds to the eigenvectors $AA^T$\n",
    "- $V$ corresonds to the eigenvectors $A^TA$\n",
    "- $D$ corresonds to the eigenvalues $AA^T$ or $A^TA$ (they are the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[7, 2], [3, 4], [5, 3]])\n",
    "U, D, V = np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69366543 -0.59343205 -0.40824829]\n",
      " [-0.4427092   0.79833696 -0.40824829]\n",
      " [-0.56818732  0.10245245  0.81649658]]\n",
      "[[-0.69366543  0.59343205 -0.40824829]\n",
      " [-0.4427092  -0.79833696 -0.40824829]\n",
      " [-0.56818732 -0.10245245  0.81649658]]\n"
     ]
    }
   ],
   "source": [
    "# U = AA^T\n",
    "print( np.linalg.eig( A.dot(A.T) )[1] )\n",
    "print( U )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.25142677  2.62835484]\n",
      "[10.2514268  2.6283548  0.       ]\n",
      "[10.25142677  2.62835484]\n"
     ]
    }
   ],
   "source": [
    "# D = AA^T or A^TA\n",
    "print( np.sqrt( np.linalg.eig( A.T.dot(A) )[0] ) )\n",
    "print( np.round( np.sqrt( np.linalg.eig( A.dot(A.T) )[0] ), 7 ) )\n",
    "print( D )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.88033817 -0.47434662]\n",
      " [ 0.47434662  0.88033817]]\n",
      "[[-0.88033817 -0.47434662]\n",
      " [ 0.47434662 -0.88033817]]\n"
     ]
    }
   ],
   "source": [
    "# V = A^TA\n",
    "print( np.linalg.eig( A.T.dot(A) )[1] )\n",
    "print( V )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

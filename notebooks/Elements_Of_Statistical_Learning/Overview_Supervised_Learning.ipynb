{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models and Least Squares\n",
    "\n",
    "It is common to represent the intercept within the $X$ matrix as a column vector composed of the value $1$ so the linear model can be written as:\n",
    "$$\\hat Y = X^T\\ \\hat \\beta$$\n",
    "\n",
    "We take $X^T$ because $\\beta$ is a column of feature coefficients. $X$ is a matrix of observations set up as rows and feature values as columns. By taking the transpose we can do component-wise multiplication of each observation's feature value and the corresponding coefficient at that feature. Since the constant that is the intercept is included in $X$, we can assume that $X$ is a hyperplane that includes the origin and is a subspace.\n",
    "\n",
    "###### Scenarios\n",
    "- Train data in each class is generated from bivariate Gaussian distributions with uncorrelated components and different means. Linear Regression is suitable for this scenario.\n",
    "- Train data in each class comes from a mixture of 10 low-variance Gaussian distributions with individual means themselves distributed as Gaussian. KNN would be ideal for this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Least Squares to Nearest Neighbors\n",
    "\n",
    "Least Squares has low variance, but high bias. KNN has low bias, but high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statstical Decision Theory\n",
    "\n",
    "KNN and Least Squares both approximate conditional expectations by averages, but differ in terms of model assumptions. \n",
    "- Least Squares assumes the function $f(x)$ is well approximated by a globally linear function.\n",
    "- KNN assumes the function $f(x)$ is well approximated by a locally constant function. But as dimensionalty increases, the convergence decreases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Models, Supervised Learning and Function Approximation\n",
    "\n",
    "KNN can fail in two ways:\n",
    "- if the dimension of the input space is high, the nearest neighbors need not be close to the target point, and can result in large errors.\n",
    "- if the spacial structure is known to exist, this can be used to reduce both the bias and the variance of the estimates.\n",
    "\n",
    "The maximum log-likelihood estimation assumes the most reasonable values of $\\theta$ are those for which the probability of the observed sample is largest. The log-likelihood of the data is:\n",
    "$$L(\\theta) = -\\frac{N}{2}\\ log(2\\pi) - N\\ log(\\sigma) - \\frac{1}{2\\sigma^2}\\ sum_{i = 1}^N\\ (y_i - f_{\\theta}(x_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection and Bias-Variance Tradeoff\n",
    "\n",
    "As model complexity increases, the variance of the model increases and bias of the model decreases. As model complexity decreases, the opposite between bias and variance will occur."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binomial sampling model is:\n",
    "\n",
    "$$p(y | \\theta) = Bin(y | n, \\theta) = {N\\choose y} \\theta^y(1 - \\theta)^{n - y}$$\n",
    "\n",
    "where $N$ is the number of exchangeable trials to choose from and $y$ is the number of successes in $n$ trials. Here $\\theta$ represents the proportion of successes in the population (i.e. the probability of success in each trial).\n",
    "\n",
    "$${N\\choose y} = \\frac{n!}{y!(n - y)!}$$\n",
    "\n",
    "for example:\n",
    "\n",
    "$${4\\choose 2} = \\frac{4!}{2!(4 - 2)!} = 12$$\n",
    "\n",
    "and since the denominator as $(4 - 2)!$ this will cancel out the 2! from the numerator leaving only $4*3$ and the denominator with $2*1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The currently accepted value of the proportion of female births in large European-race populations is $0.485$. For this example we define the parameter $\\theta$ to be the proportion of female births, but an alternative way of reporting this parameter is as a ratio of male to female birth rates, $\\phi = \\frac{(1 - \\theta)}{\\theta}$. Let $y$ be the number of girls in $n$ births. Using the formula above, we assume the $n$ births are conditionally independent given $\\theta$, with the probability of female birth equal to $\\theta$ for all cases.\n",
    "\n",
    "First we need a prior distribution for $\\theta$ before using the binomial distribution. We will assume it is a uniform interval $[0, 1]$. Posterior density for $\\theta$ is:\n",
    "\n",
    "$$p(\\theta | y) \\propto \\theta^y(1 - \\theta)^{n - y}$$\n",
    "\n",
    "With a fixed $n$ and $y$ the ${n \\choose y}$ can be treated as a constant when calculating the posterior distribution of $\\theta$.\n",
    "\n",
    "##### Sub-Example\n",
    "In analyzing the binomial model, Laplace also used the uniform prior distribution. His first serious application was to estimate the proportion of girl births in a population. A total of $241,945$ girls and $251,527$ boys were born in Paris from 1745 to 1770. Letting $\\theta$ be the probability that any birth is female, Laplace showed that:\n",
    "\n",
    "$$Pr( \\theta \\ge 0.5 | y = 241,945, n = 251,527 + 241,945 ) \\approx 1.15 * 10^{-42}$$\n",
    "\n",
    "Assume $\\tilde y$ is a single new trial, the outcome is:\n",
    "\n",
    "$$Pr(\\tilde y = 1 | y) = \\int_0^1 \\theta p(\\theta | y)d \\theta = \\frac{y + 1}{n + 2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior distribution is $p(\\theta)$ and posterior distribution is $p(\\theta|y)$. Given:\n",
    "\n",
    "$$E(\\theta) = E(E(\\theta | y))$$\n",
    "\n",
    "the prior mean of $\\theta$ is the average of all possible posterior means over the distribution of possible data. The variance formula:\n",
    "\n",
    "$$var(u) = E(var(u | v)) + var(E(u | v))$$\n",
    "\n",
    "says that the posterior variance is on average smaller than the prior variance by an amount that depends on the variation in posterior mean over the distribution of possible data.\n",
    "\n",
    "The Beta Distribution:\n",
    "\n",
    "$$\\theta | y \\approx \\beta(y + 1, n - y + 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean, median, mode is used for the location of a distribution while variation is described by standard deviation, interquartile range, and other quantiles. Mean is the posterior expectation of the parameter, and mode may be interpreted as the single most likely value given the data and the model. Much practical inference relies on the use of normal approximations, often improved by applying a symmetrizing transformation to $\\theta$, and here the mean and standard deviation play key roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior Qunatiles and Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to report posterior uncertainty. If interval of symmetry is desired, a central interval of posterior probability - which corresponds to $100(1 - \\alpha)\\%$ interval - to the range of values above and below which lie exactly $100(\\frac{\\alpha}{2})\\%$ of the posterior probability. These are Posterior Intervals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the population interpretation, the prior distribution represents a population of possible values, from which the $\\theta$ of current interest has been drawn. In the more subjective state of knowledge interpretation, we must express our knowledge/uncertainty about $\\theta$ as if its value could be thought of as a random realization from the prior distribution. Typically the prior distribution should include all plausible values of $\\theta$, but the distribution need not be realistically concentrated around true values because the info about $\\theta$ contained in the data will far outweight any reasonable prior probability specification.\n",
    "\n",
    "In a uniform prior distribution for $\\theta$, the prior predictive value for $y$ (given $n$) is uniform, which gives equal probability to the $n + 1$ possible values. This is usually sufficient when nothing about the data is known. There are weaknesses to this assumption, however. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prior distribution of a parameter is your uncertainty about the parameter before the current data are examined. Multiplying the prior distribution and the likelihood function together leads to the posterior distribution of the parameter. You use the posterior distribution to carry out all inferences.\n",
    "\n",
    "Bayesian probability measures the degree of belief you have in a random event. All priors are subjective priors. Objective/Noninformative distributions are more objective because they have minimal impact on the posterior distribution. Noninformative distributions occur when the prior is flat relative to the likelihood function. Noninformative priors are also invariant under transformation (unchanged after transformations are applied).\n",
    "\n",
    "Improper priors are \n",
    "\n",
    "$$\\pi(\\theta) \\propto 1$$\n",
    "\n",
    "for $(-\\infty, \\infty)$. These are used to yield noninformative priors and proper posterior distributions. To determine if a posterior distribution is proper, you need to make sure the normalization constant for all $y$ is finite.\n",
    "\n",
    "A prior is said to be conjugate for a family of distributions if the prior and the posterior distributions are from the same family (posterior and prior have the same distributional form)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binomial with Different Prior Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood is in the form:\n",
    "\n",
    "$$p(y | \\theta) \\propto \\theta^a(1 - \\theta)^{b - 1}$$\n",
    "\n",
    "or in other words:\n",
    "\n",
    "$$p^y(1 -p)^{n - y}$$\n",
    "\n",
    "Thus if the prior is of the same form, with its own values a and b, then the posterior density will also be of this form (shape $(\\alpha)$ and scale $(\\beta)$):\n",
    "\n",
    "$$p(\\theta) \\propto \\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}$$\n",
    "\n",
    "or in other words:\n",
    "\n",
    "$$p^{\\alpha - 1}(1 - p)^{\\beta - 1}$$\n",
    "\n",
    "which is a $\\beta$ distributuon with the parameters $\\alpha$ and $\\beta: \\theta \\sim \\beta(\\alpha, \\beta)$ Comparing $p(\\theta)$ and $p(y | \\theta)$ suggests that this prior density is equivalent to $\\alpha - 1$ prior successes and $\\theta - 1$ prior failures. Parameters of the prior distribution are referred to as hyperparameters. The proir distribution is indexed by two hyperparameters, which means we can specify a fixed distribution by fixing two features of the distribution like the mean and the variance. \n",
    "\n",
    "Posterior density for $\\theta$ is:\n",
    "\n",
    "$$p(\\theta | y) \\propto \\beta(\\theta | \\alpha + y, \\beta + n - y)$$\n",
    "\n",
    "or in other words:\n",
    "\n",
    "$$p^{\\alpha + y - 1}(1 - p)^{\\beta + n - y - 1}$$\n",
    "\n",
    "Conjugacy defines how the posterior distribution follows the same parametric form as the prior distribution ($\\beta$ prior distribution is a conjugate family for the binomial likelihood). The posterior mean $\\theta$, which may be interpreted as the posterior probability of success for a future draw from the population, is now:\n",
    "\n",
    "$$E(\\theta | y) = \\frac{\\alpha + y}{\\alpha + \\beta + n}$$\n",
    "\n",
    "which lies between the sample proportion, $y/n$, and the prior mean, $\\alpha/(\\alpha + \\beta)$. The posterior variance is:\n",
    "\n",
    "$$var(\\theta | y) = \\frac{(\\alpha + y)(\\beta + n - y)}{(\\alpha + \\beta + n)^2(\\alpha + \\beta + n + 1)} = \\frac{E(\\theta | y)[1 - E(\\theta | y)]}{\\alpha + \\beta + n + 1}$$\n",
    "\n",
    "As $y$ and $n - y$ become large with fixed $\\alpha$ and $\\beta$, $E(\\theta | y) \\approx y/n$ and $var(\\theta | y) \\approx 1/n y/n(1 - y/n)$, which approaches zero at the rate $1/n$. The central limit theorem of probability can be put in a Bayesian context to show:\n",
    "\n",
    "$$(\\frac{\\theta - E(\\theta | y)}{\\sqrt(var(\\theta | y))}|y) \\to N(0, 1)$$\n",
    "\n",
    "This limit is used to justify approximating the posterior distribution with a normal distribution. The normal distribution is more accurate approximation in practice for $\\theta$ if we transform $\\theta$ to the logit scale; that is $log(\\theta / 1 - \\theta)$, which expands the probability space from $[0, 1]$ to $(-\\infty, \\infty)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjugate Proir Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjugacy: if $F$ is a class of sampling distribution $p(y | \\theta)$, and $F$ is a class of prior distributions for $\\theta$, then the class $F$ is conjugate for $F$ if:\n",
    "\n",
    "$$p(\\theta | y) \\in P$$\n",
    "\n",
    "for all\n",
    "\n",
    "$$p(. | \\theta) \\in F\\ \\&\\ p(.) \\in P$$\n",
    "\n",
    "We are interested in natural conjugate prior families, which arise by taking $F$ to be the set of all densities having the same functional form as the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonconjugate Prior Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

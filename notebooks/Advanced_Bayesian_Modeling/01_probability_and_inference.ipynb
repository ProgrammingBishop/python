{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting up probability model\n",
    "    - Joint probability distribution for all observable and unobservable quantities in a problem\n",
    "    - Model consistent with knowledge about underlying scientific problem and the data collection process\n",
    "\n",
    "- Conditioning on observed data\n",
    "    - Calculating and interpreting the appropriate posterior distribution\n",
    "\n",
    "- Evaluating fit of model and the implications of the resulting posterior distribution\n",
    "    - How well does the model fit the data\n",
    "    - Are conclusions reasonable\n",
    "    - How sensitive are the results to the modeling assumptions to the probability model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Estimands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unobserved quantities\n",
    "    - Future observations\n",
    "    - Outcome under treatment not recieved\n",
    "- Observed quantities\n",
    "    - parameters governing hypothetical process (like regression coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\theta$ are population parameters\n",
    "- $y$ denotes the observed data\n",
    "- $\\tilde y$ represents the unknown, but potentially observable quantities\n",
    "- Exchangeability are iid variables (independent and identically distributed) given $\\theta$ and distribution $p(\\theta)$\n",
    "    - Exchangeability means data is unaffected by random permutations of the indexes (indexes are randomly assigned)\n",
    "    - If two units have same value of $x$, then distributions of $y$ are equal\n",
    "- $X$ are features, $X_k$ is $k$th feature of $X$, and feature matrix is a $n\\ x\\ k$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional on the observed value of $y$: $p(\\theta | y)$ or $p(\\tilde y | \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p(.)$ denotes a marginal distribution (distribution and density can be used interchangeably)\n",
    "    - $Pr(.)$ may be used for probability of an event in some instances so to avoid confusion\n",
    "- $\\theta\\ \\sim N(\\mu, \\sigma^2)$ us a normal distribution\n",
    "    - $N(\\mu, \\sigma^2)$ for random variables\n",
    "    - $N(\\theta | \\mu, \\sigma^2)$ for density functions\n",
    "- $\\frac{sd(\\theta)}{E(\\theta)}$ is coefficient for variation\n",
    "- $exp(E[log(\\theta)])$ is geometric mean\n",
    "- $exp(sd[log(\\theta)])$ is geometric standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make probability statements about $\\theta$ and $y$, we need to provide joint probability distribution for $\\theta$ and $y$. The Probability Mass/Density Function can be written as a product of two densities referred to as Prior Distribution $p(\\theta)$ and Sampling Distribution $p(y | \\theta)$:\n",
    "\n",
    "$$p(\\theta | y) = p(\\theta)p(y | \\theta)$$\n",
    "\n",
    "Posterior Density:\n",
    "\n",
    "$$p(\\theta | y) = \\frac{p(\\theta | y)}{p(y)} = \\frac{p(\\theta)p(y | \\theta)}{p(y)}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$p(\\theta | y) \\propto p(\\theta)p(y | \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before $y$ is considered, Marginal Distribution (Prior Predictive Distribution) of unknown $y$ is:\n",
    "\n",
    "$$p(y) = \\int p(y, \\theta)d \\theta = \\int p(\\theta)p(y | \\theta)d \\theta$$\n",
    "\n",
    "It is prior because it is not conditional on a previous observation of the process, and predictive because it is the distribution for a quantity that is observable. After $y$ data has been observed, we can predict an unknown observable $\\tilde y$ from the same process. The distribution of $\\tilde y$ is called the Posterior Predictive Distribution: posterior because it is conditioned on an observed $y$ and predictive because it is a prediction for an observable $\\tilde y$:\n",
    "\n",
    "$$p(\\tilde y | y) = \\int p(\\tilde y | \\theta)p(\\theta | y)d \\theta$$\n",
    "\n",
    "This displays the posterior predictive distribution as an average of the conditional predictions over the posterior distribution of $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood Function is when $y$ affects the posterior inference only through $p(y | \\theta)$ when regarded as a function of $\\theta$ for a fixed $y$. Likelihood Principle mstates that for a given sample of data, any two probability models $p(y | \\theta)$ that have the same likelihood function yield the same inference for $\\theta$. Be willing to apply Bayes' Rule under a variety of possible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prior Distribution\n",
    "Consider a woman who has an affected brother, which implies that her mother must be a carrier of the hemophilia gene with one ‘good’ and one ‘bad’ hemophilia gene. We are also told that her father is not affected; thus the woman herself has a fifty-fifty chance of having the gene. The unknown quantity of interest, the state of the woman, has just two values: the woman is either a carrier of the gene $(\\theta = 1)$ or not $(\\theta = 0)$. Based on the information provided thus far, the prior distribution for the unknown $\\theta$ can be expressed simply as $Pr(\\theta = 1) = Pr(\\theta = 0) = 0.5$.\n",
    "\n",
    "##### Data Model and Likelihood\n",
    "The data used to update the prior information consist of the affection status of the woman’s sons. Suppose she has two sons, neither of whom is affected. Let $y_i = 1$ or $0$ denote an affected or unaffected son, respectively. The outcomes of the two sons are exchangeable and conditional on the unknown $\\theta$ and are independent; we assume the sons are not identical twins. The two items of independent data generate the following likelihood function:\n",
    "\n",
    "$$Pr(y_1 = 0, y_2 = 0 | \\theta = 1) = (0.5)(0.5) = 0.25$$\n",
    "$$Pr(y_1 = 0, y_2 = 0 | \\theta = 0) = (1)(1) = 1$$\n",
    "\n",
    "These expressions follow from the fact that if the woman is a carrier, then each of her sons will have a $50\\%$ chance of inheriting the gene and so being affected, whereas if she is not a carrier then there is a probability close to 1 that a son of hers will be unaffected. (In fact, there is a nonzero probability of being affected even if the mother is not a carrier, but this risk—the mutation rate—is small and can be ignored for this example.)\n",
    "\n",
    "##### Posterior Distribution\n",
    "$$Pr(\\theta = 1 | y) = \\frac{p(y | \\theta = 1)Pr(\\theta = 1)}{p(y | \\theta = 1)Pr(\\theta = 1) + p(y | \\theta = 1)Pr(\\theta = 0)} = \\frac{(0.25)(0.5)}{(0.25)(0.5) + (1.0)(0.5)} = \\frac{0.125}{0.629} = 0.20$$\n",
    "\n",
    "The $0.25$ in the numerator represents the product of all priors $y_i$. In this case it is the chance the each son can have the gene given that the mother has the gene (i.e. $0.5 * 0.5$). This part of the equation can be understood as:\n",
    "\n",
    "$$\\prod_{i = 1}^n\\ y_i$$\n",
    "\n",
    "The denominator can be simplified to the following for larger sums:\n",
    "\n",
    "$$\\sum_{j = 1}^n\\ p(\\theta_j)p(y | \\theta_j)$$\n",
    "\n",
    "where all $j$s represent possible outcomes for $y$.\n",
    "\n",
    "##### Adding More Data\n",
    "A key aspect of Bayesian analysis is the ease with which sequential analyses can be performed. For example, suppose that the woman has a third son, who is also unaffected. The entire calculation does not need to be redone; rather we use the previous posterior distribution as the new prior distribution, to obtain:\n",
    "\n",
    "$$\\frac{(0.5)(0.2)}{(0.5)(0.2) + (1.0)(0.8)} = \\frac{0.125}{0.629} = 0.111$$\n",
    "\n",
    "Initial prior values can all be defaulted to $\\frac{1}{n}$, or be defaulted to a value pre-calculated from a larger database. Or, if only a fraction of possible values from a database are used, then the default for each possible outcome can be a normalized value from the outcomes in question:\n",
    "\n",
    "$$1(p(y)) = \\frac{p_{max}(y | \\theta)}{\\sum_{j = 1}^n\\ p(\\theta_j)p(y | \\theta_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability: numerical quantities, defined on a set of ‘outcomes,’ that are nonnegative, additive over mutually exclusive outcomes, and sum to 1 over all possible mutually exclusive outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of previously obtained scores for the candidate matches is considered a mixture of distribution scores for true matches and a distribution for non-matches. Parameters are estimated. These parameters allow us to estimate the probability of false match for any given decision threshold on the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint Density: given two quantities $(u, v)$, we write the joint density as $p(u, v)$. Conditional Distribution (Desnity Function) is $p(u | v)$ and Marginal Density is $p(u) = \\int p(u, v)dv$. The $u$ and $v$ can be vectors.The integral refers to te entire range of the variable being integrated out. It is useful to factor a joint density as a product of marginal and conditional densities:\n",
    "\n",
    "$$p(u, v, w) = p(u | v, w)p(v | w)p(w)$$\n",
    "\n",
    "to be more explicit, the following notation is helpful:\n",
    "\n",
    "$$p(\\theta, y | H) = p(\\theta | H)p(y | \\theta, H)$$\n",
    "\n",
    "where $H$ refers to the set of hypotheses or assumptions used to define the model. \n",
    "\n",
    "$E(.)$ and $var(.)$ for mean and variance respectively:\n",
    "\n",
    "$$E(u) = \\int up(u)du$$\n",
    "$$var(u) = \\int (u - E(u))^2p(u)du$$\n",
    "\n",
    "Covariance (variance matrix and covariance matrix is used interchangeably) matrix is defined as:\n",
    "\n",
    "$$var(u) = \\int (u - E(u))(u - E(u))^Tp(u)du$$\n",
    "\n",
    "In expressions involving expectations, any variable that does not appear explicitly as a conditioning variable is assumed to be integrated out in the expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Means and Variances of Conditional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of $u$ can be obtained by averaging the conditional mean over the marginal distribution of $v$, given $u$ is a random variable and $v$ is some related quantity.\n",
    "\n",
    "$$E(u) = E(E(u | v))$$\n",
    "\n",
    "where inner expectation averages over $u$, conditional on $v$, and the outer expectation averages over $v$. Identity is derived by writing the expectation in terms of the joint distribution of $u$ and $v$ and then factoring the joint distribution:\n",
    "\n",
    "$$E(u) = \\int \\int up(u, v)dudv = \\int \\int up(u | v)dup(v)dv = \\int E(u | v)p(v)dv$$\n",
    "\n",
    "The corresponding result for the variance includes two terms:\n",
    "- Mean of the conditional variance\n",
    "- Variance of the conditional mean\n",
    "\n",
    "$$var(u) = E(var(u | v)) + var(E(u | v))$$\n",
    "\n",
    "If $u$ is a vector, then $E(u)$ is a vector and $var(u)$ is a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation of Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $p_u(u)$ is the density of the vector $u$, and we transform to $v = f(u)$ where $v$ has the same number of componenets as $u$. If $p_u$ is a discrete distribution, and $f$ is a one-to-one function, then the density of $v$ is given by:\n",
    "\n",
    "$$p_v(v) = p_u(f^{-1}(v)$$\n",
    "\n",
    "If $f$ is many-to-one function, then a sum of terms appears on the right side of this expression for $p_v(v)$, with one term corresponding to each of the branches of the inverse function. If $p_u$ is a continuous distribution, and $v = f(u)$ is a one-to-one transformation, the the joint density of the transformed vector is:\n",
    "\n",
    "$$p_v(v) = |J| p_u(f^{-1}(v))$$\n",
    "\n",
    "where $|J|$ is the determinant of the Jacobian of the transformation $u = f - 1(v)$ as a function of $v$ (Jacobian $J$ is a square matrix of partial derivatives with dimension given by the number of components of $u$, with the $i, j$th entry equal to $\\frac{\\partial u_i}{\\partial u_j}$. If $f$ is a many-to-one, then $p_v(v)$ is a sum or integral of terms.\n",
    "\n",
    "When working with parameters defined on the open interval (0, 1), we often use logistic transformation:\n",
    "\n",
    "$$logit(u) = log(\\frac{u}{1 - u})$$\n",
    "\n",
    "whose inverse is:\n",
    "\n",
    "$$logit^{-1}(v) = \\frac{e^v}{1 + e^v}$$\n",
    "\n",
    "Another common choice is the probit transformation $\\phi^{-1}(U)$ where $\\phi$ is the standard normal cumulative distribution function, to transform from $(0, 1)$ to $(-\\infty, \\infty)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling using Inverse Cumulative Distribution Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cumulative Distribution Function (cdf F) of a one-dimensional distribution $p(v)$ is defined by:\n",
    "\n",
    "- $F(v_*) = Pr(v \\le v_*) = $\n",
    "    - $\\sum_{v \\le v_*} p(v)$ if $p$ is discrete\n",
    "    - $\\int_{-infty}^{v_*}p(v)dv$ if $p$ is continuous\n",
    "    \n",
    "The inverse cdf can be used to obtain random samples from the distribution $p$, as follows:\n",
    "- First draw random value $U$ from the uniform distribution $[0, 1]$ using a table of numbers or - more likely - a random function on the computer\n",
    "- Second let $v = F^{-1}(U)$. The function $F$ is not necessarily one-to-one, but the inverse in regard to $U$ is unique with a probability of 1.\n",
    "- Value $v$ will be a random draw from $p$, and is easy to compute as lonf as $F^{-1}(U)$ is simple.\n",
    "\n",
    "For a continuous example, suppose $v$ has an exponential distribution with parameter $\\lambda$, then its' cdf is $F(v) = 1 - e^{\\lambda v}$, and the value of $v$ for which $U = F(v)$ is:\n",
    "\n",
    "$$v = \\frac{log(1 - U)}{\\lambda}$$\n",
    "\n",
    "Recognizing that $1 - U$ also has the uniform distribution $[0, 1]$, we see we can obtain random draws from the exponential distribution as $-log U \\frac{U}{\\lambda}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Inference in Applied Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pragmatic rationale for the use of Bayesian methods is the inherent flexibility introduced by their incorporation of multiple levels of randomness and the resultant ability to combine information from different sources, while incorporating all reasonable sources of uncertainty in inferential summaries.\n",
    "\n",
    "Strength of the Bayesian approach lies in:\n",
    "- Its ability to combine information from multiple sources (thereby in fact allowing greater ‘objectivity’ in final conclusions)\n",
    "- Its more encompassing accounting of uncertainty about the unknowns in a statistical problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

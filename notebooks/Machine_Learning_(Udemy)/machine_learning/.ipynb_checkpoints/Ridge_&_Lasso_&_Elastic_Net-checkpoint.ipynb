{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge & Lasso & Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Least Squares with L2 regularization and minimizes the objective function:\n",
    "$$||y - Xw||^2_2 + alpha * ||w||^2_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- alpah: regularization strength and must be a positive float. This reduces the variance of the estimates. If an array is passed, penalties are expected to be specific to targets. \n",
    "- fit_intercept: whether to calculate the intercept for this model. \n",
    "- normalize: ignored when fit_intercept is False. If True regressors will be normalized before regression by subtracting the mean and dividing by the L2-Norm. To standardize use sklearn.preprocessing.StandardScaler.\n",
    "- copy_X: if True X will be copied and not overwritten.\n",
    "- max-iter: number of iterations for gradient solver. \n",
    "- tol: precision of the solution\n",
    "- solver:\n",
    "    - auto: chooses solver automatically based on data\n",
    "    - svd: uses Singular Value Decomposition of X. This is more appropriate than cholesky for singular matrices\n",
    "    - cholesky: uses scipy.linalg.solve to obtain closed-form solution\n",
    "    - sparse_cg: uses conjugate gradient solver. This is more appropriate than cholesky for large-scale data.\n",
    "   - lsqr: uses least squares (fastest)\n",
    "   - saga: uses Stochastic Gradient descent. Usually fastest when n_features and n_samples is large.\n",
    "- random_state\n",
    "\n",
    "#### Attributes\n",
    "- coef_: weight vectors\n",
    "- intercept_: independent term in decision function\n",
    "- n_iter_: actual number of iterations for each target. This is available for only sag and lsqr solvers. \n",
    "\n",
    "#### Methods\n",
    "- fit\n",
    "- get_parames\n",
    "- predict\n",
    "- score\n",
    "- set_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- alpah: regularization strength and must be a positive float. This reduces the variance of the estimates. If an array is passed, penalties are expected to be specific to targets. \n",
    "- fit_intercept: whether to calculate the intercept for this model. \n",
    "- normalize: ignored when fit_intercept is False. If True regressors will be normalized before regression by subtracting the mean and dividing by the L2-Norm. To standardize use sklearn.preprocessing.StandardScaler.\n",
    "- precompute: whether to use a precomputed Gram matrix to speed up calculations.\n",
    "- copy_X: if True X will be copied and not overwritten.\n",
    "- max-iter: number of iterations for gradient solver. \n",
    "- tol: precision of the solution\n",
    "- warm_start: reuse the solution of the previos call to fit as init\n",
    "- positive: forces coefficients to be positive\n",
    "- selection: if 'random' a random coefficient is updated every iteration rather than looping over features sequentially. \n",
    "\n",
    "#### Attributes\n",
    "- n_features\n",
    "- intercept_\n",
    "- n_iter_\n",
    "\n",
    "#### Methods\n",
    "- fit()\n",
    "- get_params()\n",
    "- path(X, y[, l1_ratio, eps, n_alphas, â€¦]): Compute elastic net path with coordinate descent\n",
    "    - X: training data\n",
    "    - y: target values\n",
    "    - l1-ratio: float between 0 and 1 passed to elastic net\n",
    "    - eps: length of the path\n",
    "    - n_alphas: list of alphas where to compute the models\n",
    "    - precompute: use Gram matrix\n",
    "    - Xy: X transpose y (dot multiplication). Useful when Gram matrix is computed\n",
    "    - copy_X\n",
    "    - coef_init: initial coefficient values\n",
    "    - return_n_iter\n",
    "    - positive\n",
    "    - check_input\n",
    "- predict()\n",
    "- score()\n",
    "- set_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- alpha\n",
    "- l1_ratio\n",
    "- fit_intercept\n",
    "- normalize\n",
    "- precompute\n",
    "- max_iter\n",
    "- copy_X\n",
    "- tol\n",
    "- warm_start\n",
    "- positive\n",
    "- random_state\n",
    "- selection\n",
    "\n",
    "#### Attributes\n",
    "- coef_\n",
    "- intercept_\n",
    "- n_iter_\n",
    "\n",
    "#### Methods\n",
    "- fit\n",
    "- get_params\n",
    "- path\n",
    "- score\n",
    "- set_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

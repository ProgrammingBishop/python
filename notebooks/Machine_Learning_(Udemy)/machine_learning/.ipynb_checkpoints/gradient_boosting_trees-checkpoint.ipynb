{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- loss: default is 'deviance'. Loss function to be optimized. \n",
    "- learning rate: deafault is 0.1. Shrinks the contribution of each tree by the learning rate. There is a trade-off between learning rate and the number of estimators.\n",
    "- n_estimators: default is 100. The number of boosting stages to perfrom. A large number usually results in good performance. \n",
    "- max_depth: Max depth of an individual regression estimator. This limits the number of nodes in a tree. The best value depends on the interaction of the input variables. \n",
    "- min_samples_split: the minimum number of samples required to split an internal node. \n",
    "- subsample: fraction of samples to be used for fitting the individual base learners. If less than 1.0 this results in Stochastic Gradient Descent and a reduction in variance and increase in bias. This interacts with the n_estimators.\n",
    "- max_features: Number of features to look at for the best split. If less than n_features this results in reduction of variance and an increase in bias. \n",
    "- max_leaf_nodes: Grow trees with this in best-first fashion. \n",
    "- init: estimator object to compute the initial predictions. If nothing is provided it will use loss.init_estimator\n",
    "- warm_start: reuse solution of the previous call to fit and add more estimators to the ensemble. \n",
    "\n",
    "#### Attributes\n",
    "- feature_importances_: higher number means a more important features\n",
    "- oob_improvement_: improvement in loss on the oob samples relative to the previous iteration. \n",
    "- train_score_: ith score is the deviance of model at iteration i on the oob sample. If subsample == 1 then this is the deviance on training data. \n",
    "- loss_: concrete loss function object.\n",
    "- init: estimator that provides the init predictions. \n",
    "- estimators_: collection of fitted sub_estimates.\n",
    "\n",
    "#### Methods\n",
    "- decision_function(X): Compute the decision function of X.\n",
    "- feature_importances_: return feature importances \n",
    "- fit(X, y[, monitor]): Fit the gradient boosting model.\n",
    "- fit_transform(X[, y]): Fit to data, then transform it.\n",
    "- get_params([deep]): Get parameters for this estimator.\n",
    "- predict(X): Predict class for X.\n",
    "- predict_proba(X): Predict class probabilities for X.\n",
    "- score(X, y[, sample_weight]): Returns the mean accuracy on the given test data and labels.\n",
    "- set_params(**params): Set the parameters of this estimator.\n",
    "- staged_decision_function(X): Compute decision function of X for each iteration.\n",
    "- staged_predict(X): Predict classes at each stage for X.\n",
    "- staged_predict_proba(X): Predict class probabilities at each stage for X.\n",
    "- transform(X[, threshold]): Reduce X to its most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

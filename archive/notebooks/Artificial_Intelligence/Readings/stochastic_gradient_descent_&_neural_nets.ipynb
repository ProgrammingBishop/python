{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent & Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural nets accomplish random starting places for gradient descent by each hidden node in a layer (these represent different starting states). This allows each hidden node to converge to a different patterns in the network. Parameterizing this size allows a user to try variations of different local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is for this reason that neural networks are great: they can search a very large space exponentially faster than a brute force approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a slope is too small the local minima can occur too soon, a way around this is to increase the $\\alpha$ parameter. We couls also multiply our $\\delta$s by a weight greater than 1 (this is rare)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ reduces the size of each iteration's update in the simplest way possible. Right before an update occurs, we multiply the weight by $\\alpha$ to reduce the size of the weight update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Takeaways for Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When the alpha was tiny, the derivatives almost never changed direction.\n",
    "- When the alpha was optimal, the derivative changed directions a TON.\n",
    "- When the alpha was huge, the derivative changed directions a medium amount.\n",
    "- When the alpha was tiny, the weights ended up being reasonably small too\n",
    "- When the alpha was huge, the weights got huge too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of the Hidden Layer: increasing the size of the hidden layer increases the amount of search space that is converged in each iteration. But be sure to keep an eye on how much more the error decreases because if a huge increase in hidden nodes results in a very small error decrease the extra nodes may not need to be in there. More nodes will make the network converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
